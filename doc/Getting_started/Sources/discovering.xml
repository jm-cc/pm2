<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE TEXT SYSTEM "file:../xml2html/data/xml/DOCUMENT.dtd">

<TEXT>

<P>
This chapter aims at illustrating the basic features of PM2 through
some very simple programs. The next chapters present more advanced
features that may be needed when developing <EM>real</EM> parallel
applications.
</P>

<SUBSECTION TITLE="Compiling your own PM2 program">

<P>
You may compile you own PM2 program just as a usual
C<SPACE/>program. You just need to make sure the necessary definitions
and libraries are included. As the command line is rather complex,
PM2 provides a simple utility to generate it online:
</P>

<UL>
<LI>
<TT>pm2-config --cc</TT> generates the name of the adequate C<SPACE/>compiler (most probably, <TT>gcc</TT>),
</LI>
<LI>
<TT>pm2-config --cflags</TT> generates the necessary <TT>CFLAGS</TT>,
</LI>
<LI>
and <TT>pm2-config --libs</TT> generates the list of libraries to be searched on linking.
</LI>
</UL>

<P>
Thus, the standard command line to compile a PM2 program <TT>hello.c</TT> looks like:
</P>

<PROGRAM FILE="./Progs/compileHello.c"/>

<P>
Observe that the source file <TT>hello.c</TT> should be mentioned
<EM>before</EM> the PM2 libraries, as the external symbols are searched by
<TT>gcc</TT> in the libraries from left to right.
</P>

<P>
Well... Such a command line is rather tedious to enter! You will find
on Program <REF LABEL="prog:Makefile1"/> a <TT>Makefile</TT> which does
all the work for you. Executing <TT>make hello</TT> will compile
the source file <TT>hello.c</TT> with all the necessary parameters.
</P>

<PROGRAM FILE="./Progs/Makefile1" TITLE="A minimal GNU Makefile for PM2 programs" LABEL="prog:Makefile1"/>

<P>
Observe that using dynamic calls to the <TT>pm2-config</TT> utility guarantees
that you are compiling with the suitable options with respect to the
current value of the flavor, as specified by the <TT>PM2_FLAVOR</TT>
shell variable.
</P>

<P>
Let us start our PM2 tour by writing the traditional ``<EM>Hello
World!</EM>'' program. Then, we will extend it step by step to cover
the main functionalities provided by the different PM2 programming interfaces. We
will in the following sections present the Hello World! program
written on top of Madeleine, Marcel, and <MU/>PM2.
</P>

</SUBSECTION>

<SUBSECTION TITLE="Initializing and terminating a PM2 program">

Initialization and termination phases are similar to all PM2 programs,
whatever the programming interface you wish to use: Marcel, Madeleine
or <MU/>PM2. This section will explain these two phases.

<SUBSUBSECTION TITLE="The header files">

<P>
A PM2 program must always include the PM2-specific
<TT>pm2_common.h</TT> header file, along with other standard header
files.  Note that this is the <EM>only</EM> PM2 header file that
has to be included by user applications.
</P>

</SUBSUBSECTION>

<SUBSUBSECTION TITLE="Initialization">

<P>
A PM2 program has to call the two functions
<TT>common_pre_init</TT> and <TT>common_post_init</TT> to effectively
initialize the PM2 runtime system. Moreover, this step involves a
global synchronization phase (actually, a global synchronization
barrier) among all the PM2 nodes so that each process is assigned a
unique <EM>rank number</EM> on return from the functions. In
consequence, it makes no sense to call functions concerned with the
global execution environment (such as node numbers, etc.)
<EM>before</EM> this point. Please, refer to Section <REF
LABEL="sec:rank"/> for details.
</P>

<P>
The first two arguments of the function are the usual pair
<TT>argc/argv</TT> of the C <TT>main</TT> function. The type of the
last argument is <TT>common_attr_t</TT>, it is used to specify
parameters for the configuration of the application, in most cases,
the value <TT>NULL</TT> will do.
</P>

<P>
Most importantly, the initialization functions spawn a number of
internal <EM>thread daemons</EM> that are in charge of listening to
the network and answering to external requests such as RPCs, incoming
thread migrations, etc. A node should be ready to handle all possible
incoming requests from any other node at this point. As a consequence,
it is not safe to do any initialization <EM>after</EM> this point, as
the user has no control about the interleaving of requests and the
relative speed of the nodes. Even though the initialization call would
be placed just after the initialization functions call, an arbitrary
delay may occur between the two successive instructions! It follows
that if some initialization code needs to be performed before any
thread is started, then this code <EM>must</EM> be called
<EM>before</EM> calling the initialization functions.
</P>

<P>
Also note that the Unix standard input/output streams may not be
correctly initialized before the call to the initialization
functions. Thus, the behavior of programs using I/O operations before
the initialization is not defined.
</P>

</SUBSUBSECTION>

<SUBSUBSECTION TITLE="Termination">

<P>
To exit from a PM2 session, each node must call the
<TT>common_exit</TT> routine. The parameter is similar to the one for
the <TT>common_pre_init</TT> function.
</P>

<P>
Note that the termination of any PM2 program is a tricky task as PM2
does not perform any automatic termination detection. Thus, the
termination decision must be made at the application level.
</P>

</SUBSUBSECTION>

</SUBSECTION>

<SUBSECTION TITLE="A minimal Madeleine program: Hello World!">

<P>
Program <REF LABEL="prog:hello-mad"/> shows a minimal Madeleine program. Compile it:
</P>

<PROGRAM FILE="./Progs/makeHelloMad.txt"/>

<PROGRAM FILE="./Progs/gs_mad_hello.c" LABEL="prog:hello-mad" TITLE="Minimal Madeleine program"/>

<P>
Compilation using the Makefile given in Program <REF
LABEL="prog:Makefile1"/> creates the executable file in the
appropriate PM2 build directory, e.g
<TT>${PM2_BUILD_DIR}/${PM2_ASM}/${PM2_FLAVOR}/examples/bin</TT>. When needed, the
program will be located using the utility <TT>pm2-which</TT>.
</P>

<PROGRAM FILE="./Progs/pm2which.txt"/>

<P>
You can now execute the application:
</P>

<PROGRAM FILE="./Progs/runHelloMad.txt"/>

<SUBSUBSECTION TITLE="Informations about the application">

<P>
In order to have a global view of the application, each node needs to
retrieve the Madeleine object which contains for example informations
about the communication channel, the set of processes in this
channel. These informations will allow each node to retrieve its local
rank in the current channel. Here a set of instructions showing how to
get these informations:
</P>

<PROGRAM FILE="./Progs/mad-init.c"/>

</SUBSUBSECTION>

<SUBSUBSECTION TITLE="Packing and unpacking messages" LABEL="sec:mad-pack">

<P>
The packing model of PM2 has been carefully designed so as to enable
high-performance communication on modern Gigabit network interfaces
such as BIP/Myrinet, SISCI/SCI, VIA, etc. In this context, it is of
uttermost significance to avoid copies: actually, the time for copying
a buffer within a node is of the same order of magnitude as the time
for sending it over the network to some remote node!  The key for
performance is therefore to allow for <EM>zero-copy</EM>
communication: the message has to be directly taken from its initial
location in user-space, and directly placed into its final destination
in user-space, without any additional copy. Designing communication
interfaces which can efficiently deal with messages featuring a
complex structure, or messages of unpredictable size, is a
difficult task. The approach of PM2, or more accurately of its
underlying communication library Madeleine, is to control the
packing and unpacking operations with additional <EM>flags</EM>.
</P>

<P>
A Madeleine message consists of several pieces of data, located
anywhere in user-space. It is constructed (resp. de-constructed)
incrementally using <EM>packing</EM> (resp. <EM>unpacking</EM>)
routines, possibly at multiple software levels without losing
efficiency. The following example illustrates the power of the
Madeleine interface. Let us consider a remote procedure call which
takes an array of unpredictable size as a parameter. When the request
reaches the destination node, the header is examined both by the
multithreaded runtime (to allocate the appropriate thread stack and
then to spawn the server thread) and by the user application (to
allocate the memory where the array should be stored).
</P>

<P>
The critical point of a sending operation is obviously the series of
<EM>packing</EM> calls. Such packing operations simply <EM>virtually</EM>
append the piece of data to a message under construction. In addition
to the address of data and its size, the packing primitive features a
<EM>flag</EM> parameter which specifies the behaviour of the operation.
Available sending flags are defined as follows:
</P>

<DL>
    
<DT><TT>mad_send_SAFER</TT></DT>
<DD> This flag indicates that PM2 should pack the data
  in a way that further modifications to the corresponding memory area
  should not corrupt the message. This is particularly mandatory if
  the data location is reused before the message is actually sent.
</DD>

<DT><TT>mad_send_LATER</TT></DT>
<DD> This flag indicates that PM2 should not consider
  accessing the value of the corresponding data until the
  <TT>mad_end_packing</TT> primitive is called. This means that any
  modification of these data between their packing and their sending
  shall actually update the message contents.
</DD>

<DT><TT>mad_send_CHEAPER</TT></DT>
<DD> This is the default flag. It allows PM2 to do
  its best to handle the data as efficiently as possible. The
  counterpart is that no assumption should be made about the way PM2
  will access the data. Thus, the corresponding data should be left
  unchanged until the send operation has completed. Note that most
  data transmissions involved in parallel applications can accommodate
  this <TT>mad_send_CHEAPER</TT> behaviour.
</DD>
</DL>

<P>
The following flags control the reception of user data packets:
</P>

<DL>
    
<DT><TT>mad_receive_EXPRESS</TT></DT>
<DD> This flag forces PM2 to guarantee that the
  corresponding data are immediately available after the the
  <EM>unpacking</EM> operation. Typically, this flag is mandatory when
  the data is needed to determine the next forthcoming <EM>unpacking</EM>
  calls.  On some network protocols, this functionality may be
  available for free. On some others, it could penalize the latency and
  the bandwidth. Users should therefore extract data using this behaviour 
  only when necessary.
</DD>

<DT><TT>mad_receive_CHEAPER</TT></DT>
<DD> This flag allows PM2 to defer the extraction of
  the corresponding data until the execution of the
  <TT>mad_end_unpacking</TT> routine.  Thus, no assumption can be made
  about the exact moment at which the data will be extracted.
  Depending on the underlying network protocol, PM2 will do its best
  to minimize the overall message transmission time. If combined with
  <TT>mad_send_CHEAPER</TT>, this flag always guarantees that the corresponding
  data is transmitted as efficiently as possible.
</DD>
</DL>

<P>
Observe that the emission and reception flags should be <EM>both</EM>
specified  by the matching packing/unpacking calls, and these
specifications should be identical. Again, unspecified behavior would
result from mismatching flags.
</P>

<PROGRAM FILE="Progs/mad-params.c" TITLE="Sending and receiving data with Madeleine" LABEL="prog:mad-params"/>

<P>
In Program <REF LABEL="prog:mad-params"/>, everything is sent
<TT>CHEAPER</TT>, so as to save time. The user must be careful not to
corrupt the variables <TT>len</TT>, <TT>s</TT> and <TT>s2</TT> before calling
<TT>mad_end_packing</TT>. On the reception side, the variable
<TT>len</TT> <EM>must</EM> be received <TT>EXPRESS</TT>, as its value
is needed to allocate the buffer for the unpacking of the variables
<TT>s</TT> and <TT>s2</TT>.  Then, these two variables can be received
<TT>CHEAPER</TT>. If the underlying operating system and the network
interface permit, then this (possibly large) message will thus be
directly installed into the reception buffer without any extra copy,
providing the user with optimal performances.
</P>

</SUBSUBSECTION>

<SUBSUBSECTION TITLE="Sending and receiving messages">

<P>
In order to sent a message to another node, you need to know the
channel it belongs to, and its local rank within this channel. You can
then open an outgoing connection with this node by calling
<TT>mad_begin_packing</TT>, pack data in the message as explained in
the previous section, and finalize the message by calling
<TT>mad_end_packing</TT>.
</P>

<P>
Nodes can receive messages sent on a specific channel by calling
<TT>mad_begin_unpacking</TT>. Note here that is not possible to specify
which node you want to receive a message from. If two nodes belonging
to the same channel send a message to a third node, then receiving a
message on this third node will either read the message from the first
or the second node. After calling <TT>mad_begin_unpacking</TT>, data
have to be unpacked as explained in the previous section, a call to
<TT>mad_end_unpacking</TT> will finalize the reception of the message.
</P>

</SUBSUBSECTION>

<CLEARPAGE/>
</SUBSECTION>

<SUBSECTION TITLE="A minimal Marcel program: Sum of the first n integers">

<P>
Program <REF LABEL="prog:hello-marcel"/> shows a minimal Marcel program. Compile it and run it:
</P>

<PROGRAM FILE="./Progs/makeAndRunHelloMarcel.txt"/>

<PROGRAM FILE="./Progs/marcel-hello.c" LABEL="prog:hello-marcel" TITLE="Minimal Marcel program"/>

<SUBSUBSECTION TITLE="The marcel_main function">

<P>
The main function of a Marcel program is named <TT>marcel_main</TT>,
in contrast to traditional C<SPACE/>programs which use the well-known
<TT>main</TT> function name. In fact, the <EM>real</EM> <TT>main</TT>
function of the program is provided by the PM2 libraries. It has to
set up the execution environment before calling the user
<TT>marcel_main</TT> function.  This allows PM2 to greatly enhance the
performance of various thread management functions.  The arguments
remain the regular <TT>argc</TT>/<TT>argv</TT> pair, with their usual
meaning.
</P>

<P>
Although most programs (or rather, programmers!) can accommodate such a
violation of the usual <EM>C<SPACE/>convention</EM>, there exists some
applications that require to use the regular <TT>main</TT> function
name. In particular, it may be the case with applications linked with
non-C code (<EM>e.g.</EM>, Fortran code).  In this case, you must use the
<TT>stackalign</TT> module, please refer to Section <REF LABEL="sec:tradimain"/> for
details.
</P>

</SUBSUBSECTION>

<SUBSUBSECTION TITLE="Thread management">
<NOTE><EM>TODO...</EM></NOTE>
</SUBSUBSECTION>

<SUBSUBSECTION TITLE="Semaphores and mutexes">
<NOTE><EM>TODO...</EM></NOTE>
</SUBSUBSECTION>

<CLEARPAGE/>
</SUBSECTION>

<SUBSECTION>
<TITLE>A minimal <MU/>PM2 program: Hello World!</TITLE>

<P>
Program <REF LABEL="prog:hello"/> shows an example of a minimal <MU/>PM2
code. Compile it and run it:
</P>

<PROGRAM FILE="./Progs/makeHello2.txt"/>

<PROGRAM FILE="./Progs/pm2-hello.c" LABEL="prog:hello">
<TITLE>Minimal <MU/>PM2 program</TITLE>
</PROGRAM>

<IGNORE>
<WARNING>
  <EM>
Compilation using the Makefile given in Program <REF
LABEL="prog:Makefile1"/> creates the executable file in the current
directory. For PM2 to locate the executable when connecting to remote
machines, you need to make sure your current directory is in the
<TT>PATH</TT> or to move the executable <TT>hello</TT> into a
directory available in the <TT>PATH</TT>.
  </EM>
</WARNING>
</IGNORE>

<IGNORE>
<P>
Though it looks simple, this is a full-fledged SPMD parallel program
that spawns on several processing nodes during execution! Let's
examine it step by step.
</P>
</IGNORE>

<SUBSUBSECTION TITLE="The pm2_main function">

<P>
The main function of the <MU/>PM2 program is named <TT>pm2_main</TT>, in contrast
to traditional C<SPACE/>programs which use the well-known <TT>main</TT>
function name. In fact, the <EM>real</EM> <TT>main</TT> function of the
program is provided by the PM2 libraries. It has to set up the
execution environment before calling the user <TT>pm2_main</TT> function.
This allows PM2 to greatly enhance the performance of various thread
management functions.  The arguments remain the regular
<TT>argc</TT>/<TT>argv</TT> pair, with their usual meaning.
</P>

<P>
Although most programs (or rather, programmers!) can accommodate such a
violation of the usual <EM>C<SPACE/>convention</EM>, there exists some
applications that require to use the regular <TT>main</TT> function name. In
particular, it may be the case with applications linked with non-C
code (<EM>e.g.</EM>, Fortran code).  In this case, please refer to
Section <REF LABEL="sec:tradimain"/> for details.
</P>

</SUBSUBSECTION>

<SUBSUBSECTION TITLE="Termination">

<P>
In the case of <MU/>PM2, it is important to have a synchronization
phase before stopping the nodes. This synchronization phase is
achieved by calling the <TT>pm2_halt</TT> routine, it should be
called by exactly one node of the application. It performs a broadcast
that requires all nodes to stop answering to requests from the outside
world. In fact, this step cuts the links between nodes: it stops the
internal daemon threads that are in charge of polling the network.
Note that after a node receives this order, it still continues its
normal execution in a <EM>standalone</EM> mode: <EM>halting</EM> is no
<EM>killing</EM>!
</P>

<P>
In a second step, all the nodes can then call the usual
<TT>common_exit</TT> routine. That will block the calling thread until
all other threads (belonging to the same node) terminate. In our
example, no other application thread except from the main one is
running. Thus, this second step completes as soon as the internal
daemon threads are stopped. Without the first step being performed,
all nodes would remain hanging, waiting for ever for external
requests!
</P>

<P>
As the call to <TT>common_exit</TT> is potentially blocking, the thread in
charge of calling <TT>pm2_halt</TT> should do it first. However, observe
that it may well be the case that at a node, some thread is in charge
of calling <TT>pm2_halt</TT> and another one <TT>common_exit</TT>, so that the order
may in fact be irrelevant.
</P>

<P>
From the user point of view, the <MU/>PM2 program terminates as soon as the
shell prompts for the next command from the terminal. This corresponds
to the end of the main node. However, note that some nodes may
actually keep running after the main node has completed.  
</P>

<IGNORE>
In our example,
the main node can even terminate before any other node has started
executing <TT>tprintf</TT>! Fortunately, this is not a problem here, since
<TT>tprintf</TT> does not involve any communication with other nodes.

We will further discuss this termination problem in Section <REF LABEL="sec:output"/>.
</IGNORE>

</SUBSUBSECTION>

<IGNORE>
<SUBSUBSECTION TITLE="Main code">

<P>
Going back to our example program, note the call to the <TT>tprintf</TT>
routine of PM2. This is a simple wrapper of the regular <TT>printf</TT>
routine of the C<SPACE/>library, which is protected against multithreading
(technically, it is made <EM>reentrant</EM> by disabling scheduling).
As mentioned previously, all nodes perform this call and actually
produce the ``<EM>Hello World!</EM>'' string on their output stream.
<IGNORE>
Yet, only the output of node<SPACE/>0 is observed here.
Section <REF LABEL="sec:output"/> describes where these outputs actually go
during execution.
</IGNORE>
</P>

</SUBSUBSECTION>
</IGNORE>

<SUBSUBSECTION TITLE="Who's who?" LABEL="sec:rank">

<P>
Each processing node (that is, Unix process) taking part in a given
execution receives its own unique <EM>rank</EM> number. This is an
<TT>unsigned int</TT> between 0 and <TT>pm2_config_size() - 1</TT>. A
node can get its own rank by calling the <TT>pm2_self</TT> routine.
</P>

<P>
The processing node with rank<SPACE/>0 has a particular status because it is
the only one which whose input/output streams are directly linked to the
terminal the application was launched from. We later refer to this
process as the <EM>main node</EM> of an application. As a consequence,
only the main node of an application can access its standard input
stream (<EM>e.g.</EM>, using <TT>scanf</TT>, <TT>gets</TT>, etc.)
</P>

<P>
As an example on how to use <TT>pm2_self</TT>, the program performs a
test on the rank of the current process: if the current process is the
main process, then the <TT>pm2_halt</TT> routine is called.
</P>

</SUBSUBSECTION>

<IGNORE>
<SUBSUBSECTION TITLE="Where does the output go?" LABEL="sec:output">

<P>
This simple <TT>hello</TT> program only printed the message generated by the
master node node. Indeed, the standard output of the other nodes is
redirected to local log files located into the <TT>/tmp</TT> directory.  The
logs are easily accessible using the command <TT>pm2-logs</TT> which is in
charge of retrieving and displaying logs from each slave node of the
session configuration. Observe that such requests are quite slow, as
they use <TT>rsh</TT> connections:
</P>

<PROGRAM FILE="./Progs/pm2-logs.txt"/>

<PROGRAM FILE="./Progs/hello1.c" TITLE="Output redirection, Example 1" LABEL="prog:hello1"/>

<P>
Observe that there is no reason why all the nodes should make the
<EM>same</EM> output requests. Consider for instance the variant of the
<TT>hello</TT> program on Program <REF LABEL="prog:hello1"/>. Only node<SPACE/>1 outputs a
message.  Also, just to show you that any node can force termination,
node<SPACE/>2 is this time in charge of it!
</P>

<PROGRAM FILE="./Progs/makeHello1.txt"/>

<SUBSUBSUBSECTION TITLE="Redirecting output to the terminal">

<P>
For user convenience, it is possible to redirect output directly
to the output of the master node using the <TT>pm2_printf</TT> function. As
<TT>tprintf</TT>, it is protected against multithreading. Moreover, it sends
its output as a regular message to node<SPACE/>0, where it is printed out.
However, the user must be aware that this facility should be used with
caution, as it uses the common communication subsystem. 
</P>

<PROGRAM FILE="./Progs/hello2.c" TITLE="Output redirection, Example 2" LABEL="prog:hello2"/>

<P>
Consider program <TT>hello2</TT> on Program <REF LABEL="prog:hello2"/>. The output is
immediately printed on the terminal attached to the master node. The
<TT>[t1]</TT> label is inserted to identify the outputting node. By
convention, outputs from node<SPACE/>0 have no label, so as to mimic the
behavior of the original <TT>printf</TT> function.
</P>

<PROGRAM FILE="./Progs/pm2-load2.txt"/>

</SUBSUBSUBSECTION>

</SUBSUBSECTION>
</IGNORE>

</SUBSECTION>

<SUBSECTION>
<TITLE>A minimal <MU/>PM2 program: Hello World! using Remote Procedure Calls</TITLE>

<P>
The goal of this section is to let you issue a <EM>Remote Procedure
Call</EM> in <MU/>PM2. We start with a basic scheme, and we refine it step
by step so as to demonstrate the versatility of <MU/>PM2.
</P>

<SUBSUBSECTION TITLE="Invoking a remote service">

<P>
A <EM>service</EM> is a function located on a <EM>server</EM> node, which
can be invoked by some <EM>client</EM> node. Observe that the client
node may be the same as the server node. It may also be run on the
same processor, or on a sibling processor on the same SMP board, or on
some remote one. All of this is fully transparent for PM2 users.
Nodes are specified through their numbers, as allocated by the
<TT>pm2-conf</TT> command. In this part, we consider our usual 3-node
configuration:
</P>

<PROGRAM FILE="./Progs/pm2conf1.txt"/>

<PROGRAM FILE="./Progs/rpc-hello.c" TITLE="Defining a service in PM2" LABEL="prog:rpc"/>

<P>
In Program <REF LABEL="prog:rpc"/>, a basic service is defined. Its behavior is
to print <TT>Hello, World!</TT> on the standard output of the node which is
executing it. At the level of PM2, a <EM>service</EM> is just an <TT>int</TT>
associated to some function using the <TT>pm2_rawrpc_register</TT> routine.
Observe that the service function takes no argument.
</P>

<P>
Of course, all the available servers have to be registered
<EM>before</EM> they may be invoked. The association between the service
identifier and the service function has thus to be set up
<EM>before</EM> calling the PM2 initialization routines. Doing otherwise will
result in unspecified behavior, probably depending on the relative
speed of the nodes. The user is therefore <EM>strongly advised</EM> to
give extra attention to this requirement.
</P>

<P>
Returning to the program, the processing node<SPACE/>0 invokes the remote service
<TT>service_id</TT> on the processing node<SPACE/>1. The service invocation is
initiated using the <TT>pm2_rawrpc_begin</TT> routine.  The first argument
of the routine is the unique number of the server node.  The second
argument is the service identifier. It has to be a valid number on the
<EM>server node</EM>.  The third argument is used for specifying
additional <EM>attributes</EM> to be discussed later. In this basic
case, no additional data is to be provided together with the service
invocation, and <TT>NULL</TT> is a sensible default. The <TT>pm2_rawrpc_end</TT>
routine finalizes the service invocation. Returning from this routine
guarantees that the service invocation has been completed from the
client's point of view.
</P>

<P>
In this example, the service is synchronous: the <TT>pm2_rawrpc_end</TT>
returns in the client node only <EM>after</EM> the service function has
returned on the remote server node. Thus, the client node can safely
call <TT>pm2_halt</TT> to force the termination of the distributed program.
</P>

<P>
It is now time to run the program:
</P>

<PROGRAM FILE="./Progs/pm2loadrpc.txt"/>

<IGNORE>
<P>
Observe that an image of the program has been started <EM>both</EM> on
hosts <TT>debussy</TT> and <TT>faure</TT>. In fact, the service function has been
registered on all 3 nodes, including root node<SPACE/>0.  Yet, only 
node<SPACE/>1 has been used as a server for the request of node<SPACE/>0.
</P>

<P>
In this example, the two processing nodes of PM2 have been launched on
two distinct machines for the sake of clarity.  However, the machine
running the server node may well be the same as the one running the
client node:
</P>

<PROGRAM FILE="./Progs/pm2-conf2.txt"/>
</IGNORE>

<P>
The careful reader will have noticed that the definition of the
service function includes a call to the <TT>pm2_rawrpc_waitdata</TT>
routine. This is necessary to instruct the service function that no
additional data is to be expected, so that it can proceed safely.
Actually, the service function has no way of discovering that the RPC
has been invoked without any additional data, and it is up to the user
to build the program client and server parts of the program in a
consistent way. More about this subject in the next section!
</P>

</SUBSUBSECTION>

<SUBSUBSECTION TITLE="Passing parameters">

<P>
Let us see now the case of a service function invoked with additional
parameters. In Program <REF LABEL="prog:rpc-params"/>, node<SPACE/>0 invokes a service
on node<SPACE/>1 with a string as a parameter. The service consists namely in
printing the string on the standard output.
</P>
  
<PROGRAM FILE="./Progs/rpc-params.c" TITLE="Service with a string parameter" LABEL="prog:rpc-params"/>

<P>
Which string? Well, the original motivation of PM2 was to provide a
runtime environment for high-performance distributed programs with a
highly irregular behavior: branch-and-bound search, computation on
sparse matrices, etc. In one word: In Search of Lost Time, <EM>À
  la recherche du temps perdu!</EM> You may remember that a
<EM>Madeleine</EM>, a typically French (delicious) cookie, played a
central role in the life of <EM>Marcel</EM> Proust...
</P>
  
<P>
On the client side, the parameters of the service are <EM>packed</EM>
together between the <TT>pm2_rawrpc_begin</TT> and the <TT>pm2_rawrpc_end</TT>
calls, using the <TT>pm2_pack_*</TT> routine family. You can pack integers,
byte arrays, etc. (even pointers!) very much as in PVM or PMI.  On the
server side, they are <EM>unpacked</EM> within the service function.
Again, the service function has no way of guessing what kind of
parameters have been packed by the client, nor how many of them have
been packed. In short, PM2 messages are <EM>not self-described</EM>.
This choice is motivated by performance considerations: including the
description of the objects together with the objects into the messages
yields a significant overhead for small messages. In this respect, PM2
follows the choice of many other communication interfaces. The
counterpart is that the user is responsible for the consistency of the
series of packing and unpacking actions. No verification is made at
the level of PM2, and unspecified behavior will result in any kind
of inconsistency in types and numbers. The user specifies the end of
reception on the server side by a call to the <TT>pm2_rawrpc_waitdata</TT>
routine.
</P>

<P>
Let us now run the program.
</P>

<PROGRAM FILE="./Progs/pm2load3.txt"/>

<WARNING>
  <EM>
  Program <REF LABEL="prog:rpc-params"/> and similar programs may not work
  properly when used with communication interfaces which require some
  specific alignment for communication buffers: for instance,
  BIP/Myrinet and SISCI/SCI. In this case, variable <TT>s</TT> should
  be aligned to a proper boundary. This can be done for instance as
  follows using the specific attributes of <TT>gcc</TT>:
  </EM>

  <PROGRAM FILE="./Progs/aligned.c"/>
</WARNING>

</SUBSUBSECTION>

<SUBSUBSECTION TITLE="Packing and unpacking data">

<P>
The packing/unpacking API of <MU/>PM2 is identical to the Madeleine
API presented in Section <REF LABEL="sec:mad-pack"/>. The sending and
receiving flags for <MU/>PM2 are as follows:
<TT>SEND_SAFER</TT>, <TT>SEND_LATER</TT>, <TT>SEND_CHEAPER</TT>, <TT>RECV_EXPRESS</TT>, <TT>RECV_CHEAPER</TT>. 
</P>

<P>
On the sender side, <TT>pm2_rawrpc_end</TT> is used to signal the end
of the data to be packed, and on the receiver side,
<TT>pm2_rawrpc_waitdata</TT> signals the end of the reception of the
data.
</P>

<P>
Note that the PM2 primitives to send data to a remote service are written on top
of the Madeleine primitives.
</P>

</SUBSUBSECTION>

<SUBSUBSECTION TITLE="Threaded services">

<P>
Service handlers are executed sequentially. As long as a service
function has not returned on the server, the service is not available
for another client. This may result into deadlocks if the service
includes some potentially blocking actions, such as invoking other
services, either explicitly or implicitly.
</P>

<PROGRAM FILE="./Progs/rpc-threads.c" TITLE="Spawning a fresh thread to serve a remote request" LABEL="prog:rpc-threads"/>

<P>
Such problems can be addressed by executing services into a fresh
thread instead of the original service handler. This technique is
demonstrated in Program <REF LABEL="prog:rpc-threads"/>. The service function
<TT>service</TT> spawns a fresh thread each time it is invoked
through a RPC request. Then, it immediately returns. The child thread
is in charge of extracting the message from the network through a series
of <TT>unpack</TT> calls in the usual way. 
</P>

<P>
Observe that PM2 does not care about <EM>who</EM> extracts a message at a
node. Any thread can do it, as soon as it has been created with a
special creation routine <TT>pm2_service_thread_create</TT>, instead of the
regular <TT>pm2_thread_create</TT> one. The only requirement enforced within
the runtime system is that the same thread unpacks the series of data
pieces and calls the <TT>pm2_rawrpc_waitdata</TT> routine. Having two or
more threads extracting data from the network concurrently would
actually not make sense: it may raise a run-time error.
</P>

<NOTE>
  <EM>
  Luc to Olivier + Raymond: Is it right? A sentence should probably be
  inserted here to warn that this behavior may be revised in the
  future versions.
  </EM>
</NOTE>

<P>
This feature enables the newly spawn thread to do the work on behalf
of the original service handler. Yet, this raises a new
problem. Actually, the service handler immediately returns after
spawning the thread, and then looses any control on its
progression. Thus, the <TT>pm2_rawrpc_end</TT> routine on the client side
may return <EM>before</EM> the service thread has even started any
unpacking whatsoever! 
</P>

<P>
This may result into an incorrect behavior, as demonstrated by the
following scenario. Consider that the client is very fast. After
exiting the <TT>pm2_rawrpc_end</TT> routine, it calls the <TT>pm2_halt</TT>
routine. This triggers the broadcasting of a termination request to all the
nodes. Assume now that the messages used for this broadcast do not
travel on the same <EM>channel</EM> as the regular messages, so that
they can take over and arrive <EM>before</EM> the service thread
initiates the unpacking. Then, all the reception facilities of the
server node are closed down. When the daemon service thread finally
calls the <TT>pm2_unpack_*</TT> routine, nobody remains living here to
physically extract the data from the network, and a run-time error
results!
</P>

<P>
Well, you may argue that this scenario is just like SF and little
green men attacking the White House... The writer's personal
experience is that the worst case is quite common in this matter, in
opposite<SPACE/>(?) to other domain of life. Just don't try it!
</P>

<P>
How can one circumvent the problem? A generic technique called
<EM>completion</EM> will be introduced later. In the specific example
shown in Program <REF LABEL="prog:rpc-threads"/>, a simple way-out is to let
the service thread issue the call to the <TT>pm2_halt</TT> routine, as it is
guaranteed that this thread issues the inter-thread last interaction.
</P>

<P>
Let us now run the program.
<IGNORE>
 Observe that 3 threads have been
created on node<SPACE/>1, instead of the regular<SPACE/>2.
</IGNORE>
</P>

<PROGRAM FILE="./Progs/pm2loadrpc2.txt"/>

<P>
To have a bit more fun on closing this part, let us design a slightly
more complex program, as the one on Program <REF LABEL="prog:rpc-threads1"/>.
Node<SPACE/>0 issues a RPC request to node<SPACE/>1. On node<SPACE/>1, servicing this
request consists in spawning a fresh thread and re-issuing the request
to node<SPACE/>2 after having signed it with its name.  Similarly for node<SPACE/>2
with respect to node<SPACE/>0. Finally, node<SPACE/>0 serves the request by printing
the string to the standard output and triggers termination.
</P>

<PROGRAM FILE="./Progs/rpc-threads1.c" TITLE="Spawning threads to serve requests, an extended example" LABEL="prog:rpc-threads1"/>

<P>
Executing this program should produce a output as follow
</P>

<PROGRAM FILE="./Progs/pm2loadrpc3.txt"/>

<P>
Now is a good point to clarify the notion of <EM>node</EM> used
throughout this presentation. Actually, the nodes we are considering
here are <EM>virtual</EM>: they only refer to Unix processes located
on physical nodes. It looks like common sense practice to use exactly
one virtual node per physical node, so that the two notions just match
together. However nothing in PM2 requires such practice. PM2 virtual
nodes may be located at any physical nodes. The association between
PM2 virtual and physical node is done using the <TT>pm2-conf</TT>
command. For instance, saying
</P>

<PROGRAM FILE="./Progs/pm2conf4.txt"/>

<P>
is fully correct. Two PM2 processes will be launched on each
of the three machines. From the point of view of PM2 programs, this
makes no difference.
</P>

<P>
You may even start all the processes on the same machine!
</P>

<PROGRAM FILE="./Progs/pm2conf5.txt"/>

<P>
Also, the running machine does not have to be the one you are logged
on. Any machine you have access to can do it as well!
</P>

<PROGRAM FILE="./Progs/pm2conf6.txt"/>

</SUBSUBSECTION>

<CLEARPAGE/>
</SUBSECTION>

<SUBSECTION TITLE="A minimal NewMadeleine program: Hello World!">

<P>
NewMadeleine provides two API's : a send/receive interface similar to
the point-to-point communication interface in MPI and a pack/unpack
interface similar to the one provided by Madeleine. Programs
<REF LABEL="prog:nmHello"/> and <REF LABEL="prog:nmSrhello"/> show
minimal NewMadeleine programs. You can compile and execute them.
</P>

<PROGRAM FILE="./Progs/compileNmad.txt" TITLE="Compiling and executing NewMadeleine applications"/>
<PROGRAM FILE="./Progs/gs_pack_hello.c" TITLE="NewMadeleine application using the pack/unpack interface" LABEL="prog:nmHello"/>
<PROGRAM FILE="./Progs/gs_sr_hello.c" TITLE="NewMadeleine application using the send/receive interface" LABEL="prog:nmSrhello"/>

<SUBSUBSECTION TITLE="Initialisation and termination">

<IGNORE>
<P>
NewMadeleine applications can be started using different bootstrap
mechanisms, either the leonie bootstrap (See Section
<REF LABEL="Sec:leonie"/>), or PadicoTM, or your own bootstrap. The
core of NewMadeleine is completely independant on how the application
has been started. To help end-users, NewMadeleine provides an
initialisation interface which can be implemented on top of any
bootstrap mechanism.
</P>

<P>
Programs <REF LABEL="prog:nmHello"/> and <REF LABEL="prog:nmSrhello"/> use
this interface, but there is no requirement to do so. Users can choose
to ignore this interface and start their applications <EM>by
    hand</EM>. One just need to make sure the different internal
NewMadeleine objects are correctly initialised.
</P>

<P>
When using this interface, the following functionnalities are
available:
<UL>
<LI> <TT>int nm_so_init(int *argc, char	**argv);</TT> which
  initialises all the necessary NewMadeleine components.
</LI>
<LI> <TT>int nm_so_exit(void);</TT> which terminates a NewMadeleine
  application.
</LI>
<LI> <TT>int nm_so_get_rank(int *);</TT> which returns the rank of
  the local process
</LI>
<LI> <TT>int nm_so_get_size(int *);</TT> which returns the number of
  processes in the application
</LI>
<LI> <TT>int nm_so_get_sr_if(struct nm_so_interface **sr_if);</TT>
  which returns the send/receive interface.
</LI>
<LI> <TT>int nm_so_get_pack_if(nm_so_pack_interface *pack_if);</TT>
  which returns the pack/unpack interface.
</LI>
<LI> <TT>int nm_so_get_gate_out_id(int dest, nm_gate_id_t
    *gate_id);</TT> which returns the gate id to send data to the
    specified process.
</LI>
<LI> <TT>int nm_so_get_gate_in_id(int dest, nm_gate_id_t
    *gate_id);</TT> which returns the gate id to receive data from
    the specified process.
</LI>
</UL>
</P>
</IGNORE>

</SUBSUBSECTION>

<SUBSUBSECTION TITLE="Communication interface">

<SUBSUBSUBSECTION TITLE="The send/receive interface">

<P>
The send/receive interface mainly provides methods to send or receive
data in a asynchronous way, and to test and to wait for the completion
of a communication request.
</P>

</SUBSUBSUBSECTION>

<SUBSUBSUBSECTION TITLE="The pack/unpack interface">

<P>
A call to <TT>nm_begin_packing()</TT> initialises a sending
connection. Data can then be packed into this connection using
succcessive calls to the
function <TT>nm_pack()</TT>. <TT>nm_end_packing()</TT> ends the
building of the message and flush the data.
</P>

<P>
On the receiving side, <TT>nm_begin_unpacking()</TT> starts
receiving and extracting a new message, data can be extracted from the
current message using successive calls to <TT>nm_unpack()</TT> and
finally, <TT>nm_end_unpacking</TT> ends the reception of the
message.
</P>

<P>
Users have to be careful when handling data involved in a
communication. Packed data should not be modified between the call
to <TT>nm_pack(...)</TT> and <TT>nm_end_pack(...)</TT>, as it
may modify the message contents. To overcome this restriction, a call
to the function <TT>nm_flush_packs(...)</TT> will only return once
ongoing send requests have completed. Similarly, on the receiving
side, the extraction of the data may be defered until the call
to <TT>nm_end_unpacking(...)</TT>. A call
to <TT>nm_flush_unpacks(...)</TT> will insure previously requested
data are available.
</P>

</SUBSUBSUBSECTION>

</SUBSUBSECTION>

<SUBSUBSECTION TITLE="The MPI interface">
<P>
A minimal MPI implementation has been developed on top of
NewMadeleine. More informations on this implementation are available
at the following URL: <URL>http://runtime.bordeaux.inria.fr/MadMPI/</URL>.
</P>
</SUBSUBSECTION>

</SUBSECTION>

<SUBSECTION TITLE="Leonie" LABEL="Sec:leonie">

<P>
So far, we used <TT>pm2-conf</TT> to specify which machines to run applications on, and <TT>pm2-load</TT> to actually start the application. These tools are written on top of and use Leonie, the PM2 bootstrap code, which reads the configuration files and then bootstraps the application. Leonie uses two configuration files:
</P>

<OL>
<LI>
<P>
<EM>The application configuration file</EM> which describes the machines to use for the application,
</P>
</LI>

<LI>
<P>
<EM>The network configuration file</EM>, as explained in Section <REF LABEL="sec:network-conf"/>.
</P>
</LI>

</OL>

<SUBSUBSECTION TITLE="The configuration files">

<P>
Let's explain these concepts using a realistic example. Let's suppose that two clusters
are available:
</P>

<IMAGE FILE="./Figures/config-small" TITLE="Example of interconnected clusters" SCALE="0.5" LABEL="fig:config-small"/>

<UL>
<LI>
the first cluster named <TT>foo</TT>, is composed of 3 nodes, named
<TT>foo0</TT>, <TT>foo1</TT> and <TT>foo2</TT>, linked by a Myrinet network;
</LI>

<LI>
the second cluster named <TT>goo</TT>, is composed of 4 nodes, named
<TT>goo0</TT>, <TT>goo1</TT>, <TT>goo2</TT>, <TT>goo3</TT>, linked by a SCI network.
</LI>
</UL>

<P>
Both clusters also feature an Ethernet network (TCP) which links
together all the machines of each cluster. The clusters can be seen on
Figure <REF LABEL="fig:config-small"/>.
</P>

<P>
Leonie uses objects called <EM>channels</EM> in order to virtualize
the available networks in a given configuration. There are basically
two types of channels:
</P>

<OL>
<LI> <EM>physical channels</EM> which are simple abstractions of real
existing networks and; </LI>

<LI> <EM>virtual channels</EM> which are build above physical channels
and can be used to create <EM>heterogeneous networks</EM>. </LI>
</OL>

<P>
With our simple example, we can build three physical channels:
</P>

<OL> 
<LI> a channel build above the Myrinet network. This channel
encompasses the nodes {<TT>foo0</TT>, <TT>foo1</TT>, <TT>foo2</TT>};
</LI>

<LI> a channel build above the SCI network. This channel
encompasses the nodes {<TT>goo0</TT>, <TT>goo1</TT>, <TT>goo2</TT>, <TT>goo3</TT>};
</LI>

<LI> a channel build above the  TCP network. This channel
encompasses all the nodes of both clusters. </LI>
</OL>

<P>
On top of these three different physical channels, we can build a
virtual channel which encompasses all the nodes of the
configuration. One may think that there is no difference with the TCP
physical channel, but in fact the behavior of a program using the
virtual channel will be totally different as Leonie will
automatically select the best available network to communicate between
two nodes of this virtual channel.
</P>

<P>
Indeed, all communications occurring within the <TT>foo</TT> cluster
will use the Myrinet network, all communications occurring within the
<TT>goo</TT> cluster will use the SCI network.  And if two nodes
belonging to different clusters want to exchange messages, the TCP
network will be used.
</P>

<P>
More complicate configurations can be expressed: if we suppose now
that the node <TT>goo3</TT> features a SCI NIC, we can build a virtual
channel over the physical channels corresponding to the Myrinet and
SCI networks. In that case, we do not need (and most important: use)
the TCP network. In fact, with that new configuration, from the
application's point of view, all the nodes can communicate with each
other. Indeed even if a node <TT>A</TT> is not <EM>physically</EM>
connected to a node <TT>B</TT>, it can in any case send messages to
it. Internally, the node <TT>goo3</TT>, which features both Myrinet
and SCI NICs, will <B>forward</B> the Madeleine message from
<TT>A</TT> to <TT>B</TT>.
</P>

<IMAGE FILE="./Figures/config" TITLE="Example of configuration" SCALE="0.7" LABEL="fig:config"/>

<P>
How is this information given to Leonie/Madeleine? The library uses
configuration files (the following example files describe the
configuration shown in Figure <REF LABEL="fig:config"/> with the node
<TT>goo3</TT> featuring a SCI NIC):
</P>

<UL>
<LI>
<P>
the first file, the network configuration file, named
<TT>localnet.cfg</TT>, describes the different available networks.
</P>
<PROGRAM FILE="./Progs/localnet.cfg"/>
<P>
A network is defined with its name (tag <TT>name</TT>), a list of
machines it includes (tag <TT>hosts</TT>), and the identifier of the
device connecting these machines (tag <TT>dev</TT>, should be one of
the predefined identifiers recognized by Madeleine).
</P>
</LI>

<LI>
<P>
the second file, the application configuration file, named
<TT>appli.cfg</TT>, describes the application and the channel mapping
over the different nodes.
</P>
<PROGRAM FILE="./Progs/appli.cfg"/>
<P>
A application is defined with a flavor (tag <TT>flavor</TT>), a
network configuration file (tag <TT>include</TT>), a list of physical
channel and a virtual channel.
</P>
<P>
A physical channel is defined with its name (tag <TT>name</TT>), the
identifier of the network is based on (tag <TT>net</TT> which has to
be defined in the network configuration file), and a list of machines
it encompasses (tag <TT>hosts</TT>).
</P>
<P>
A virtual channel is defined with its name (tag <TT>name</TT>) and the
list of physical channels it is build upon (tag <TT>channels</TT>).
</P>
</LI>
</UL>

<P>
Once a virtual channel is build, the physical channels below it are no
longer visible by the application. However, it is possible to create
several different Madeleine physical channels over the same physical
network. Hence a physical channel can truly be seen as a <EM>logical
network</EM> or a <EM>network abstraction</EM>.
</P>

</SUBSUBSECTION>

<SUBSUBSECTION TITLE="Starting a PM2 application using Leonie">

<P>
Once the configuration files describing your machines are written, a
simple call of <TT>leonie</TT> to start an application would be:
</P>

<PROGRAM FILE="./Progs/leonie1.txt"/>

<P>
<TT>leonie</TT> accepts different parameters. The call <TT>leonie
--help</TT> shows the list of all these parameters.
</P>


</SUBSUBSECTION>
</SUBSECTION>

</TEXT>
