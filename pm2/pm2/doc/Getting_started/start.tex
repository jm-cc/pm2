\documentclass[a4paper,11pt]{report}

\usepackage{start}

\begin{document}

\title{Getting started with PM2}

\author{Olivier Aumage \and Gabriel Antoniu \and Luc Boug\'e \and
  Vincent Danjean \and Raymond Namyst}

\date{\footnotesize%
\url{$Id: start.tex,v 1.3 2001/04/02 14:47:48 bouge Exp $}%
}

\maketitle

{\small \parskip = 0pt%
\tableofcontents
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introducing PM2}

\section{What is PM2?}

PM2 is developed at LIP (\emph{Laboratoire de l'Informatique du
  Parallélisme}), a research laboratory located at the ENS Lyon
(\emph{\'Ecole Normale Sup\'erieure de Lyon}) France, jointly
supported by the INRIA and CNRS institutions. PM2 was originally
developed at LIFL, University of Lille, France.
  
PM2 (\emph{Parallel Multithreaded Machine}) is a distributed
multithreaded programming environment designed to efficiently support
irregular parallel applications on distributed architectures of SMP
nodes. PM2 is primarily designed for medium-size clusters of commodity
processing nodes interconnected by high-performance networks. However,
nothing in the design of PM2 prevents from using it on massively
parallel MIMD machines at one end of the spectrum, or as a support for
metacomputing over the Internet on the other end. Actually, a new
version of PM2's communication library has been developed to support
heterogeneous networking configurations, such as a set of
interconnected clusters.

The thread management subsystem of PM2 is called Marcel. It is
designed to manage several hundreds of threads on each available
physical processor. The PM2 programming interface provides
functionalities for the management of this high degree of parallelism
and for dynamic load balancing. Interesting features of PM2 include
its priority driven scheduling policy, its thread migration mechanisms
and its ability to support various load balancing policies. PM2 has
been design to provide threads as light as possible: the switching
time is well under the micro-second. Yet, Marcel is completely written
in portable C, but a dozen of lines of assembly code, which makes it
portable across most processors and flavors of Unix systems, including
Solaris and Linux. However, significative improvements could been made
for Linux by introducing specific support into the operating system.

The communication subsystem of PM2 is called Madeleine. It is
specifically designed to provide PM2 threads with efficient RPC
facilities on modern high-performance networks. In particular,
Madeleine strives at providing zero-copy communication in user-space
so as to obtain a minimal latency and a maximal bandwidth: less than
10~\us latency and Gigabit/s bandwidth have been reported for
Myrinet networks.  Yet, the design has been careful enough to keep
Madeleine portable across a wide range of communication interfaces,
including TCP and MPI. This makes PM2 portable on most Unix
platforms.

\subsection{The PM2 Programming model}

PM2 adheres to the SPMD (\emph{Single Program Multiple Data})
programming model, in a way very similar to the PVM and MPI
communication libraries.  The user writes a single program text, a
copy of which is launched by a specific \|load| command on each
\emph{processing node} of the current configuration. Then,
it is up to this common program text to include branching so as to
differentiate between the processing nodes, based on a programming
scheme like
\begin{program}
if(pm2_self() == 0) { /* Do something... */ }
else { /* Do something else... */ }
\end{program}
At this level of presentation, a processing node is simply a Unix
process. The association between \emph{processing nodes} and
\emph{physical SMP nodes}, or even \emph{physical processors of the
  SMP nodes} are made outside of the PM2 model by specific commands
described below.


On each node, the \|pm2_main| function of the program is activated.
This function may then \emph{spawns} (i.e., create) an arbitrary
number of children threads.  PM2 adheres as much as possible to the
POSIX Threads programming model. A PM2 thread is just declared as an
ordinary function. The only difference is that it is called through a
certain library routine, which makes it run \emph{concurrently} to its
caller function. The caller and the callee share all the global
variables of the local copy of program. Yet, the callee is launched
with a fresh stack. This implies that there is no return for a
function launched as a thread. On returning from the initial
activation of its function, the thread merely dies.  Observe also that
the caller and the callee functions run concurrently: once launched,
there is no longer any notion of a \emph{father}, nor a \emph{son},
and the set of threads is just flat: if one of the running threads
spawns a new thread, then it is just added to the current, flat pool
of running threads, whatever its creator.  Also, any thread may
terminate the program, whatever its rank of creation.

\subsection{Threads and RPCs}

Once spawned, a thread runs until its function returns. Threads living
on the same node may synchronize by a number of means, as defined by
the POSIX Threads model: semaphore, monitors, etc. Threads living on
distinct nodes may not directly interact together. The only way a PM2
thread may interact with a remote node is to issue a \emph{remote
  procedure call} (RPC): the \emph{client} thread requires the
execution of a \emph{service} function on the remote, \emph{server}
node. PM2 provides the programmer with many flavors of RPCs. The
client thread may or may not wait for the result of the function; the
argument of the service function may be packed and unpacked in various
ways, depending on their use; the server node may or may not create a
new, fresh thread to execute the service function. This latter
alternative deserves some comments. In many thread package, the
default behavior is that a fresh thread is spawn on the server node to
execute the service function, so that the node is left available to
handle further requests. However, such a behavior induces an
additional cost which may be prohibitive with respect to the requested
service.  The PM2, the server node is left to execute the service
function \emph{in person} so as to save this overhead. As a
side-effect, this keeps the node busy and unable to serve any further
request: it is up to the programmer to make sure that no deadlock may
result. Of course, service functions may also dynamically spawn
threads if necessary, getting back to the usual behavior.

Synchronization between threads is a crucial aspect of multithreaded
programming. If the threads live on the same node (that is, within the
same Unix process), then threads can synchronize through \emph{locks},
according to the POSIX Threads model. If they lives on distinct nodes,
then PM2 defines a special object called a \emph{completions}. In
short, a completion is a special, self-described token which can be
generated by a thread, and then forwarded to some other threads. The
originating thread can then \|wait| for the completion to be sent back
to him using a special \|signal| routine. This routine may be executed
by any thread, including a thread living at the same node as the
originator. 


\subsection{Starting and stopping programs}

How does a PM2 SPMD distributed program start and stop? This is a
difficult problem, which requires much attention from the programmer
as it is the never-ending source of subtle failures (the writers have
painfully experienced it!). The execution of a PM2 program can be
divided in three phases.
\begin{description}
  
\item[Prelude.] The prelude starts on launching the \|pm2_main| root
  function of the program. This function should include all necessary
  initializations, especially the registration of the service
  functions which may be activated by remote clients
  (\|pm2_rawrpc_register| routine), and more generally all objects
  which may be seen from remote nodes. This phase ends with a call to
  the \|pm2_init| routine, which spawns the servicing threads to
  listen to external requests (RPC, migration, etc.).
  
\item[Main part.] After returning from the \|pm2_init| routine, all
  PM2 nodes can freely proceed, behaving both as a client and a
  server. In general, it is considered as \emph{literate PM2
    programming} to let only node~0 to be active, all the other nodes
  waiting for incoming requests.
  
\item[Postlude.] Each PM2 node should eventually issue a call to the
  \|pm2_exit| routine. This function waits for \emph{all} local
  threads to terminate, including the local internal system threads
  spawned by the \|pm2_init| function to serve the remote RPC
  requests.  Again, in \emph{literate PM2 programming}, all nodes but
  node~0 should immediately call the \|pm2_exit| function after the
  \|pm2_init|, just waiting for incoming requests through internal
  service threads. A call to \|pm2_exit| should be issued
  \emph{exactly once by each node}: unpredictable behavior may result
  from multiple calls to this routine at a ingle node. However, any
  thread of the node may call it: there is no reason why this should
  necessarily be done by the initial thread.

\end{description}
Of course, such a scheme does not fully meet our needs: the system
eventually reaches a state where only one thread remains living at
each node, blocked within a \|pm2_exit| call, all the service threads
waiting for incoming requests. So to speak, the program has
terminated, but nobody knows it for sure, so that it may not stop!
This situation is well-known in the field of distributed algorithms:
it has been called \emph{distributed termination}. PM2 solves the
problem by providing the programmer with a special routine called
\|pm2_halt|.  This routine should be called \emph{exactly one within
  all the nodes}. By issuing such a call, the programmer commits that
no request remains in transit and that no further request shall ever
be issued. The \|pm2_halt| routine broadcasts a signal to all the
nodes of the configuration, whose effect is to stop all the system
service threads. Then, the \|pm2_exit| routine can finally return at each
node, and the local copy of the program eventually exit. Again,
issuing multiple calls to the \|pm2_halt| routine, or issuing it
before all requests have been handled will most probably lead to a
deadlock.

\begin{note}
  Luc to Olivier: Please, check the above section *very* carefully!
\end{note}

\subsection{Migrating threads}

An interesting feature of PM2 is that it provides a mechanism for
\emph{preemptive thread migration}. The principle is as follows. A any
point, a thread can request from the system the list of the threads
living on the same node, pick up a thread (including itself!)  for
migration to some remote processing node and thrigger its effective
migration.  On such a request, the system suspends the selected thread
and removes it from the scheduler queue. The thread is marshaled
(i.e., its memory image is copied) into a buffer and physically
deleted at the node. A RPC is then issued to the remote node which has
been specified as destination for the migration. The service function
takes the buffer as its argument. On the destination node, this
function unmarshals the buffer, installs the thread and finally
inserts it into the scheduler queue, ready to run.

Seen from the thread's point of view, the migration is completely
transparent, very much like a context switch. The only difference is
that only the \emph{proper resources} of the thread have been
migrated: its descriptors, its stack and the memory areas which have
been allocated through a specialized \emph{iso-allocation} routine
called \|pm2_isomalloc|.  Thanks to this specialized
\emph{iso-allocation} facility, all the thread's resources are
relocated at the emph{same virtual addresses} upon a migration, so
that pointers remain valid.  Yet, no provision is made at this level
for the data shared by the thread with other local threads, for
instance the global data of the program image running at the node. It
is up to the programmer to make sure that the resulting behavior is
correct.

The concept of migration has been introduced above for one single
thread. In fact, PM2 provides a general migration facility which
enables the programmer to migrate \emph{several threads} at a time,
each thread being migrated to a specific processing node. Also, it
should be stressed that the series of operations described above at
the origin node can be made atomic if desired, using specific PM2
routines.

PM2 features an additional functionality to provide threads with a
uniform access to data, whatever their physical location. It is called
DSM-PM2. The idea is that the programmer can register certain global
data in to be \emph{uniformly shared} by all the threads. This can be
done \emph{statically} by using a special declaration for the global
variables of the program, or \emph{dynamically} by allocating memory
areas with a specific allocation function. Exactly like the
\|pm2_isomalloc| function above, the system guarantees that all
pointers remain valid on all nodes, so that these memory areas can be
used completely transparently by the programmer.  DSM-PM2 is
parametrized by the consistency protocol to be used for managing
shared data. A number of built-in protocols are provided in the
standard distribution for various consistency models, including
\emph{sequential consistency} ones (for instance, Li-Hudak), and
\emph{release consistency}, with several alternative protocols for
each of them. The protocol to be used is actually specified within the
prelude phase (before the call to \|pm2_init|) using a special routine
called \|dsm_set_default_protocol|. The user may thus swtich from one
protocol to another without recompiling the program! Also, additional
protocols can be dynamically constructed by the user using the basic
blocks provided by the library. A typical protocol may be specified at
this level with a few hundreds of lines.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Installing PM2}

This section describe how to install PM2 on your own machine.
Please, first check that your system configuration fits the
installation pre-requisites, especially with respect to the GNU
utilities as mentionned in Section~\ref{sec:tools}.

\subsection{PM2 pre-requisites}

PM2 is a highly portable and efficient environment and the current
software is yet available on a wide range of architectures. The
implementation is built on top of several distinct software
components: Marcel and Madeleine. we briefly review the currently
supported architectures and systems. 

\begin{note}
  Luc: There should be something alike for Madeleine.
  \|${PM2_ROOT}/INTERFACES|?
\end{note}

\subsubsection{Multithreading: Marcel}

Marcel is a POSIX-compliant thread package which provides extra
features, such as thread migration.  Marcel is currently available on
the following platforms, listed in the \|ARCHITECTURES| file of the
distribution:
\begin{shell}
ravel% more ${PM2_ROOT}/ARCHITECTURES
AIX/RS6K
FREEBSD/X86
IRIX/MIPS
LINUX/ALPHA
LINUX/PPC
LINUX/X86
OSF/ALPHA
SOLARIS/SPARC
SOLARIS/X86
UNICOS/ALPHA
\end{shell}


\subsubsection{Communication: Madeleine}

PM2 was initially designed with a preliminary version of the
Madeleine. This version is now obsolete and not maintained any longer;
it is referred to as the Madeleine~I. The current version of the
interface is called Madeleine~II. The internal design is completely
different, and the new version is only partly compatible with the
preliminary one. This manual only introduces the Madeleine~II
communication interface, simply referred to as Madeleine hereafter
when no confusion may occur.

Madeleine (that is, Madeleine~II) is a generic communication interface
which is able to fully exploit the low latency and the high bandwidth
of high-speed networks such as Myrinet or SCI. This PM2 communication
subsystem currently supports the following communication interfaces:
TCP, MPI (LAM-MPI, MPI-BIP), VIA, BIP (on top of Myrinet), SISCI (on
top of SCI) and SBP (on top of Fast-Ethernet).

\subsubsection{Required development tools}
\label{sec:tools}

As previously mentionned, PM2 as been designed to be easily portable
and only relies on the availability of the two following development
tools:
\begin{itemize}
  \small
\item GNU C Compiler \|gcc| (versions \|2.7.2.3|, \|2.8.1| and higher,
  or \|Egcs| versions);
\item GNU \|make| (versions \|3.7x.x|).
\end{itemize}
Also, PM2 includes a large number of shell scripts, which are assumed
to be run with the \emph{GNU utilities}. This is specially important
for \|expr|, \|tr|, \|head|, \|cut|, \|test|, etc. (version 2.0 or
higher), \|grep| (version 2.3), \|tar|, etc. Please, make sure that
you actually use the right version of these utilities!
 
\begin{note}
  Luc: There should be a way of listing all utilities used in the
  scripts.
\end{note}

\begin{note}
  Luc: I do not understand what \|Egcs| means here. Olivier, please
  correct.
\end{note}

\subsection{Getting the PM2 software and help}

The PM2 archive containing all the necessary sources is available
from the PM2 web site at:
\begin{quote}
  \url{http://www.pm2.org/}
\end{quote}

Please, send an email to \url{pm2-users@ens-lyon.fr} if you encounter
problems with the PM2 software. However, please make sure first that
your problem is not already listed in the \emph{Frequently Asked
  Questions} Section~\ref{sec:faq}.

\subsection{Unpacking the PM2 distribution}

The PM2 archive can be extracted with one of the following commands
entered at the shell prompt:
\begin{shell}
ravel% tar xvfz pm2.tar.gz         # Using the GNU tar utility
\end{shell}
Once extracted, the PM2 distribution should be available under the
\|./pm2/| directory. This PM2 distribution is organized as follows:
\begin{center}
\begin{tabular}{|l|l|}
\hline 
\|bin| &        PM2 scripts \\
\|common| &     ??? \\
\|console| &    ??? \\
\|doc| &        Documentation files \\
\|dsm| &        Distributed shared memory manager \\
\|ezflavor| &   Flavor configuration graphic interface \\
\|generic| &    ??? \\
\|leonie| &     ??? \\
\|leoparse| &   ??? \\
\|mad2| &       Communication subsystem \\
\|make| &       PM2 makefiles \\
\|marcel| &     Multithreading subsystem \\
\|modules| &    ??? \\
\|ntbx| &       Generic network managment utility toolbox \\
\|pm2| &        PM2 \emph{en personne}! \\
\|pm2debug| &   PM2 debugging utilities (useful!) \\
\|profile| &    PM2 profiling utilities \\
\|sigmund| &    ??? \\
\|swann| &      ??? \\
\|tbx| &        Generic PM2 utility toolbox \\
\hline
\end{tabular}
\end{center}
The \|mad1| directory contains the obsolete Madeleine~I communication
subsystem for backward reference, but should not be used any
longer. Just ignore it.

\begin{note}
  Luc to Olivier: Please fill up the holes!
\end{note}

\subsubsection{Environment variables}

Several environment variables have to be set so that PM2 correctly.
\begin{itemize}
  
\item First, the \|PM2_ROOT| variable must contain the path to the PM2
  distribution root directory. 
  
\item Second, the \|${PM2_ROOT}/bin| directory has to be included in
  the active search path.
  
\item Finally, all the files generated within the compilation process
  will be placed by default in the \|${HOME}/build| directory. Though
  this may be convenient in a first place, you may wish to keep these
  files in a more dedicated place. Just define the \|PM2_BUILD_DIR|
  shell variable to the desired value. Just note that this directory
  has to be reachable from any machine used in running PM2 programs.
  Be careful that \|/tmp| does not work in general!

\end{itemize}
Please make sure that these two variables are set correctly within
remote shell commands (i.e., scripts invoked by \|rsh|).  Thus,
you may insert the following lines in your \|.cshrc| file (if your
default shell is \|csh|):
\begin{shell}
ravel% setenv PM2_ROOT          ${HOME}/pm2
ravel% setenv PATH              "${PATH}:${PM2_ROOT}/bin"
ravel% setenv PM2_BUILD_DIR     ${HOME}/build  
\end{shell}
  
\subsection{Creating standard PM2 flavors}
\label{subsec:compiling}

You should now create your own, private configuration database. It is
stored in the \|${HOME}/.pm2| file in your home directory. It is made
of a list of \emph{flavors}. A flavor is simply a (rather complex!)
series of options to be passed to the PM2 modules at compile-time
and/or at run-time. Flavors are organized so as to let you easily
maintain several versions of the PM2 system at the same time: for
instance, one compiled for the TCP protocol and one compiled for the
BIP/Myrinet protocol. Or, one compiled with the debugging checks on,
and one with the checks turned off for performance. Do not forget to
re-source your \|.cshrc| file to activate the new environment
variables and the new search \|PATH|.
\begin{shell}
ravel% source ${HOME}/.cshrc           # If you are running csh
ravel% cd ${PM2_ROOT}
ravel% make init
\end{shell} 
This checks the whole source hierarchy for consistency extracting the
option sets for each PM2 module if necessary. Then, this creates the
the various flavors. You can just list them by
\begin{shell}
ravel% pm2-flavor list
\end{shell}

In the following, we will advise you to use the \|pm2| standard
flavor. Just have a look at it to get a feeling of your current
flavor. You'll probably never dare do it again!
\begin{shell}
ravel% pm2-flavor get --flavor="pm2"
\end{shell}
There are many more way of using the \|pm2-flavor| command. See
\|pm2-flavor -h| for the extensive on-line help.

\subsection{Selecting your own PM2 flavor}
\label{subsec:configuring}

Configuring PM2 is quite straightforward. The underlying platform is
automatically detected. Hence, there is no need to specify the
operating system/processor pair.

Configuring flavors is done through a set of external utilities to be
introduced later. We strongly discourage you to edit the flavor files
by hand.

The only interesting setting at this time is the underlying network
protocol selection, which is currently not desirable to be
auto-detected by the Madeleine~II communication library of PM2.  It is
set to TCP by default in the \|pm2| flavor, which should fit your
needs for the very first run at least. 
\begin{shell}
ravel% setenv PM2_FLAVOR pm2
\end{shell}

\subsection{Compiling the library}

You are now ready for the big thing: compiling the whole library.  No
files are created in the \|${PM2_ROOT}| directory, so that you can
compile and install PM2 from a CDROM. All executable will be placed
into the \|${PM2_BUILD_DIR}| directory. Remember that this directory
has to be accessible from all machines included in the PM2
configuration. A local \|/tmp| directory will not work!

To launch the whole compilation, just invoke \|make|. Make sure you
use the \emph{right} version of the program (see above for detailed
specification). Only recent GNU versions will make it!
\begin{shell}
ravel% cd ${PM2_ROOT}/pm2
ravel% make
\end{shell}
You may wish to precisely follow the compilation process, just set the
\|VERB| variable of the makefile to \|verbose|.
\begin{shell}
ravel% make VERB=verbose
\end{shell}

\subsection{Cleaning out everything}

The directory tree of the PM2 distribution may be cleaned at any time
from compiled objects and libraries using the command \|pm2clean|.
This may be needed to free some disk space when PM2 is not in use or
to force the whole PM2 distribution to be completely remade by a
subsequent \|make| call.
\begin{shell}
ravel% cd ${PM2_ROOT}/pm2
ravel% make clean
\end{shell}
Note that this does not affect your private \|.pm2| flavor file, nor
your \|build| directory. If wanted, they have to be removed by hand.

\begin{note}
  Luc: Is this still correct?
\end{note}


\subsection{If everything else fails...}

If something goes wrong at any point below this point, then you can
always activate the emergency repair tool provided by PM2.
\begin{shell}
ravel% cd ${PM2_ROOT}
ravel% make sos
\end{shell} 
This result into a listing of your current configuration, attempts to
rebuild the flavor database and clears the compiled PM2 library as
cleanly as possible. It should restore a fresh and safe configuration
for you, ready to restart the whole compilation. The output of the
command may look like:
\begin{shell}
********* Checking environment variables *********
PM2_HOME = 
FLAVOR = pm2
CURDIR = .../pm2
PM2_ROOT = .../pm2
PM2_BUILD_DIR = .../pm2/build
********* Refreshing files for current flavor *********
make[1]: Entering directory `.../pm2'
Re-generating flavor pm2...
flavor 'pm2' unmodified
Cleaning for flavor pm2...
make[1]: Leaving directory `.../pm2'
Humm... Well, all should be ok now!
\end{shell}
(Note that the final message is automatically generated!)

\subsection{Compiling a PM2 program}

The PM2 library should now fully configured. Please, check again that
your flavor is actually set:
\begin{shell}
ravel% setenv PM2_FLAVOR pm2
\end{shell}
You are ready to compile the example programs of the distribution.
Let's start with the simplest one: \emph{Hello world!}, located in
file \|hello.c|.
\begin{shell}
ravel% cd ${PM2_ROOT}/pm2/examples/simple
ravel% make hello
\end{shell}

The compiled version of our program is automatically placed into your
private \|build/${PM2_FLAVOR}| directory, but you need not bother with
this detail at this point: PM2 cares about it for you! Use the
\|pm2which| command to learn where a file is physically located;
\begin{shell}
ravel% pm2which hello
\end{shell}

\subsection{Specifying the configuration for execution}

The final step before execution is to specify the list of hostnames on
which the PM2 application is going to run. For this, use the
\|pm2conf| command. My current machine is called \|ravel|, and the two
neighboring ones are called \|debussy| and \|faure| (well, this should
actually read \emph{Faur\'e}!).
\begin{shell}
ravel% pm2conf ravel debussy faure
The current PM2 configuration contains 3 host(s) :
0 : ravel
1 : debussy
2 : faure
\end{shell}
PM2 will consider that processing node~0 is a process run by \|ravel|,
node~1 by \|debussy| and node~2 by \|faure|.  Observe that I select
here my current worktation as node~0. This is for convenience only,
but nay machine could make it. Indeed, there is no reason why my local
machine should get enrolled in the execution of the program. In
running a PM2 on a dedicated cluster, the local machine is most
preferably out of the configuration.

Just to avoid disturbing problems in the sequel, please make sure that
you local machine is allowed to issue \|rsh| commands to each machine
in the configuration. For instance, try:
\begin{shell}
ravel% rsh debussy date
ravel% rsh faure date
\end{shell}
If you do not get the current date, that is, either nothing, or a
message like \|permission denied|, etc., then you are in trouble. A
possible turnaround is to use \|ssh| instead of \|rsh|:
\begin{shell}
ravel% setenv PM2_RSH ssh
\end{shell}
If this is not possible, it is still possible to run all three PM2
\emph{virtual nodes} as three processes of your local machine:
\begin{shell}
ravel% pm2conf ravel ravel ravel
The current PM2 configuration contains 3 host(s) :
0 : ravel
1 : ravel
2 : ravel
\end{shell}

\subsection{Loading the program}

It remains to actually load and run the program.
\begin{shell}
ravel% pm2load hello
[Threads : 2 created, 0 imported (0 cached)]
Hello world!
\end{shell}
Congratulations, you've run your first PM2 program! \|;-)|

You can see that our program generates two messages. The first one is
the expected \emph{Hello World!} message coming from the master
process with a call to the \|tprintf| PM2 routine, which is a simple
wrapper of the regular \|printf| Unix~routine to be used in
multithreaded context. The second one is some PM2 running statistics
related to the master process. Note that the output of the nodes
different from node~0 does not appear in the terminal. We will see
later how to get them.

\subsection{If everything else fails...}

Feedback from PM2 users showed that several common pitfalls may be
encountered at this time. If this is your case, please refer to
Section~\ref{sec:commonpitfalls}: \emph{Common Pitfalls}. Here are the
most common.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Discovering PM2}

This chapter aims at illustrating the basic features of PM2 through
some very simple programs. Next chapter will present more advanced
features that may be needed when developing \emph{real} parallel
applications.

\section{Compiling your own PM2 program}

\figurelisting {Makefile1} {A minimal GNU Makefile for PM2
  programs} {prog:Makefile1}

You may compile you own PM2 program just as a usual C~program. The
only point is to include the necessary definitions and libraries.  As
the command line is rather complex, PM2 provides a simple utility to
generate it on-line: \|pm2-config --cc| generates the name of the
adequate C~compiler (most probably, \|gcc|), \|pm2-config --cflags|
generates the necessary \|CFLAGS|, and \|pm2-config --libs| generates
the list of \|pm2-config --cflags| generate the necessary \|CFLAGS|,
and \|pm2-config --libs| generates the list of libraries to be
searched on linking. Thus, the standard command line to compile a PM2
program \|hello.c| looks like:
\begin{shell}
# Assuming you use csh...
setenv CC       "`pm2-config --cc`"
setenv CFLAGS   "`pm2-config --cflags`"
setenv LIBS     "`pm2-config --libs`" 
$CC $CFLAGS hello.c $LIBS -o hello
\end{shell}
Observe that the source file \|hello.c| should be mentioned
\emph{before} the PM2 libraries, as the external symbols are searched by
\|gcc| in the libraries from left to right.

Well... Such a command line is rather tedious to enter! You will find
on Figure~\ref{prog:Makefile} (page~\pageref{prog:Makefile}) a
\|Makefile| which does all the work for you. Executing \|make hello|
will compile source file \|hello.c| with all the necessary
parameters. 

Observe that using dynamic calls to the \|pm2config| utility garantees
that you are compiling with the suitable options with respect to the
current value of the flavor, as specified by the \|PM2_FLAVOR|
shell variable.

\section{Hello World!}

Let us start our PM2 tour by writing the traditional ``\emph{Hello
  World!}'' program. Then, we will extend it step by step to cover the
main functionalities provided by the PM2 programming interface.

\subsection{The minimal PM2 program}

\figurelisting {hello.c} {Minimal PM2 program} {prog:hello}

Figure~\ref{prog:hello} (page~\pageref{prog:hello}) shows an
example of a minimal PM2 code. Compile it and run it:
\begin{shell}
ravel% make hello
[...]   # Normally, no warning whatsoever!

ravel% pm2conf ravel debussy faure
The current PM2 configuration contains 3 host(s) :
0 : ravel
1 : debussy
2 : faure

ravel% pm2load hello
[Threads : 2 created, 0 imported (0 cached)]
Hello World!
\end{shell}
Though it looks simple, it is a full-fledged SPMD parallel program
that spawns on several processing nodes during execution! Let's
examine it step by step.

\subsubsection{Header files}

A PM2 program must always include the PM2-specific \|pm2.h| header
file, along with other standard header files.  Note that this is the
\emph{only} PM2 header file that has to be included by user
applications.

\subsubsection{The \|pm2_main| function}

The main function of the PM2 program is named \|pm2_main|, in contrast
to traditional C~programs that use the well-known \|main|
function name. In fact, the \emph{real} \|main| function of the
program is provided by the PM2 libraries. It has to set up the
execution environment before calling the user \|pm2_main| function.
This allows PM2 to greatly enhance the performance of various thread
management functions.  The arguments remain the regular
\|argc|/\|argv| pair, with their usual meaning.

Although most programs (or rather, programmers!) can accomodate such a
violation of the usual \emph{C~convention}, there exists some
applications that require to use the regular \|main| function name. In
particular, it may be the case with applications linked with non-C
code (\emph{e.g.}, Fortran code).  In this case, please refer to
Section~\ref{sec:tradimain} (page~\pageref{sec:tradimain}) for
details.

\subsubsection{PM2 initialization}

A PM2 program has to call \|pm2_init| to effectively initialize the
PM2 runtime system. Moreover, this step involves a global
synchronization phase (actually, a global synchronization barrier)
among all the PM2 nodes so that each process is assigned a unique
\emph{rank number} on returning from \|pm2_init|. In consequence, it
makes no sense to call function concerned with the global execution
environment (such as node numbers, etc.)  \emph{before} this point.
Please, refer to Section~\ref{sec:rank}, page~\pageref{sec:rank} for
details..

Most importantly, the \|pm2_init| function spawns a number of internal
\emph{thread daemons} that are in charge of listening the network and
answering to external requests such as RPC request, incoming thread
migrations, etc. A node should be ready to handle all possible
incoming requests from any other node at this point. As a consequence,
it is not safe to do any initialization \emph{after} this point, as
the user has no control about the interleaving of requests and the
relative speed of the nodes. Even though the initialization call is
placed just after \|pm2_init|, an arbitrary delay may occur between
the two successive instructions! It follows that if some
initialization code needs to be performed before any thread is
started, then this code \emph{must} be called \emph{before}
\|pm2_init|.  See Section~\ref{sec:startupfunc}
(page~\pageref{sec:startupfunc}) for a more complete discussion about
this subtle initialization topic.

Note also that the Unix standard input/output streams may not be
initialized correctly before the call to \|pm2_init|. Thus, the
behavior of programs using I/O operations before \|pm2_init| is not
defined.

Going back to our example program, note the call to the \|tprintf|
routine of PM2. This is a simple wrapper of the regular \|printf|
routine of the C~library, which is protected against multithreading
(technically, it is made \emph{reentrant} by disabling scheduling).
As mentioned previously, all nodes perform this call and actually
produce the ``\emph{Hello World!}'' string on their output stream.
Yet, only the output of node~0 is observed here.
Section~\ref{sec:output}, page~\pageref{sec:output}, describes where
these outputs actually go during execution.

\subsubsection{Who's who?}
\label{sec:rank}

Each processing node (that is, Unix process) taking part to a given
execution receives its own unique \emph{rank} number. This is an
\|unsigned int| between 0 and $\mbox{\|pm2_config_size()|}-1$. A node
can learn its own rank by calling \|pm2_self()|.

The processing node with rank~0 has a particular status because it is
the only one which whose input/output streams are linked to the
terminal the application was launched from. We later refer to this
process as the \emph{main node} of an application. As a consequence,
only the main node of an application can access its standard input
stream (\emph{e.g.}, using \|scanf|, \|gets|, etc.)

As an example of use of \|pm2_self()|, the program features a test on
the rank of the current process: if the current process is the main
process, then \|pm2_halt()| is called.

\subsubsection{How to terminate?}

The termination of a PM2 program is a delicate task since PM2 performs
no automatic termination detection. Thus, the termination decision
must be made at the application level. The termination phase is
split in two steps.

\emph{Exactly one} node must perform an explicit call to
\|pm2_halt()|. In our example, the main process was chosen to execute
this function.  Actually, any process could have been selected. For
instance, we could replace 
\begin{program}
if(pm2_self() == 0)
\end{program}
by any other kind of test selecting exactly one node, for instance:
\begin{program}
if(pm2_self() == pm2_config_size()-1)
\end{program}

At least conceptually, \|pm2_halt| performs a broadcast that requires
all nodes to stop answering to requests from the outside world. In
fact, this steps cuts the links between nodes: it stops the internal
daemon threads that are in charge of polling the network.  Note that
after a node receives this order, it still continues its normal
execution in a \emph{standalone} mode: \emph{halting} is no
\emph{killing}!

To exit from a PM2 session, each node must call \|pm2_exit()|.  This
call blocks the calling thread until all other threads (belonging to
the same node) terminate. In our example, no other application thread
but the main one is running. Thus, \|pm2_exit| completes as soon as
the internal daemon threads are stopped. Had we removed the call to
\|pm2_halt|, then all nodes would remain hanging, waiting for ever for
external requests!

From the user point of view, the PM2 program terminates as soon as the
shell prompts for the next command from the terminal. This corresponds
to the end of the main node. However, note that some nodes may
actually still run after the main node has completed.  In our example,
the main node can even terminate before any other node has started
executing \|tprintf|! Fortunaltely, this is not a problem here, since
\|tprintf| does not involve any communication with other nodes. We will
further discuss this termination problem in Section~\ref{sec:output},
page~\pageref{sec:output}.

\begin{note}
  LB: Is there a specific ordering to call halt and exit? In this
  section, I suggest that it is not relevant!
\end{note}


\subsection{Where does the output go?}
\label{sec:output}

This simple \|hello| program only printed the message generated by the
master node node. Indeed, the standard output of the other nodes is
redirected to local log files located into the \|/tmp| directory.  The
logs are easily accessible using the command \|pm2logs| which is in
charge of retrieving and displaying logs from each slave node of the
session configuration. Observe that such requests are quite slow, as
they use \|rsh| connections:
\begin{shell}
ravel% pm2logs
*** Process 1 on debussy:
[Threads : 2 created, 0 imported (0 cached)]
Hello World!
*** Process 2 on faure:
[Threads : 2 created, 0 imported (0 cached)]
Hello World!
\end{shell}

\figurelisting{hello1.c} {Output redirection,
  Example~1} {prog:hello1}

Observe that there is no reason why all the nodes should make the
\emph{same} output requests. Consider for instance the variant of rge
\|hello| program on Figure~\ref{prog:hello1}. Only node~1 outputs a
message.  Also, just to show you that any node can force termination,
node~2 is this time in charge of it!
\begin{shell}
ravel% make hello1
[...]

ravel% pm2load hello1
[Threads : 2 created, 0 imported (0 cached)]

ravel% pm2logs
*** Host debussy, process 1:
[Threads : 2 created, 0 imported (0 cached)]
Hello World from node 1!
*** Host faure, process 2:
[Threads : 2 created, 0 imported (0 cached)]
\end{shell}

\subsubsection{Redirecting output to the terminal}

For the user convenience, it is possible to redirect output directly
to the output of the master node using the \|pm2_printf| function. As
\|tprintf|, it is protected against multithreading. Moreover, it sends
its output as a regular message to node~0, where it is printed out.
However, the user must be aware that this facility should be used with
caution, as it uses the common communication subsystem. 

\begin{note}
  Luc to Olivier: I am not sure to understand why this is so? Please,
  clarify!
\end{note}

\figurelisting{hello1.c} {Output redirection,
  Example~2} {prog:hello2}

Consider program \|hello2| on Figure~\ref{prog:hello2}. The output is
immediately printed on the terminal attached to the master node. The
\|[t1]| label is inserted to identify the outputting node. By
convention, outputs from node~0 have no label, so as to mimick the
behavior of the original \|printf| function.
\begin{shell}
ravel% pm2load hello2
[t1] Hello World !
[Threads : 2 created, 0 imported (0 cached)]
\end{shell}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Remote Procedure Calls}

The goal of this section is to let you issue a \emph{Remote Procedure
  Call} in PM2. We start with a basic scheme, and we refine it step by
  step so as to demonstrate the versatility of PM2.

\subsection{Invoking a remote service}

\figurelisting {rpc.c} {Defining a service in PM2} {prog:rpc}

A \emph{service} is a function located on a \emph{server} node, which
can be invoked by some \emph{client} node. Observe that the client
node may be the same as the server node. It may also by run on the
same processor, or on a sibling processor on the same SMP board, or on
some remote one. All this is fully transparent for the user of PM2.
Nodes are specified through their numbers, as allocated by the
\|pm2conf| command. In this part, we consider our usual 3-node
configuration:
\begin{shell}
ravel% pm2conf ravel debussy faure
The current PM2 configuration contains 3 host(s) :
0 : ravel
1 : debussy
2 : faure
\end{shell}

In Program~\ref{prog:rpc}, a basic service is defined. Its behavior is
to print \|Hello, World!| on the standard output of the node which is
executing it. At the level of PM2, a \emph{service} is just an \|int|
associated to some function using the \|pm2_rawrpc_register| routine.
Observe that the service function takes no argument.

Of course, all the available servers have to be registered
\emph{before} they may be invoked. The association between the service
identifier and the service function has thus to be set up
\emph{before} calling the \|pm2_init| routine. Doing otherwise will
result in unspecified behvior, probably depending on the relative
speed of the nodes. The user is therefore \emph{strongly advised} to
pay very careful attention to this requirement.

Returning to the program, processing node~0 invokes the remote service
\|service_id| on processing node~1. The service invocation is
initiated using the \|pm2_rawrpc_begin| routine.  The first
argument of the routine is the unique number of the server node.  The
second argument is the service identifier. It has to be a valid number
on the \emph{server node}.  The third argument is used for specifying
additional \emph{attributes} to be discussed later. In this basic
case, no additional data is to be provided together with the service
invocation. The \|pm2_rawrpc_end| routine finalizes the service
invocation. Returning from this routine guarantees that the service
invocation has been completed from the client's point of view.

In this example, the service is synchronous: the \|pm2_rawrpc_end|
returns in the client node only \emph{after} the service function has
returned on the remote server node. Thus, the client node can safely
call \|pm2_halt| to force the termination of the distributed program.

It is now time to run the program:
\begin{shell}
ravel% pm2load rpc
[Threads : 2 created, 0 imported (0 cached)]

ravel% pm2logs
*** Process 1 on debussy:
[Threads : 2 created, 0 imported (0 cached)]
Hello, World!
*** Process 2 on faure:
[Threads : 2 created, 0 imported (0 cached)]
\end{shell}
Observe that an image of the program has been started \emph{both} on
hosts \|debussy| and \|faure|. In fact, the service function has been
registered on all 3 nodes, including root node~0.  Yet, only the
node~1 has been used as a server for the request of node~0.

In this example, the two processing nodes of PM2 have been launched on
two distinct machines for the sake of clarity.  However, the machine
running the server node may well be the same as the one running the
client node:
\begin{shell}
ravel% pm2conf debussy debussy debussy
The current PM2 configuration contains 3 host(s) :
0 : debussy
1 : debussy
2 : debussy

ravel% pm2load rpc
[Threads : 2 created, 0 imported (0 cached)]

ravel% pm2logs
*** Host debussy, process 1:
[Threads : 2 created, 0 imported (0 cached)]
Hello, World!
*** Host debussy, process 2:
[Threads : 2 created, 0 imported (0 cached)]
\end{shell}

The careful reader will have noticed that the definition of the
service function includes a call to the \|pm2_rawrpc_waitdata|
routine. This is necessary to instruct the service function that no
additional data is to be expected, so that it can proceed safely.
Actually, the service function has no way of discovering that the RPC
has been invoked without any additional data, and it is up to the user
to build the program client and server parts of the program in a
consistent way. More on this in the next section!

\subsection{Passing parameters}

\figurelisting {rpc-params.c} {Service with a string
  parameter} {prog:rpc-params}

\begin{note}
  Luc to Raymond: Why is variable \|s| not aligned in
  Program~\ref{prog:rpc-params} as in program \|simple/synchronous.c|?
\begin{program}
#define __ALIGNED__ __attribute__ ((aligned (sizeof(int))))
#define STRING_SIZE  16
static char msg[STRING_SIZE] __ALIGNED__;
\end{program}
\end{note}

Let us now the case of a service function invoked with additional
parameters. In Program~\ref{prog:rpc-params}, node~0 invokes a service
on node~1 with a string as a parameter. The service consists namely in
printing the string on the standard output.
  
Which string? Well, the original motivation of PM2 was to provide a
runtime environment for high-performance distributed programs with a
highly irregualr behavior: branch-and-bound search, computation on
sparse matrices, etc. In one word: In Search of Lost Time, \emph{\`A
  la recherche du temps perdu!} You may remember that a
\emph{Madeleine}, a typically French (delicious) cookie, played a
central role in the life of \emph{Marcel} Proust...
  
On the client side, the parameters of the service are \emph{packed}
together between the \|pm2_rawrpc_begin| and the \|pm2_rawrpc_end|
calls, using the \|pm2_pack_*| routine family. You can pack integers,
byte arrays, etc., very much as in PVM or PMI.  On the server side,
they are \emph{unpacked} within the service function.  Again, the
service function has no way of guessing what kind of parameters have
been packed by the client, nor how many of them have been packed. In
short, PM2 message are \emph{not self-described}.  This choice is
motivated by performance considerations: including the description of
the objects together with the objects into the messages yields a
significant overhead for small messages. In this respect, PM2 follows
the choice of many other communication interfaces. The counterpart is
that the user is responsible for the consistency of the series of
packing and unpacking actions. No verification is made at the level of
PM2, and unspecified behavior will result of any unconsistency in
types and numbers. The user specifies the end of reception on the
server side by a call to the \|pm2_rawrpc_waitdata| routine.

Let us now run the program.
\begin{shell}
ravel% pm2load rpc-params
[Threads : 2 created, 0 imported (0 cached)]

ravel% pm2logs
*** Process 1 on debussy:
[Threads : 2 created, 0 imported (0 cached)]
The sentence is: A la recherche du temps perdu.
*** Process 2 on faure:
[Threads : 2 created, 0 imported (0 cached)]
\end{shell}

\subsection{The packing/unpacking API}

The packing model of PM2 has been carefully designed so as to enable
high-performance communication on modern Gigabit network interfaces
such as BIP/Myrinet, SISCI/SCI, VIA, etc. In this context, it is of
uttermost significance to save copies: actually, the time for copying
a buffer within a node is of the same order of magnitude as the time
for sending it on the network to some remote node!  The key for
performance is therefore to allow for \emph{zero-copy}
comminunication: the message has to be directly taken from its initial
location in user-space, and directly placed into its final destination
in user space, without any additional copy. As the size of the
arguments for a RPC call are usually unknown in advance, the
communication interface has to include this feature into its design.
The approach of PM2, or more accurrately of the underlying
communication library called Madeleine, is to control the packing and
unpacking operations with additional \emph{flags}.

A message consists of several pieces of data, located anywhere in
userspace. It is constructed (resp. de-constructed) incrementally
using \emph{packing} (resp. \emph{unpacking}) routines, possibly at
multiple software levels without losing efficiency. The following
example illustrates this need. Let us consider a remote procedure call
which takes an array of unpredictable size as a parameter. When the
request reaches the destination node, the header is examined both by
the multithreaded runtime (to allocate the appropriate thread stack
and then to spawn the server thread) and by the user application (to
allocate the memory where the array should be stored).

The critical point of a send operation is obviously the series of
\emph{packing} calls. Such packing operations simply \emph{virtually}
append the piece of data to a message under construction. In addition
to the address of data and its size, the packing primitive features a
\emph{flag} parameter which specifies the semantics of the operation.
Available send flags are defined as follows:
\begin{description}
    
\item[\|SEND_SAFER|] This flag indicates that PM2 should pack the data
  in a way that further modifications to the corresponding memory area
  should not corrupt the message. This is particularly mandatory if
  the data location is reused before the message is actually sent.
  
\item[\|SEND_LATER|] This flag indicates that PM2 should not consider
  accessing the value of the corresponding data until the
  \|pm2_end_packing| primitive is called. This means that any
  modification of these data between their packing and their sending
  shall actually update the message contents.
    
\item[\|SEND_CHEAPER|] This is the default flag. It allows PM2 to do
  its best to handle the data as efficiently as possible. The
  counterpart is that no assumption should be made about the way PM2
  will access the data. Thus, the corresponding data should be left
  unchanged until the send operation has completed. Note that most
  data transmissions involved in parallel applications can accomodate
  the \|send_CHEAPER| semantics.

\end{description}
The following flags control the reception of user data packets:
\begin{description}
    
\item[\|RECV_EXPRESS|] This flag forces PM2 to guarantee that the
  corresponding data are immediately available after the the
  \emph{unpacking} operation. Typically, this flag is mandatory the
  data is needed to determine the next forthcoming \emph{unpacking}
  calls.  On some network protocols, this functionality may be
  available for free. On some others, it could penalize latency and
  bandwidth. The user should therefore extract data this way only when
  necessary.
    
\item[\|RECV_CHEAPER|] This flag allows PM2 to defer the extraction of
  the corresponding data until the execution of the
  \|pm2_rawrpc_waitdata| routine.  Thus, no assumption can be made
  about the exact moment at which the data will be extracted.
  Depending on the underlying network protocol, PM2 will do its best
  to minimize the overall message transmission time. If combined with
  \|send_CHEAPER|, this flag always guarantees that the corresponding
  data is transmitted as efficiently as possible.

\end{description}
Observe that the emission and reception flags should be \emph{both}
specified  by the matching packing/unpacking calls, and these
specfications should be identical. Again, unspecified behavior would
result from mismatching flags.

In Program~\ref{prog:rpc-params}, everything is sent \|CHEAPER|, so as
to save time. The user has been careful enough not to corrupt
variables \|len| and \|s| before calling \|pm2_rawrpc_end|. On the
reception side, variable \|len| \emph{must} be received \|EXPRESS|, as
its value is needed to allocate the buffer for the unpacking of
variable \|s|.  Then, variable \|s| can be received \|CHEAPER|. If the
underlying operating system and network interface permit, then this
(possibly large) message will thus be directly installed into the
reception buffer without any extra copy, providing the user with
optimal performance.



\begin{note}
  Luc to Raymond and Olivier: Well, do you have a more
  interesting/advanced example in mind?
\end{note}

\subsection{Threaded services}

Services handlers are executed sequentially. As long as a service
function has not returned on the server, the service is not available
for another client. This may result into deadlocks if the service
includes some potentially blocking actions, such as invoking other
services, either explicitely or implicitely.

\figurelisting {rpc-threads.c} {Spwaning a fresh thread to serve a
  remote request} {prog:rpc-threads}

Such problems can be addressed by executing services into a fresh
threads instead of the original service handler. This technique is
demonstrated on Program~\ref{prog:rpc-threads}. Service function
\|sample_service| spwans a fresh thread each time it is invoked
through a RPC request. Then, it immediately returns. The child thread
is in charge of extracting the message from the network through a series
of \|unpack| calls in the usual way. 

Observe that PM2 do not care about \emph{who} extracts a message at a
node. Any thread can do it. The only requirement enforced within the
runtime system is that the same thread unpack the series of data
pieces and calls the \|pm2_rawrpc_waitdata| routine. Having two or
more threads extracting data from the network concurrently would
actually not make sense: it raises a run-time error. 

\begin{note}
  Luc to Olivier + Raymond: Is it right? A sentence should probably be
  inserted here to warn that this behavior may be revised in the
  future versions.
\end{note}

This feature enables the newly spawn thread to do the work for the
sake of the original service handler. Yet, this raises a new
problem. Actually, the service handler immediately returns after
spawning the thread, and then looses any control on its
progression. Thus, the \|pm2_rawrpc_end| routine on the client side
may return \emph{before} the service thread has even started any
unpacking whatsoever! 

This may result in an incorrect behavior, as demonstrated by the
following scenario. Consider that the client is very fast. After
exiting the \|pm2_rawrpc_end| routine, it calls the \|pm2_halt|
routine. This triggers broadcasting a termination request to all the
nodes. Assume now that the messages used on this broadcast do not
travel on the same \emph{channel} as the regular messages, so that
they can take over and arrive \emph{before} the service thread
initiates the unpacking. Then, all the reception facilities of the
server node are closed down. When the daemon service thread finally
calls the \|pm2_unpack_*| routine, nobody remains living here to
phisycally extract the data from the network, and a run-time error
results!

Well, you may argue that this scenario is just like SF and little
green men attacking the White House... The writer's personal
experience is that the worse case is quite common in this matter, in
opposite(?) to other domain of life. Just don't try!

How can one circumvent the problem? A generic technique called
\emph{completion} will be introduced later. In the specific example
displayed on Figure~\ref{prog:rpc-threads}, a simple way-out is to let
the service thread issue the call to the \|pm2_halt| routine, as it is
guaranteed that this thread issues the inter-thread last interaction.

Let us now run the program. Observe that 3 threads have been
created on node~1, instead of the regular~2.
\begin{shell}
ravel% pm2load rpc-threads
[Threads : 2 created, 0 imported (0 cached)]

ravel% pm2logs
*** Process 1 on debussy:
[Threads : 3 created, 0 imported (0 cached)]
Hello world!
*** Process 2 on faure:
[Threads : 2 created, 0 imported (0 cached)]
\end{shell}

\figurelistingdouble {rpc-threads1.c} {Spawning threads to serve
  requests, an extended example} {prog:rpc-threads1}

To have a bit more fun on closing this part, let us design a slightly
more complex program. Node~0 issues a RPC request to node~1. For
node~1, servicing this request consists in spawning a fresh thread and
re-issueing the request to node~2 after having signed it with its
name.  Similarly for node~2 with respect to node~0. Finally, node~0
serves the request by printing the string to the standard output and
triggers termination.

\begin{shell}
ravel% pm2load rpc-threads1
[Threads : 3 created, 0 imported (0 cached)]
Sending string: Init
Received back string: Init 1 2

ravel% pm2logs
*** Process 1 on debussy:
[Threads : 3 created, 0 imported (0 cached)]
Passing on string: Init 1
*** Process 2 on faure:
[Threads : 3 created, 0 imported (0 cached)]
Passing on string: Init 1 2
\end{shell}

It is a good point to clarify the notion of a \emph{node} used
throughout this presentation. Actually, the nodes we are considering
here are \emph{virtual}: they only refer to Unix processes located on
physical nodes. It looks like common sense practice to exactly use one
virtual node per physical node, so that the two notions just identify
together. But nothing in PM2 requires such an identification. PM2
virtual nodes may be located at any physical nodes. The precise
association is made by the \|pm2conf| command. For instance, saying
\begin{shell}
ravel% pm2conf ravel debussy faure ravel debussy faure
\end{shell}
is just as right as anything else. Two processes are launched on each
of the three machines. From the point of view of PM2 programs, this
makes no difference.
\begin{shell}
ravel% pm2conf ravel debussy faure ravel debussy faure 
The current PM2 configuration contains 6 host(s) :
0 : ravel
1 : debussy
2 : faure
3 : ravel
4 : debussy
5 : faure

ravel% pm2load rpc-threads1
[Threads : 3 created, 0 imported (0 cached)]
Sending string: Init
Received back string: Init 1 2 3 4 5
\end{shell}
You may even start all processes on the same machine!
\begin{shell}
ravel% pm2conf ravel ravel ravel ravel ravel ravel
The current PM2 configuration contains 6 host(s) :
0 : ravel
1 : ravel
2 : ravel
3 : ravel
4 : ravel
5 : ravel

ravel% pm2load rpc-threads1
[Threads : 3 created, 0 imported (0 cached)]
Sending string: Init
Received back string: Init 1 2 3 4 5
\end{shell}
Also, there is no reason why this machine should be the one which you
are logged in. Any machine can make it as well!
\begin{shell}
ravel% pm2conf debussy debussy debussy debussy debussy debussy
The current PM2 configuration contains 6 host(s) :
0 : debussy
1 : debussy
2 : debussy
3 : debussy
4 : debussy
5 : debussy

ravel% pm2load rpc-threads1
[Threads : 3 created, 0 imported (0 cached)]
Sending string: Init
Received back string: Init 1 2 3 4 5
\end{shell}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Mastering PM2}

Congratulation! You are now knowledgeable in PM2, ready to write your
own programs.  Interested in getting to the \emph{Wizard} level? Just
keep on reading!

\section{Invoking services synchronously}

The problem of gracefully terminating a distributed program is at the
very heart of this area of programming. The problem is even sharper
with PM2 programs because of the multithreaded behavior which
dramatically increases the asynchronicity.

\figurelisting {rpc-completion.c} {Using completion token to
  garcefully terminate a PM2 program} {prog:rpc-completion}

PM2 provides the programmer with a simple mechanism to solve the
problem, at least in most cases. It is called \emph{completion}.
Essentially, a \emph{completion} is a token which is passed along
during the computation. The idea is that the creator of the token
sends it along to its communication partner. It may eventually block and
\|wait| for receiving the completion token. When the partner has
completed its task, it can \|signal| it to the token creator by
sending back the token to it. Observe that the completion token can
travel across many partners before being used. 

Consider a typical use of such a mechanism in
Program~\ref{prog:rpc-completion}. It is a minimal variation of
Program~\ref{prog:rpc-threads}. The only difference is that node~0 is now
in charge of triggering the termination by calling the \|pm2_halt|
routine. As observed previously, it cannot do it safely in general.
It has to make sure beforehand that all the communications have
completed. This is precisely the role of the completion token. Node~0
creates token \|c| with the \|pm2_completion_init| routine. Observe
that only node~0 does it, and that it occurs in the concurrent phase
of the program, after the call to \|pm2_init|. Several parameters can
be specified on creating a completion token, and the extended profile of
the creation function is
\begin{program}
void pm2_completion_init(pm2_completion_t *c,
                         pm2_completion_handler_t handler,
                         void *arg);
\end{program}
Yet, setting the last two parameters to the default \|NULL| value is
sufficient in most usual cases. 

Then, the completion token is handled very much like an ordinary
object. It can be packed together with other data into a messages
using the \|pm2_pack_completion| routine, with the usual mode flags.
It can be similarly unpacked with the \|pm2_unpack_completion|
routine.  Finally, one can send back the token to its creator using
the \|pm2_completion_signal| routine. On its side, the token creator
blocks waiting for the token using the \|pm2_completion_wait|
routine. In contrast with Program~\ref{prog:rpc-threads}, it is now
guaranteed that the \|pm2_halt| function is called on node~0
\emph{after} the thread on node~1 has completed its job!

Any number of tokens can be used within a PM2 program. Also, observe
that the \|completion_signal| and \|completion_wait| routine involve
RPC communications in the background. It is up to the user to
guarantee that no deadlock may occur.

\begin{note}
  LB to RN: Please, proofread and expand the last paragraph. I am not
  sure about the (probably many) pitfalls here...
\end{note}

\section{Thread migration}

\figurelisting {migration.c} {Migrating a thread across the
  configuration} {prog:migration}

\subsection{The basic case: migrating oneself}
\label{sec:self-migration}

We now turn to the most amazing feature of PM2: thread migration. The
essential idea is quite simple. At some point, a thread can request
the local scheduler to migrate a thread to another node. This
\emph{victim} thread may even be the calling thread itself. The victim
is dynamically preempted in the scheduler queue. Its set of resources
(descriptors, private stack, dynamically allocated memory area) are
packed into a buffer (\emph{marshalling}) and a RPC is issued to the
destination node with the packed resources as argument.  The service
associated with this RPC consists in creating a new thread, granting
it all the packed resources and restarting it.  As far as the victim
thread is concerned, this is completely transparent.  It is not
\emph{aware} of the migration, which merely amounts to an ordinary
context switch.  Yet, the global environment of the thread is now the
one provided by the new node: global variables, node identifier, etc.
In particular, thread migration does not make any attempt to keep the
system descriptors consistent across the migration: open files, pipes,
signals, etc.  Mixing migration and I/O has requires uttermost care.

Obviously, some circumstances require a thread to protect itself
against migration. This occurs for instance on exchanging messages,
doing I/O, etc. PM2 provides the programmer with a
\|pm2_enable_migration| and \|pm2_disable_migration| routines to
manage this aspect. By default, migration is disabled.

Program~\ref{prog:migration} is a simple example of thread migration.
Each of the nodes of the configuration stores into its global variable
\|hostname| the name of the hosting physical machine. Then, node~0
spawns a thread which declares itself ready for migration. This thread
migrates from node to node in a round-robin way for a couple of hops,
and eventually stops on a node (which happens not to be the same as
its starting node). The \|pm2_halt| routine is called on the final
node. 

On each hop, it prints the node number provided by PM2 together with
the contents of variable \|hostname| of its \emph{current global
  environment}. Observe that this variable is set in the prelude
phase, \emph{before} the call to the \|pm2_init| routine. It would be
incorrect to do it later, as no assumption may be made about the time
the incoming thread will be unpacked and restarted on the destination
node.

\begin{shell}
ravel% pm2conf ravel debussy faure
The current PM2 configuration contains 3 host(s) :
0 : ravel
1 : debussy
2 : faure

ravel% pm2load migration
[Threads : 5 created, 2 imported (0 cached)]
Hop 0: I am on node 0, host ravel...
Hop 0: Leaving to node 1
Hop 3: I am on node 0, host ravel...
Hop 3: Leaving to node 1
Hop 6: I am on node 0, host ravel...
Hop 6: Leaving to node 1

ravel% pm2logs
*** Process 1 on debussy:
[Threads : 5 created, 3 imported (0 cached)]
Hop 1: I am on node 1, host debussy...
Hop 1: Leaving to node 2
Hop 4: I am on node 1, host debussy...
Hop 4: Leaving to node 2
*** Process 2 on faure:
[Threads : 4 created, 2 imported (0 cached)]
Hop 2: I am on node 2, host faure...
Hop 2: Leaving to node 0
Hop 5: I am on node 2, host faure...
Hop 5: Leaving to node 0
\end{shell}
As usual, there is no restriction whatsoever on the association
between the physical machine and the logical nodes of PM2. For
instance, you could try to run the program on the same physical node.
It works just as well!
\begin{shell}
ravel% pm2conf ravel ravel ravel
The current PM2 configuration contains 3 host(s) :
0 : ravel
1 : ravel
2 : ravel

ravel% pm2load migration
[Threads : 5 created, 2 imported (0 cached)]
Hop 0: I am on node 0, host ravel...
Hop 0: Leaving to node 1
Hop 3: I am on node 0, host ravel...
Hop 3: Leaving to node 1
Hop 6: I am on node 0, host ravel...
Hop 6: Leaving to node 1

ravel% pm2logs
*** Process 1 on ravel:
[Threads : 5 created, 3 imported (0 cached)]
Hop 1: I am on node 1, host ravel...
Hop 1: Leaving to node 2
Hop 4: I am on node 1, host ravel...
Hop 4: Leaving to node 2
*** Process 2 on ravel:
[Threads : 4 created, 2 imported (0 cached)]
Hop 2: I am on node 2, host ravel...
Hop 2: Leaving to node 0
Hop 5: I am on node 2, host ravel...
Hop 5: Leaving to node 0
\end{shell}

\subsection{Combing migration and completion}

\figurelisting {migration1.c} {Using completions in presence of
  migration} {prog:migration1}

The main problem in the previous program is to \emph{gracefully} terminate
the computation. The easy solution implemented in
Program~\ref{prog:migration} was to let the moving thread initiate the
terminaison. However, this is obviously a very specific trick, which
does not generalize!

The idea is to combine the migration facility together with the
completion one. On spawning a migrating thread, the spawning thread
generates a completion token and passes it on to the migrating thread.
Then, it blocks, waiting on the completion to be signalled. When the
thread has completed its series of hops, it signals the completion to
its originator, which can then initiates the termination phase.
Observe that this crucially relies on the fact that a completion
includes enough information to be routed back to its originator, and
that this information is physically stored \emph{explicitely} in the
completion (in opposite to being accessed through pointers).  If the
completion token is stored within the resources of the moving thread,
for instance, its stack as a local variable, then it gets migrated
together with the thread and can be used from any node (including the
originator node!)

This approach is examplified in Program~\ref{prog:migration1}. Observe
how the completion is passed from its originator to the moving thread.
The completion is created in a global variable on node~0. The first
action of the created thread is to copy the completion into a local
variable, so that is gets migrated together with the rest of the
resources. This uses a special function: 
\begin{program}
void pm2_completion_copy(pm2_completion_t *to, 
                         pm2_completion_t *from)
\end{program}
You are strongly advised not to copy completion directly as in
\|my_c = c|, though it works in the current version of PM2.
Instead, you should say \|pm2_completion_copy(&my_c, &c)| to
guarantee that a \emph{physical} copy of the relevant information is
properly stored into the completion. Unfortunately for the oblivious
user, PM2 does very little verification about the proper use of such
auxiliary routines. It also provide few facilities to debug them.
Just believe me: You'd better write a correct code at the first
attempt!

\figurelisting {migration2.c} {Passing completions as
  parameters} {prog:migration2}

This technique used above, having the originator thread passing a
completion token to the moving thread, is rather contrived. A better
approach is examplified in Program~\ref{prog:migration2}. A pointer to
the completion is passed as an argument to the thread creation
function. The spawned thread allocate storage locally for a
completion, and copies the argument completion into its personal
resources using the \|pm2_completion_copy| routine:
\begin{program}
  pm2_completion_copy(&my_c, (pm2_completion_t *)arg)
\end{program}
This saves the allocation of some global storage and it is much
cleaner. Yet, observe that this works because the storage for the
original completion is guaranteed to remain accessible at the time of
the copy. There is a subtle synchronization game here, as it will be
only detroyed \emph{after} the completion is signalled back to its
originator. In case of doubt, it would have been much safer to
allocate the original completion in the global space, as in
Program~\ref{prog:migration}.

\figurelistingdouble {migration3.c} {Passing completions through a RPC
  call} {prog:migration3}

If the moving thread is started on a \emph{remote} node using a RPC,
then the completion can be passed as an argument to the RPC call, as
demonstrated in Program~\ref{prog:migration3}. This is probably the
cleanest way of all. Unfortunately, the semantics of PM2 does not
allow a RPC to the local node, which rules out this technique in the
case of a local creation.

\subsection{Migrating other threads}

In the Section~\ref{sec:self-migration}, we have considered the simple
case: a thread migrates itself off its current node. The migration
facility of PM2 is much more powerful, as it allows a thread to
migrate off other sibling threads, provided these threads have agreed
on getting migrated.

\figurelistingdouble {migrate-group.c} {Migrating off a group of
  threads} {prog:migrate-group}

Consider the program displayed on Figure~\ref{sec:self-migration}. It
is a good opportunity to demonstrate advanced techniques for threads
management. The idea is the following: Node~0 spwans 4~children
threads, numbered from~0 to~3. (Remind that threads disregard
migration by default.)  Then, the children threads with an even number
agree on migrating, and all children threads loop, repeatedly yielding
control to the scheduler with the \|marcel_yield| routine until variable
\|barrier| takes a non null value.  When such a value has been
recorded, the children threads issue a message and terminate. On its
side, the master thread on node~0 records the list of \|MIGRATABLE|
threads, and requests the migration of all of them to node~1. Then, it
issues a RPC to node~1 to set variable \|barrier| to 1, and does so
for its own node. Finally, the master thread calls the \|halt| routine
to stop all daemon service threads.
\begin{shell}
ravel% pm2load migrate-group
Initialization completed on node 0
Thread 0: Created on node 0, host ravel
Thread 1: Created on node 0, host ravel
Thread 2: Created on node 0, host ravel
Thread 3: Created on node 0, host ravel
2 threads among 4 migrated off to node 1
Issuing RPC to node 1
Just halting
Thread 3: Now on node 0, host ravel
Thread 1: Now on node 0, host ravel
[Threads : 6 created, 0 imported (0 cached)]

ravel% pm2logs

*** Host debussy, process 1:
[Threads : 4 created, 2 imported (0 cached)]
Initialization completed on node 1
Service activated on node 1, host debussy
Thread 0: Now on node 1, host debussy
Thread 2: Now on node 1, host debussy

*** Host faure, process 2:
[Threads : 2 created, 0 imported (0 cached)]
Initialization completed on node 2
\end{shell}

Let's now look more closely at the program. First, observe how the
master thread spawns the children threads and synchronize with them
using a completion token. Each child thread is created wil two
parameters: a completion token \|c| and a number \|i|. As the thread
creation routine only accepts one argument of type \|void *|, an
auxiliary structure has to be used: the two parameters have to be
packed into a structure; a pointer to the structure is passed as the
argument of the \|pm2_thread_create| routine; the structure is
eventually unpacked by the thread. This scheme is correct \emph{only}
because of the synchronization implemented by the completion token:
the master thread cannot spawn another thread before the current
thread has unpacked the structure.  Once one the completion token has
been received from each children thread, then the master thread is
garanteed that all of them have been created, and have decided upon
whether they enable their own migration or no.

Consider now how the master thread can migrate the children threads.
The first step is to store the children threads to be migrated into an
array.  This can be done by hand, but PM2 provides a spcialized
routine to do teh work. A call \|pm2_threads_list(N, threads, &n,
MIGRATABLE_ONLY)| takes as parameters an array of threads
\|threads[N]| (a PM2 thread has type \|marcel_t|) and a selection
condition. A number of condition are available, which can be or-ed and
and-ed as usual for flags. Here, we use the \|MIGRATABLE_ONLY| flag,
which does what you guess. Well, not exactly, as the flag is
(silently) and-ed with the \|NOT_BLOCKED_ONLY| flag: only the threads
which have enabled migration \emph{and} are not currently blocked are
stored into the array. At most \|N| such threads are stored, and the
number of actually stored threads is recorded into \|n|. Observe that
the calling thread is listed, too, if it meets the condition. This is
not the case in this example.

You may object that this sampling operation may yield inconsistent
result has the sampled threads are running asynchronously with the
sampling one, very much as in the \|ps| command of Unix. This is not
the case, as the \|pm2_threads_list| routine internally disables
Marcel, the thread scheduler fo PM2 using the
\|lock_task|/\|unlock_task| internal routines of Marcel. However, the
user has no control about the exact sampling time, but through the
explicit synchronization which have been introduced into the program.
Once the list of \emph{victim} threads to be migrated has been
constructed, then PM2 can migrated all of them to a common node using
the \|pm2_migrate_group| routine. This node may well be the local
node.  Of course, attempting to migrate a thread which has disabled
migration to a remote node raises an error.

This raises a new problem: how can the user make sure that the threads
sampled out with the \|pm2_threads_list| routine are still enabling
migration? Or even, that they are still running on the local node? Or
even, that they are still alive? Well, the only general solution is to 
garantee the atomicity of the two successive calls. This is the role
of the \|pm2_freeze| routine: it disables the scheduler, so that the
state of the other threads may not change until the \|pm2_unfreeze|
routine is called. Of course, this also suspends all daemon service
threads, so that no message can be handled. This is up to the user to
garantee that freezing the scheduling will not cause any deadlock in
the system, for instance by overfilling communication buffers. Also, 
calls to \|pm2_freeze|/\|pm2_unfreeze| may be nested. It is the
responsability of the user to ensure the proper nesting.

Well, well, well... Imagine the following scenario. Consider a thread
which enables its own migration; it freezes the scheduling on the
\emph{local} node, samples the migratable threads, and call the
\|pm2_migrate_group| routine to migrate them away to some
\emph{remote} node. It is itself migrated! Calling the \|pm2_unfreeze|
routine afterwards will most probably raise an error, as no call to
\|pm2_freeze| has ever been made on the \emph{remote} node. Also,
nobody is left on the \emph{local} node to unfreeze the scheduler, and
this would most probably result in a deadlock.  Interesting problem,
isn't it? The only solution is that PM2 calls the \|pm2_freeze|
routine on the local node \emph{on behalf} of the migrated thread.
This somewhat non standard call is actually included at the end of the
\|pm2_migrate_group| routine.  This is why there is no \emph{explicit}
call to \|pm2_unfreeze| in our program.  It is in
fact hidden in the call to \|pm2_migrate_group|.

Observe that the program on Figure~~\ref{prog:migrate-group} uses a
variable \|barrier| which is asynchronously read by many threads and
written by one thread. Strictly speaking, it is not correct to do so,
as there is no reason why the writing operation should be
atomic. Actually, the readers could in general read various
intermediate values from the variable, depending for instance on the
order in which the bits are written. One should guard all accesses to
a shared variable with a \emph{mutual exclusion object} (a mutex, for
short). In their simplest form, they are used as follows:
\begin{program}
marcel_mutex_t mutex;

marcel_mutex_init(mutex, NULL);

marcel_mutex_lock(mutex);
// Mutual exclusion section
marcel_mutex_unlock(mutex);

marcel_mutex_destroy(mutex);
\end{program}
However, the behavior of this very program does not depend on
concurrent reading and writing, as the variable is only asynchronously
tested against~0. Spotting a bit set to~1 early or late does not
matter. This is why we have left the shared variable unprotected.

\begin{note}
  LB to RN: Difficult section. Please, proofread!
\end{note}


\subsection{Iso-allocation and dynamically allocated data}

\figurelisting {iso-alloc.c} {Iso-allocation enables unlimited use of
  pointers in migrating threads} {prog:iso-alloc}

On migrating, a thread is moved together with its private resources:
its private descriptors and its private stack. All these resources are
allocated by PM2 so that the virtual addresses remains invariant upon
migration. This allows to freely use pointers in the stack of a
migrating thread. This is examplified in
Program~\ref{prog:iso-alloc}. Variable \|x| is allocated in the
private of the thread on node~1. After migration to node~2, the value
of \|x| \emph{and its address} are left unchanged.
\begin{shell}
ravel% pm2logs
*** Process 1 on debussy:
[Threads : 3 created, 0 imported (0 cached)]
First, I am on node 1, host debussy... &x = affafda0, x = 1234
*** Process 2 on faure:
[Threads : 3 created, 1 imported (0 cached)]
Now, I am on node 2, host faure... &x = affafda0, x = 1234
\end{shell}

This amazing feature is made possible thanks to the
\emph{iso-allocation} technique used by PM2 to allocate memory
dynamically. The whole virtual space of the nodes is managed in a
consistent way. If some storage is granted to some thread at one node,
then the iso-allocation technique guarantees that the corresponding
range of virtual addresses will not be used by any other node in the
configuration. Then, on a migration, the storage area can just get
copied exactly at the same virtual address in the destination node.
All pointers remain valid without any dependency on the compiler nor
any optimization strategy! You can therefore freely manupilate linked
lists within migrating threads without the smallest worrying...

\figurelisting {isomalloc.c} {Allocating dynamic storage which
  migrates together with the thread} {prog:isomalloc}

All storage requests in PM2 are handled with this technique, including
the storage allocated for the stack on creating a thread. However, it
is also available for explicit, dynamic storage allocation by the
programmer, as a substitute for the \|malloc|/\|free| functions. They
are just called \|pm2_isomalloc| and \|pm2_isofree|, with the same
profiles as the usual \|malloc| and \|free| functions. Dynamic memory
storage allocated with these function are included in the private
resources of the threads and are migrated together with the
thread. Because of the iso-allocation technique, the storage is mapped
at the \emph{same} virtual address in all nodes, so that pointers can
be freely used. You can use them within a storage area. You can use
them across areas. You can use them between the stack and a storage
area. 

\begin{note}
  LB to GA: Please, review this carefully!
\end{note}

In Program~\ref{prog:isomalloc}, the thread \emph{iso-allocates}
two storage area at addresses \|p| and \|q|. The variable located at
\|p|, (that is, \|*p|) holds the address \|q|. The variable located a
\|q| (that is, \|*q|) holds some data. Thus, \|**p| returns this
data through a double indirection. On allocating the storage areas, the
\|pm2_isomalloc| function registers them in the private descriptors of
the thread. When the thread is about migrating, the registered storage
areas are packed together with its descriptors and its stack. They are
unpacked on the destination node, and installed at the same virtual
address as in the origin node. This is why \|**p| returns the same
value \emph{after} the migration as before. 
\begin{shell}
ravel% pm2load isomalloc
First, I am on node 0, host ravel...
p = aff8f040, *p = aff8f080, **p = 1234
[Threads : 3 created, 0 imported (0 cached)]

ravel% pm2logs
*** Host debussy, process 1:
[Threads : 3 created, 1 imported (0 cached)]
Then, I am on node 1, host debussy...
p = aff8f040, *p = aff8f080, **p = 1234
\end{shell}

\figurelisting {isomalloc1.c} {Ordinary allocation results in
  undefined behavior in the presence of
  migration} {prog:isomalloc1}

A thread may allocate as many such dynamic storage areas as
needed. Observe that this allocation primitive is compatible with the
usual \|malloc| primitives. A thread may well allocate storage with
both functions within the same program. However, only areas allocated
with \|pm2_isomalloc| will migrate together with the thread. The
result of accessing an address allocated with the ordinary \|malloc|
function, after a migration is of course undefined as demonstrated by
Program~\ref{prog:isomalloc1}: the value of \|*p| is \emph{not}
preserved through the migration!
\begin{shell}
ravel% pm2load isomalloc1
First, I am on node 0, host ravel...
p = 80d2500, *p = 1234
[Threads : 3 created, 0 imported (0 cached)]

ravel% pm2logs
*** Host debussy, process 1:
[Threads : 3 created, 1 imported (0 cached)]
Then, I am on node 1, host debussy...
p = 80d2500, *p = 8
\end{shell}

\section{Sharing data across nodes: Distributed Shared Memory}

\figurelistingdouble {dsm.c} {Distributed shared memory in PM2}
{prog:dsm}

Up to now, we have only considered regular threads, which share data
with their sibling threads within the same processing node, that is,
their hosting Unix process. In regular PM2 programming, no action is
made to manage such data and to preserve consistency on migration.
For this purpose, PM2 includes a special library called DSM-PM2 (DSM
stands for \emph{Distributed Shared Memory}) to provide such a
functionality. Variables assigned within \emph{shared storage area}
can be accessed from any thread, whatever its hosting node. The system
garantees the concistency of the values observed by the threads
according a given semantic model, to be specified by the user through
an initialization routine. 

Consider Program~\ref{prog:dsm}. Variable \|shvar| is declared to be
\emph{globally shared} by all the threads of the program.  Each
processing node but the master node, successively spawns a thread to
increment this single shared variable for 20~units, recording the
initial and the final value it observed.  Finally, the master node
prints the value of the variable.  As expected, the final value of the
variable is exactly as it would have been with a sequence of
incrementation at the master node.
\begin{shell}
ravel% pm2load dsm
shvar=40
[Threads : 6 created, 0 imported (0 cached)]

ravel% pm2logs
*** Host debussy, process 1:
Thread 1 from node 1 finished on node 1: from 0 to 20!
[Threads : 8 created, 0 imported (0 cached)]
Thread 1 on node 1

*** Host faure, process 2:
Thread 2 from node 2 finished on node 2: from 20 to 40!
[Threads : 6 created, 0 imported (0 cached)]
Thread 2 on node 2
\end{shell}

DSM-PM2 implements a complex protocol to garantee that the final
result is the right one. In this example, the variable is physically
installed on the master node: it is called its \emph{home node}. Each
time a thread attempts to \emph{read} the value of the variable, it
gets a (read-only) copy of the variable from the master node, so that
repeated reads are very cheap. Each time a thread attempts
to\emph{write} a new value into the variable, all the subsidiary
copies of the variable are invalidated, and the new value is stored
into the master copy of the variable at its home node. This protocol
is known in the literature as the Li-Hudak's protocol. It implements a
\emph{sequential consistency}: everything happens as if all the
read/write actions had been applied \emph{sequentially} to the
variable at its home node, though the order in which the operations
have been aplied is unknown. This is why the program says in the
initialization phase: \|dsm_set_default_protocol(LI_HUDAK)|.

\figurelistingdouble {dsm1.c} {Distributed shared memory in PM2, using
  the \|MIGRATE_THREAD| sequential consistency protocol}
{prog:dsm1}

Other protocols are available in DSM-PM2. For instance, using
\|MIGRATE_THREAD| instead of \|LI_HUDAK| implements another protocol
to garantee sequential consistency, based on dynamic thread migration:
each time a thread attempts to access a globally shared variable, to
read it as well as to write it, then it silently migrates to the home
node of the variable. As you can observe, the result is the same as
above, but all the incrementation threads end up on the home node of
the shared variable.
\begin{shell}
ravel% pm2load dsm1
Thread 1 from node 1 finished on node 0: from 0 to 20!
Thread 2 from node 2 finished on node 0: from 20 to 40!
shvar=40
[Threads : 4 created, 2 imported (0 cached)]

ravel% pm2logs
*** Host debussy, process 1:
[Threads : 3 created, 0 imported (0 cached)]
Thread 1 on node 1

*** Host faure, process 2:
[Threads : 3 created, 0 imported (0 cached)]
Thread 2 on node 2
\end{shell}


\figurelistingdouble {dsm2.c} {Distributed shared memory in PM2, with
  concurrent access to shared variables} {prog:dsm2}

In the two examples above, each thread accessing shared variable
\|shvar| is launched in turn, waiting for the previous one to signal
completion. Then, no access conflict on the shared variable is to be
threatened. What if the thread attempt to increment the variable
concurrently as in Program~\ref{prog:dsm2}? Actually, the
incrementation instruction \|shvar++| is compiled into an
\emph{sequence} of elementary, atomic machine instructions, something
like:
\begin{program}
{int tmp; tmp = shvar; tmp++; shvar = tmp;}
\end{program}
Thus, we have to add extra synchronization objects to guarantee that
this sequence of atomic instructions is actually executed in mutual
exclusion, as if it was a single instruction. DSM-PM2 provides
\emph{global} mutexes for this purpose, as shown in the program.
Observe that this mutexes are much more powerful than the regular PM2
mutexes, which are local to a PM2 node.
\begin{shell}
ravel% ./run dsm2
Compiling dsm2.c... for flavor 'pm2'
ravel% pm2conf ravel debussy faure
The current PM2 configuration contains 3 host(s) :
0 : ravel
1 : debussy
2 : faure

ravel% pm2load dsm2
shvar=40
[Threads : 10 created, 0 imported (0 cached)]

ravel% pm2logs
*** Host debussy, process 1:
[Threads : 8 created, 0 imported (0 cached)]
Thread 1 started on node 1
Thread 1 from node 1 finished on node 1: from 0 to 20!

*** Host faure, process 2:
[Threads : 6 created, 0 imported (0 cached)]
Thread 2 started on node 2
Thread 2 from node 2 finished on node 2: from 20 to 40!
\end{shell}

Just for fun, you may wish to try out this last program \emph{without}
the mutexes! This is legal in DSM-PM2, and the sequential consistency
semantics model for the DSM fully addresses it. However, you should
remind that the model specifies the behavior at the level of
elementary atomic instructions: reading and writing shared variables,
so that the actual program which is run is the one with the expanded
form is \|shvar++|.  I run the experiment on my machine: here is the
result:
\begin{shell}
ravel% more dsm3.c
[...]
  // dsm_mutex_lock (&L);
  initial = shvar;
  for (i = 0; i < NB_ITERATIONS; i++)
    shvar++;
  final = shvar;
  // dsm_mutex_unlock (&L);
[...]

ravel% pm2load dsm3
shvar=20
[Threads : 7 created, 0 imported (0 cached)]

ravel% pm2logs
*** Host debussy, process 1:
[Threads : 7 created, 0 imported (0 cached)]
Thread 1 started on node 1
Thread 1 from node 1 finished on node 1: from 0 to 20!

*** Host faure, process 2:
[Threads : 6 created, 0 imported (0 cached)]
Thread 2 started on node 2
Thread 2 from node 2 finished on node 2: from 0 to 20!
\end{shell}






% Advanced Threading
%_____________________________________________________________________________
\section{Advanced thread features}

\subsection{Thread synchronization}

% Miscellaneous
%_____________________________________________________________________________
\section{Miscellaneous}

\subsection{Application initialization with PM2}
\label{sec:startupfunc}

\|pm2_set_startup_func|


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Additional PM2 material}

% Standard Main
%_____________________________________________________________________________
\section{Using a standard main}
\label{sec:tradimain}

Some application may require to use the traditional \|main| function
name instead of \|pm2_main|. In this case, the \|STANDARD_MAIN|
compilation flag must be defined for each PM2 file. This can be done
by adding the definition in the PM2 makefile (\|pm2/make/common.mak|):

\begin{small}
\begin{makefile}
   GLOBAL_OPTIONS   =   -Wall -O6 -DSTANDARD_MAIN
\end{makefile}
\end{small}

Note that this may affect the performance of several thread management
functions...

% Scripts
%_____________________________________________________________________________
\section{Synopsys of PM2 scripts}
\label{sec:scripts}

\subsection{pm2custom}

\subsection{pm2conf}

\subsection{pm2load}

% Common Pitfalls
%_____________________________________________________________________________
\section{Common Pitfalls}
\label{sec:commonpitfalls}

The Table~\ref{tbl:pitfalls} sums up the symptoms together with
possible solutions.
\begin{table}
\caption{Common Pitfalls\label{tbl:pitfalls}}
\begin{center}
\begin{tabular}{|p{5cm}|p{7cm}|}                             \hline
Symptoms                       & Solutions                    \\ \hline
`Permission denied' error      & Each configuration node must be made
accessible by \|rsh| from the local host: update you
\|.rhosts| file                                         \\ \hline
\end{tabular}
\end{center}
\end{table}

% FAQ
%_____________________________________________________________________________
\section{FAQ}
\label{sec:faq}
\begin{quote}
  Why do I get the message "Permission denied" when running pm2load?
  
  Why does my program crash the entire cluster?
\end{quote}

% Bibliography
%_____________________________________________________________________________
\section{Bibliography}
    
\end{document}

