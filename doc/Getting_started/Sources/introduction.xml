<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE TEXT SYSTEM "file:../xml2html/data/xml/DOCUMENT.dtd">

<TEXT>

<P>
PM2 is developed at LaBRI (<EM>Laboratoire Bordelais de Recherche en
Informatique</EM>), a research laboratory located in Bordeaux, France,
jointly supported by the INRIA, the CNRS, and the University of
Bordeaux 1. Before that, PM2 was developed at LIP (<EM>Laboratoire de
l'Informatique du Parallélisme</EM>), a research laboratory located at
the ENS Lyon (<EM>Ecole Normale Supérieure de Lyon</EM>), France,
jointly supported by the INRIA, the CNRS and the University Claude
Bernard Lyon 1. PM2 was originally designed at LIFL, University of
Lille, France.
</P>

<P>
PM2 (<EM>Parallel Multithreaded Machine</EM>) is a distributed
multithreaded programming environment designed to support efficiently
irregular parallel applications on distributed architectures of SMP
nodes. PM2 is primarily designed for medium-size clusters of commodity
processing nodes interconnected by high-performance networks. However,
nothing in the design of PM2 prevents from using it on massively
parallel MIMD machines at one end of the spectrum, or as a support for
metacomputing over the Internet on the other end. Actually, a new
version of PM2's communication library has been developed to support
heterogeneous networking configurations, such as sets of
interconnected clusters.
</P>

<IGNORE>
<P>
The thread management subsystem of PM2 is called Marcel. It is
designed to manage several hundreds of threads on each available
physical processor. The PM2 programming interface provides
functionalities for the management of this high degree of parallelism
and for dynamic load balancing. Interesting features of PM2 include
its priority driven scheduling policy, its thread migration mechanism
and its ability to support various load balancing policies. PM2 has
been designed to provide threads as light as possible: the switching
time is well under the micro-second. Yet, Marcel is completely written
in portable C, but a dozen of lines of assembly code, which makes it
portable across most processors and flavors of Unix systems, including
Solaris and Linux. However, significant improvements have been made
for Linux by introducing specific support into the operating system.
</P>

<P>
The communication subsystem of PM2 is called Madeleine. It is
specifically designed to provide PM2 threads with efficient
communication facilities on modern high-performance networks. In
particular, Madeleine strives at providing zero-copy communication in
user-space to obtain a minimal latency and a maximal bandwidth: less
than 10 <MICROSECOND/> latency and Gigabit/s bandwidth have been
reported for Myrinet networks.  Yet, the design has been careful
enough to keep Madeleine portable across a wide range of communication
interfaces, including TCP and MPI. This makes PM2 portable on most
Unix platforms.
</P>
</IGNORE>

<P>
The PM2 software suite is composed of different software components
for clusters and clusters of clusters presented in the following sections.
</P>


<SUBSECTION TITLE="Runtime Systems">

<SUBSUBSECTION><TITLE><MU/>PM2</TITLE>
<P>
<MU/>PM2 is a low level generic runtime system which integrates
multithreading management, communication management and elementary
shared memory management. Its interface is reduced to simple RPC
paradigm as well as the classical multithreading primitives. Its
implementation is somewhat a software glue in-between the Marcel and
Madeleine libraries and a iso-address memory management module. The PM2
environment was rewritten to rely exclusively on this runtime system.
</P>
</SUBSUBSECTION>

</SUBSECTION>

<SUBSECTION TITLE="Communication Libraries">

<SUBSUBSECTION TITLE="Madeleine">
<P>
Madeleine is a communication multi-cluster library which implements a
concept of communication channel that can be either physical (an
abstraction of a physical network) or virtual. In that later case, it
is possible to build heterogeneous virtual networks. Madeleine
provides a forwarding mechanism relying onto gateways when the
configuration allows it (nodes which belong to several physical
networks with different technologies). Madeleine is able to
dynamically select the best way to transmit data according to the
underlying network technology (multi-paradigm). This is made possible
by specifying constraints upon data to be sent (concept of programming
by contract) and enables to keep a good level of performance on top of
technologies with very differing functioning.  Madeleine is available
on top of various network hardware: Myrinet, SCI, Ethernet or even VIA
and runs on the following architectures: Linux/IA32, Linux/Alpha,
Linux/Sparc, Linux/PowerPC, Solaris/Sparc, Solaris/IA32, AIX/PowerPC,
WindowsNT/IA32.
</P>
</SUBSUBSECTION>

<IGNORE>
<SUBSUBSECTION TITLE="MPICH-Madeleine">
<P>
MPICH-Madeleine is an implementation of the MPI (Message Passing
Interface) communication standard. This standard is widely used in
scientific parallel computing applications. This library is an
adaptation of the well known MPICH (MPI CHameleon), which has the
property to be relatively easily adaptable on top of new network
technologies thanks to a well defined layered architecture.
</P>

<P>
To have more information about MPICH-Madeleine, please consult the
following URL: <URL>http://runtime.bordeaux.inria.fr/mpi/</URL>.
</P>
</SUBSUBSECTION>
</IGNORE>

<SUBSUBSECTION TITLE="NewMadeleine">
<P>
NewMadeleine is a complete redesign and rewrite of Madeleine. The new
architecture aims at enabling the use of a much wider range of
communication flow optimization techniques. It is entirely modular: The
request scheduler itself is interchangeable, allowing experimentations
with multiple approaches or on multiple issues with regard to
processing communication flows. In particular we implemented an
optimizing scheduler called SchedOpt. SchedOpt targets applications
with irregular, multi-flow communication schemes such as found in the
increasingly common application conglomerates made of multiple
programming environments and coupled pieces of code, for instance.
SchedOpt itself is easily extensible through the concepts of
optimization strategies  (what to optimize for, what the optimization
goal is) expressed in terms of tactics (how to optimize to reach the
optimization goal). Tactics themselves are made of basic communication
flows operations such as packet merging or reordering.
</P>
</SUBSUBSECTION>

<SUBSUBSECTION TITLE="Mad-MPI">
<P>
Mad-MPI is a new light implementation of the MPI standard. This
simple, straightforward proof-of-concept implementation is a subset of
the MPI API, that allows MPI applications to benefit from the
NewMadeleine communication engine.
</P>

<P>
Mad-MPI also implements some optimizations mechanisms for derived
datatypes. MPI derived datatypes deal with noncontiguous
memory locations. The advanced optimizations of NewMadeleine allowing
to reorder packets lead to a significant gain when sending and
receiving data based on derived datatypes.
</P>

<P>
To have more information about Mad-MPI, please consult the following
URL: <URL>http://runtime.bordeaux.inria.fr/MadMPI/</URL>
</P>
</SUBSUBSECTION>

<SUBSUBSECTION TITLE="PadicoTM">
<P>
PadicoTM is a communication framework for grids, built upon Madeleine. It is able to make a large spectrum of middleware systems (MPI, CORBA, JXTA, SOAP, HLA, JVM, etc.) work over Madeleine.
</P>

<P>
To have more information about PadicoTM, please consult the following URL: <URL>http://runtime.bordeaux.inria.fr/PadicoTM/</URL>
</P>
</SUBSUBSECTION>
</SUBSECTION>

<SUBSECTION TITLE="Multithreading Libraries">

<SUBSUBSECTION TITLE="Marcel">
<P>
 Marcel is a thread library that provides a POSIX-compliant interface
 and a set of original extensions. It can also be compiled to provide
 ABI-compabiblity with NTPL threads under Linux, so that multithreaded
 applications can use Marcel without being recompiled.
</P>

<P>
Marcel features a two-level thread scheduler (also called N:M
scheduler) that achieves the performance of a user-level thread
package while being able to exploit multiprocessor machines. The
architecture of Marcel was carefully designed to support a high number
of threads and to efficiently exploit hierarchical architectures (e.g.
multi-core chips, NUMA machines).
</P>

<P>
So as to avoid the blocking of kernel threads when the application
 makes blocking system calls, Marcel uses Scheduler Activations when
 they are available, or just intercepts such blocking calls at dynamic
 symbols level.
</P>
</SUBSUBSECTION>

<SUBSUBSECTION TITLE="PIOMan">

<P>
PIOMan is an event detector server that is dedicated to the
reactivity, typically communication events. PIOMan provides the
NewMadeleine library with methods to detect quickly communication
queries.
</P>

<P>
To have more information about PIOMan, please consult the following URL: <URL>http://runtime.bordeaux.inria.fr/pioman/</URL>
</P>
</SUBSUBSECTION>

<IGNORE>
<SUBSUBSECTION TITLE="LinuxActivations">

<P>
LinuxActivations is a software component that can be used conjointly
with the Marcel multithreading library. It is an kind of extension to
the Linux kernel which enables to precisely control the user-level
thread scheduling when they perform blocking input/output operations
in the kernel. LinuxActivations is based on an extension of the
SchedulerActivations proposed by Anderson and uses a mechanism which
is the opposite of a system call (named upcall) in order to inform the
user level scheduler about events generated by the kernel.
</P>
</SUBSUBSECTION>
</IGNORE>
</SUBSECTION>

<IGNORE>
<SUBSECTION TITLE="The PM2 Programming model">

<P>
PM2 follows the SPMD (<EM>Single Program Multiple Data</EM>)
programming model, in a way very similar to the PVM and MPI
communication libraries.  Users write a single program text, a
copy of which is launched by a specific <TT>load</TT> command on each
<EM>processing node</EM> of the current configuration. Then,
it is up to this common program text to include branching so as to
differentiate between the processing nodes, based on a programming
scheme like

<PROGRAM FILE="./Progs/init.c"/>
</P>

<P>
At this level of presentation, a processing node is simply a Unix
process. The association between <EM>processing nodes</EM> and
<EM>physical SMP nodes</EM>, or even <EM>physical processors of the
  SMP nodes</EM> are made outside of the PM2 model by specific commands
described below.
</P>

<P>
On each node, the <TT>pm2_main</TT> function of the program is activated.
This function may then <EM>spawn</EM> (i.e., create) an arbitrary
number of children threads.  PM2 adheres as much as possible to the
POSIX Threads programming model. A PM2 thread is just declared as an
ordinary function. The only difference is that it is called through a
specific library routine, which makes it run <EM>concurrently</EM> to its
caller function. The caller and the callee share all the global
variables of the local copy of program. Yet, the callee is launched
with a fresh stack. This implies that there is no return for a
function launched as a thread. On returning from the initial
activation of its function, the thread merely dies.  Observe also that
the caller and the callee functions run concurrently: once launched,
there is no longer a notion of <EM>father</EM> and <EM>son</EM>,
and the set of threads is just flat: if one of the running threads
spawns a new thread, then it is just added to the current, flat pool
of running threads, whatever its creator.  Also, any thread may
terminate the program, whatever its rank of creation.
</P>

</SUBSECTION>

<SUBSECTION TITLE="Threads and RPCs">

<P>
Once spawned, a thread runs until its function returns. Threads living
on the same node may synchronize by a number of means, as defined by
the POSIX Threads model: semaphore, monitors, etc. Threads living on
distinct nodes may not directly interact together. The only way a PM2
thread may interact with a remote node is to issue a <EM>remote
  procedure call</EM> (RPC): the <EM>client</EM> thread requires the
execution of a <EM>service</EM> function on the remote, <EM>server</EM>
node. PM2 provides the programmer with many flavors of RPCs. The
client thread may or may not wait for the result of the function; the
argument of the service function may be packed and unpacked in various
ways, depending on their use; the server node may or may not create a
new, fresh thread to execute the service function. This latter
alternative deserves some comments. In many thread packages, the
default behavior is that a fresh thread is spawned on the server node to
execute the service function, so that the node is left available to
handle further requests. However, such a behavior induces an
additional overhead which may be prohibitive with respect to the requested
service. In PM2, the server node is left to execute the service
function <EM>in person</EM> so as to save this overhead. As a
side-effect, this keeps the node busy and unable to serve any further
request: it is up to the programmer to make sure that no deadlock may
result. Of course, service functions may also dynamically spawn
threads if necessary, hence getting back to the usual behavior.
</P>

<P>
Synchronization between threads is a crucial aspect of multithreaded
programming. If threads live on the same node (i.e. within the same
Unix process), they can synchronize through <EM>locks</EM>, according
to the POSIX Threads model. If they live on distinct nodes, then PM2
defines special objects called <EM>completions</EM>. In short, a
completion is a special, self-described token which can be generated
by a thread, and then forwarded to some other threads. The originating
thread can then <TT>wait</TT> for the completion to be sent back to
itself using a special <TT>signal</TT> routine. This routine may be
executed by any thread, including a thread living on the same node as
the originator.
</P>

</SUBSECTION>

<SUBSECTION TITLE="Starting and stopping programs">

<P>
How does a PM2 SPMD distributed program start and stop? This is a
difficult problem, which requires much attention from the programmer
as it is a never-ending source of subtle failures (the writer has
painfully experienced it!). The execution of a PM2 program can be
divided in three phases.
</P>

<DL>

<DT>Prelude.</DT>
<DD> The prelude starts when launching the <TT>pm2_main</TT> root
  function of the program. This function should include all necessary
  initializations, especially the registration of the service
  functions which may be activated by remote clients
  (<TT>pm2_rawrpc_register</TT> routine), and more generally all objects
  which may be seen from remote nodes. This phase ends with a call to
  the <TT>pm2_init</TT> routine, which spawns the servicing threads to
  listen to external requests (RPC, migration, etc.).
</DD>

<DT>Main part.</DT>
<DD> After returning from the <TT>pm2_init</TT> routine, all
  PM2 nodes can freely proceed, behaving both as client and
  server. In general, it is considered as <EM>literate PM2
    programming</EM> to let only node<SPACE/>0 to be active, all the other nodes
  waiting for incoming requests.
</DD>

<DT>Postlude.</DT>
<DD> Each PM2 node should eventually issue a call to the
  <TT>pm2_exit</TT> routine. This function waits for <EM>all</EM> local
  threads to terminate, including the local internal system threads
  spawned by the <TT>pm2_init</TT> function to serve the remote RPC
  requests.  Again, in <EM>literate PM2 programming</EM>, all nodes but
  node<SPACE/>0 should immediately call the <TT>pm2_exit</TT> function after the
  <TT>pm2_init</TT>, just waiting for incoming requests through internal
  service threads. A call to <TT>pm2_exit</TT> should be issued
  <EM>exactly once by each node</EM>: unpredictable behavior may result
  from multiple calls to this routine at a single node. However, any
  thread of the node may call it: there is no reason why this should
  necessarily be done by the initial thread.
</DD>
</DL>

<P>
Of course, such a scheme does not fully meet our needs: the system
eventually reaches a state where only one thread remains living at
each node, blocked within a <TT>pm2_exit</TT> call, all the service threads
waiting for incoming requests. So to speak, the program has
terminated, but nobody knows it for sure, so that it may never stop!
This situation is well-known in the field of distributed algorithms:
it is called <EM>distributed termination</EM>. PM2 solves this
problem by providing the programmer with a special routine called
<TT>pm2_halt</TT>.  This routine should be called <EM>exactly once within
  all the nodes</EM>. By issuing such a call, the programmer indicates that
no request remains in transit and that no further request shall ever
be issued. The <TT>pm2_halt</TT> routine broadcasts a signal to all the
nodes of the configuration, whose effect is to stop all the system
service threads. Then, the <TT>pm2_exit</TT> routine can finally return at each
node, and the local copy of the program eventually terminates. Again,
issuing multiple calls to the <TT>pm2_halt</TT> routine, or issuing it
before all requests have been handled, will most probably lead to a
deadlock.
</P>

<P>
<NOTE>
  <EM>
  Luc to Olivier: Please, check the above section *very* carefully!
  </EM>
</NOTE>
</P>

</SUBSECTION>

<SUBSECTION TITLE="Migrating threads">

<P>
An interesting feature of PM2 is that it provides a mechanism for
<EM>preemptive thread migration</EM>. The principle is as follows. At any
point, a thread can request from the system the list of the threads
living on the same node, pick up a thread (including itself!)  for
migration to some remote processing node and trigger its effective
migration.  On such a request, the system suspends the selected thread
and removes it from the scheduler queue. The thread is marshaled
into a buffer (i.e., its memory image is copied), and physically
deleted at the node. A RPC is then issued to the remote node which has
been specified as the destination for the migration. The service function
takes the buffer as its argument. On the destination node, this
function unmarshals the buffer, installs the thread and finally
inserts it into the scheduler queue, ready to run.
</P>

<P>
Seen from the thread's point of view, the migration is completely
transparent, very much like a context switch. The only difference is
that only the <EM>proper resources</EM> of the thread have been
migrated: its descriptors, its stack and the memory areas which have
been allocated through a specialized <EM>iso-allocation</EM> routine
called <TT>pm2_isomalloc</TT>.  Thanks to this specialized
<EM>iso-allocation</EM> facility, all the thread's resources are
relocated at the <EM>same virtual addresses</EM> upon a migration, so
that pointers remain valid.  Yet, no provision is made at this level
for the data shared by the thread with other local threads, for
instance the global data of the program image running at the source
node. It is up to the programmer to make sure that the resulting
behavior is correct.
</P>

<P>
The concept of migration has been introduced above for one single
thread. In fact, PM2 provides a general migration facility which
enables the programmer to migrate <EM>several threads</EM> at a time,
each thread being migrated to a specific processing node. Also, it
should be stressed that the series of operations described above at
the origin node can be made atomic if desired, using specific PM2
routines.
</P>

</SUBSECTION>
</IGNORE>

</TEXT>

