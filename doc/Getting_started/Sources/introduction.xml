<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE TEXT SYSTEM "file:///net/cvs/runtime/usr/xml2html/data/xml/DOCUMENT.dtd">

<TEXT>

<P>
PM2 is developed at LaBRI (<EM>Laboratoire Bordelais de Recherche en
Informatique</EM>), a research laboratory located in Bordeaux, France,
jointly supported by the INRIA, the CNRS, and the University of
Bordeaux I. Before that, PM2 was developed at LIP (<EM>Laboratoire de
l'Informatique du Parallélisme</EM>), a research laboratory located at
the ENS Lyon (<EM>Ecole Normale Supérieure de Lyon</EM>) France,
jointly supported by the INRIA and CNRS institutions. PM2 was
originally designed at LIFL, University of Lille, France.
</P>

<P>
PM2 (<EM>Parallel Multithreaded Machine</EM>) is a distributed
multithreaded programming environment designed to efficiently support
irregular parallel applications on distributed architectures of SMP
nodes. PM2 is primarily designed for medium-size clusters of commodity
processing nodes interconnected by high-performance networks. However,
nothing in the design of PM2 prevents from using it on massively
parallel MIMD machines at one end of the spectrum, or as a support for
metacomputing over the Internet on the other end. Actually, a new
version of PM2's communication library has been developed to support
heterogeneous networking configurations, such as sets of
interconnected clusters.
</P>

<P>
The thread management subsystem of PM2 is called Marcel. It is
designed to manage several hundreds of threads on each available
physical processor. The PM2 programming interface provides
functionalities for the management of this high degree of parallelism
and for dynamic load balancing. Interesting features of PM2 include
its priority driven scheduling policy, its thread migration mechanism
and its ability to support various load balancing policies. PM2 has
been designed to provide threads as light as possible: the switching
time is well under the micro-second. Yet, Marcel is completely written
in portable C, but a dozen of lines of assembly code, which makes it
portable across most processors and flavors of Unix systems, including
Solaris and Linux. However, significant improvements have been made
for Linux by introducing specific support into the operating system.
</P>

<P>
The communication subsystem of PM2 is called Madeleine. It is
specifically designed to provide PM2 threads with efficient RPC
facilities on modern high-performance networks. In particular,
Madeleine strives at providing zero-copy communication in user-space
to obtain a minimal latency and a maximal bandwidth: less than
10 <MICROSECOND/> latency and Gigabit/s bandwidth have been reported for
Myrinet networks.  Yet, the design has been careful enough to keep
Madeleine portable across a wide range of communication interfaces,
including TCP and MPI. This makes PM2 portable on most Unix
platforms.
</P>

<SUBSECTION TITLE="The PM2 Programming model">

<P>
PM2 follows the SPMD (<EM>Single Program Multiple Data</EM>)
programming model, in a way very similar to the PVM and MPI
communication libraries.  Users write a single program text, a
copy of which is launched by a specific <TT>load</TT> command on each
<EM>processing node</EM> of the current configuration. Then,
it is up to this common program text to include branching so as to
differentiate between the processing nodes, based on a programming
scheme like

<CODE FILE="./Sources/code/init.c"/>
</P>

<P>
At this level of presentation, a processing node is simply a Unix
process. The association between <EM>processing nodes</EM> and
<EM>physical SMP nodes</EM>, or even <EM>physical processors of the
  SMP nodes</EM> are made outside of the PM2 model by specific commands
described below.
</P>

<P>
On each node, the <TT>pm2_main</TT> function of the program is activated.
This function may then <EM>spawn</EM> (i.e., create) an arbitrary
number of children threads.  PM2 adheres as much as possible to the
POSIX Threads programming model. A PM2 thread is just declared as an
ordinary function. The only difference is that it is called through a
specific library routine, which makes it run <EM>concurrently</EM> to its
caller function. The caller and the callee share all the global
variables of the local copy of program. Yet, the callee is launched
with a fresh stack. This implies that there is no return for a
function launched as a thread. On returning from the initial
activation of its function, the thread merely dies.  Observe also that
the caller and the callee functions run concurrently: once launched,
there is no longer a notion of <EM>father</EM> and <EM>son</EM>,
and the set of threads is just flat: if one of the running threads
spawns a new thread, then it is just added to the current, flat pool
of running threads, whatever its creator.  Also, any thread may
terminate the program, whatever its rank of creation.
</P>

</SUBSECTION>

<SUBSECTION TITLE="Threads and RPCs">

<P>
Once spawned, a thread runs until its function returns. Threads living
on the same node may synchronize by a number of means, as defined by
the POSIX Threads model: semaphore, monitors, etc. Threads living on
distinct nodes may not directly interact together. The only way a PM2
thread may interact with a remote node is to issue a <EM>remote
  procedure call</EM> (RPC): the <EM>client</EM> thread requires the
execution of a <EM>service</EM> function on the remote, <EM>server</EM>
node. PM2 provides the programmer with many flavors of RPCs. The
client thread may or may not wait for the result of the function; the
argument of the service function may be packed and unpacked in various
ways, depending on their use; the server node may or may not create a
new, fresh thread to execute the service function. This latter
alternative deserves some comments. In many thread packages, the
default behavior is that a fresh thread is spawned on the server node to
execute the service function, so that the node is left available to
handle further requests. However, such a behavior induces an
additional overhead which may be prohibitive with respect to the requested
service. In PM2, the server node is left to execute the service
function <EM>in person</EM> so as to save this overhead. As a
side-effect, this keeps the node busy and unable to serve any further
request: it is up to the programmer to make sure that no deadlock may
result. Of course, service functions may also dynamically spawn
threads if necessary, hence getting back to the usual behavior.
</P>

<P>
Synchronization between threads is a crucial aspect of multithreaded
programming. If threads live on the same node (i.e. within the same
Unix process), they can synchronize through <EM>locks</EM>, according
to the POSIX Threads model. If they live on distinct nodes, then PM2
defines special objects called <EM>completions</EM>. In short, a
completion is a special, self-described token which can be generated
by a thread, and then forwarded to some other threads. The originating
thread can then <TT>wait</TT> for the completion to be sent back to
itself using a special <TT>signal</TT> routine. This routine may be
executed by any thread, including a thread living on the same node as
the originator.
</P>

</SUBSECTION>

<SUBSECTION TITLE="Starting and stopping programs">

<P>
How does a PM2 SPMD distributed program start and stop? This is a
difficult problem, which requires much attention from the programmer
as it is a never-ending source of subtle failures (the writer has
painfully experienced it!). The execution of a PM2 program can be
divided in three phases.

<DL>
  
<DT>Prelude.</DT>
<DD> The prelude starts when launching the <TT>pm2_main</TT> root
  function of the program. This function should include all necessary
  initializations, especially the registration of the service
  functions which may be activated by remote clients
  (<TT>pm2_rawrpc_register</TT> routine), and more generally all objects
  which may be seen from remote nodes. This phase ends with a call to
  the <TT>pm2_init</TT> routine, which spawns the servicing threads to
  listen to external requests (RPC, migration, etc.).
</DD>

<DT>Main part.</DT>
<DD> After returning from the <TT>pm2_init</TT> routine, all
  PM2 nodes can freely proceed, behaving both as client and 
  server. In general, it is considered as <EM>literate PM2
    programming</EM> to let only node<SPACE/>0 to be active, all the other nodes
  waiting for incoming requests.
</DD>

<DT>Postlude.</DT>
<DD> Each PM2 node should eventually issue a call to the
  <TT>pm2_exit</TT> routine. This function waits for <EM>all</EM> local
  threads to terminate, including the local internal system threads
  spawned by the <TT>pm2_init</TT> function to serve the remote RPC
  requests.  Again, in <EM>literate PM2 programming</EM>, all nodes but
  node<SPACE/>0 should immediately call the <TT>pm2_exit</TT> function after the
  <TT>pm2_init</TT>, just waiting for incoming requests through internal
  service threads. A call to <TT>pm2_exit</TT> should be issued
  <EM>exactly once by each node</EM>: unpredictable behavior may result
  from multiple calls to this routine at a single node. However, any
  thread of the node may call it: there is no reason why this should
  necessarily be done by the initial thread.
</DD>
</DL>
</P>

<P>
Of course, such a scheme does not fully meet our needs: the system
eventually reaches a state where only one thread remains living at
each node, blocked within a <TT>pm2_exit</TT> call, all the service threads
waiting for incoming requests. So to speak, the program has
terminated, but nobody knows it for sure, so that it may never stop!
This situation is well-known in the field of distributed algorithms:
it is called <EM>distributed termination</EM>. PM2 solves this
problem by providing the programmer with a special routine called
<TT>pm2_halt</TT>.  This routine should be called <EM>exactly once within
  all the nodes</EM>. By issuing such a call, the programmer indicates that
no request remains in transit and that no further request shall ever
be issued. The <TT>pm2_halt</TT> routine broadcasts a signal to all the
nodes of the configuration, whose effect is to stop all the system
service threads. Then, the <TT>pm2_exit</TT> routine can finally return at each
node, and the local copy of the program eventually terminates. Again,
issuing multiple calls to the <TT>pm2_halt</TT> routine, or issuing it
before all requests have been handled, will most probably lead to a
deadlock.
</P>

<P>
<NOTE>
  Luc to Olivier: Please, check the above section *very* carefully!
</NOTE>
</P>

</SUBSECTION>

<SUBSECTION TITLE="Migrating threads">

<P>
An interesting feature of PM2 is that it provides a mechanism for
<EM>preemptive thread migration</EM>. The principle is as follows. At any
point, a thread can request from the system the list of the threads
living on the same node, pick up a thread (including itself!)  for
migration to some remote processing node and trigger its effective
migration.  On such a request, the system suspends the selected thread
and removes it from the scheduler queue. The thread is marshaled
into a buffer (i.e., its memory image is copied), and physically
deleted at the node. A RPC is then issued to the remote node which has
been specified as the destination for the migration. The service function
takes the buffer as its argument. On the destination node, this
function unmarshals the buffer, installs the thread and finally
inserts it into the scheduler queue, ready to run.
</P>

<P>
Seen from the thread's point of view, the migration is completely
transparent, very much like a context switch. The only difference is
that only the <EM>proper resources</EM> of the thread have been
migrated: its descriptors, its stack and the memory areas which have
been allocated through a specialized <EM>iso-allocation</EM> routine
called <TT>pm2_isomalloc</TT>.  Thanks to this specialized
<EM>iso-allocation</EM> facility, all the thread's resources are
relocated at the <EM>same virtual addresses</EM> upon a migration, so
that pointers remain valid.  Yet, no provision is made at this level
for the data shared by the thread with other local threads, for
instance the global data of the program image running at the source
node. It is up to the programmer to make sure that the resulting
behavior is correct.
</P>

<P>
The concept of migration has been introduced above for one single
thread. In fact, PM2 provides a general migration facility which
enables the programmer to migrate <EM>several threads</EM> at a time,
each thread being migrated to a specific processing node. Also, it
should be stressed that the series of operations described above at
the origin node can be made atomic if desired, using specific PM2
routines.
</P>

<P>
PM2 features an additional functionality to provide threads with a
uniform access to data, whatever their physical location. It is called
DSM-PM2. The idea is that the programmer can register certain global
data in to be <EM>uniformly shared</EM> by all the threads. This can be
done <EM>statically</EM> by using a special declaration for the global
variables of the program, or <EM>dynamically</EM> by allocating memory
areas with a specific allocation function. Exactly like the
<TT>pm2_isomalloc</TT> function above, the system guarantees that all
pointers remain valid on all nodes, so that these memory areas can be
used completely transparently by the programmer.  DSM-PM2 is
parameterized by the consistency protocol to be used for managing
shared data. A number of built-in protocols are provided in the
standard distribution for various consistency models, including
<EM>sequential consistency</EM> ones (for instance, Li-Hudak), and
<EM>release consistency</EM>, with several alternative protocols for
each of them. The protocol to be used is actually specified within the
prelude phase (before the call to <TT>pm2_init</TT>) using a special routine
called <TT>dsm_set_default_protocol</TT>. The user may thus switch from one
protocol to another without recompiling the program! Also, additional
protocols can be dynamically constructed by the user using the basic
blocks provided by the library. A typical protocol may be specified at
this level within a few hundreds of lines.
</P>

</SUBSECTION>

</TEXT>

