<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE TEXT SYSTEM "file:../xml2html/data/xml/DOCUMENT.dtd">

<TEXT>

<NOTE>
  <EM>
  A mettre a jour
  Nathalie: Parler de Madeleine et Marcel
  </EM>
</NOTE>

<P>
Congratulations! You are now knowledgeable in PM2, ready to write your
own programs.  Interested in getting to the <EM>Master</EM> level? Just
keep on reading!
</P>

<SUBSECTION TITLE="Invoking services synchronously">

<P>
The problem of gracefully terminating a distributed program is at the
very heart of this area of programming. The problem is even sharper
with PM2 programs because of the multithreaded behavior which
dramatically increases the asynchronism.
</P>

<P>
PM2 provides programmers with a simple mechanism to solve the
problem, at least in most cases. It is called <EM>completion</EM>.
Essentially, a <EM>completion</EM> is a token which is passed along
during the computation. The idea is that the creator of the token
sends it along to its communication partner. It may eventually block and
<TT>wait</TT> until receiving back the completion token. When the partner has
completed its task, it can <TT>signal</TT> it to the token creator by
sending back the token to it. Observe that the completion token can
travel across many partners before being used. 
</P>

<PROGRAM FILE="./Progs/rpc-completion.c" TITLE="Using completion token to gracefully terminate a PM2 program" LABEL="prog:rpc-completion"/>

<P>
Consider a typical use of such a mechanism in
Program <REF LABEL="prog:rpc-completion"/>. It is a minimal variation of
Program <REF LABEL="prog:rpc-threads"/>. The only difference is that node<SPACE/>0 is
now in charge of triggering the termination by calling the <TT>pm2_halt</TT>
routine. As observed previously, it cannot do it safely in general.
It has to make sure beforehand that all the communications have
completed. This is precisely the role of the completion token. Node<SPACE/>0
creates the token <TT>c</TT> with the <TT>pm2_completion_init</TT> routine. Observe
that only node<SPACE/>0 does it, and that it occurs in the concurrent phase
of the program, after the call to <TT>pm2_init</TT>. Two extra parameters
can be specified on creating a completion token: a specific handler
function and additional arguments.  Setting the last two
parameters to the default <TT>NULL</TT> value is sufficient in most usual
cases.
</P>

<P>
Then, the completion token is handled very much like an ordinary
object. It can be packed together with other data into a message
using the <TT>pm2_pack_completion</TT> routine, with the usual mode flags.
It can be similarly unpacked with the <TT>pm2_unpack_completion</TT>
routine.  Finally, one can send back the token to its creator using
the <TT>pm2_completion_signal</TT> routine. On its side, the token creator
blocks and waits for the token using the <TT>pm2_completion_wait</TT>
routine. In contrast with Program <REF LABEL="prog:rpc-threads"/>, it is now
guaranteed that the <TT>pm2_halt</TT> function is called on node<SPACE/>0
<EM>after</EM> the thread on node<SPACE/>1 has completed its job!
</P>

<P>
Any number of tokens can be used within a PM2 program. Also, observe
that the <TT>completion_signal</TT> involves RPC communications in the
background. It is up to the user to guarantee that no deadlock may
occur.
</P>

<NOTE>
  <EM>
  LB to RN: Please, proofread and expand the last paragraph. I am not
  sure about the (probably many) pitfalls here...
  </EM>
</NOTE>

</SUBSECTION>

<SUBSECTION TITLE="Thread migration">

<SUBSUBSECTION TITLE="The basic case: migrating oneself" LABEL="sec:self-migration">

<P>
We now turn to the most amazing feature of PM2: thread migration. The
essential idea is quite simple. At some point, a thread can request
the local scheduler to migrate some given thread to another node. This
<EM>victim</EM> thread may even be the calling thread itself. The victim
is dynamically preempted in the scheduler queue. Its set of resources
(descriptors, private stack, dynamically allocated memory area) are
packed into a buffer (<EM>marshaling</EM>) and a RPC is issued to the
destination node with the packed resources as argument.  The service
associated with this RPC consists in creating a new thread, granting
it all the packed resources and restarting it.  As far as the victim
thread is concerned, this is completely transparent.  It is not
<EM>aware</EM> of the migration, which merely amounts to an ordinary
context switch.  Yet, the global environment of the thread is now the
one provided by the new node: global variables, node identifier, etc.
In particular, thread migration does not make any attempt to keep the
system descriptors consistent across the migration: opened files, pipes,
signals, etc.  Mixing migration and I/O requires uttermost care.
</P>

<P>
Obviously, some circumstances require a thread to protect itself
against migration. This occurs for instance on exchanging messages,
doing I/O, etc. PM2 provides programmers with the
<TT>pm2_enable_migration</TT> and <TT>pm2_disable_migration</TT> routines to
manage this aspect. By default, the migration is <EM>disabled</EM>.
</P>

<PROGRAM FILE="./Progs/migration.c" TITLE="Migrating a thread across the configuration" LABEL="prog:migration"/>

<P>
Program <REF LABEL="prog:migration"/> is a simple example of thread migration.
Each of the nodes of the configuration stores into its global variable
<TT>hostname</TT> the name of the hosting physical machine. Then, node<SPACE/>0
spawns a thread which declares itself ready for migration. This thread
migrates from node to node in a round-robin way for a couple of hops,
and eventually stops on a node (which happens not to be the same as
its starting node). The <TT>pm2_halt</TT> routine is called on the final
node. 
</P>

<P>
On each hop, it prints the node number provided by PM2 together with
the contents of the variable <TT>hostname</TT> of its <EM>current global
  environment</EM>. Observe that this variable is set in the prelude
phase, <EM>before</EM> the call to the <TT>pm2_init</TT> routine. It would be
incorrect to do it later, as no assumption may be made about the time
the incoming thread will be unpacked and restarted on the destination
node.
</P>

<PROGRAM FILE="./Progs/migration.txt"/>

<P>
As usual, there is no restriction whatsoever on the association
between the physical machine and the logical nodes of PM2. For
instance, you could run the program on the same physical node.
It works just as well!
</P>

<PROGRAM FILE="./Progs/migration2.txt"/>

</SUBSUBSECTION>

<SUBSUBSECTION TITLE="Combining migration and completion">

<P>
The main problem in the previous program is to <EM>gracefully</EM> terminate
the computation. The easy solution implemented in
Program <REF LABEL="prog:migration"/> was to let the moving thread initiate the
termination. However, this is obviously a rather specific trick, which
does not generalize!
</P>

<P>
The idea is to combine the migration facility together with the
completion one. On spawning a migrating thread, the spawning thread
generates a completion token and passes it on to the migrating thread.
Then, it blocks, waiting on the completion to be signaled. When the
thread has completed its series of hops, it signals the completion to
its originator, which can then initiates the termination phase.
Observe that this crucially relies on the fact that a completion
includes enough information to be routed back to its originator, and
that this information is physically stored <EM>explicitly</EM> in the
completion (in opposite to being accessed through pointers).  If the
completion token is stored within the resources of the moving thread,
for instance, its stack as a local variable, then it gets migrated
together with the thread and can be used from any node (including the
originator node!)
</P>

<PROGRAM FILE="./Progs/migration1.c" TITLE="Using completions in presence of migration" LABEL="prog:migration1"/>

<P>
This approach is exemplified in Program <REF LABEL="prog:migration1"/>. Observe
how the completion is passed from its originator to the moving thread.
The completion is created in a global variable on node<SPACE/>0. The first
action of the created thread is to copy the completion into a local
variable, so that is gets migrated together with the rest of the
resources. This uses a special routine called <TT>pm2_completion_copy</TT>.
</P>

<WARNING>
  <EM>
  You are strongly advised not to copy completion directly as in
  <TT>new<UNDERSCORE/>c = old<UNDERSCORE/>c</TT>, though it works in
  the current version of PM2.  Instead, you should say 
  <TT>pm2<UNDERSCORE/>completion<UNDERSCORE/>copy(<AMPERSAND/>new<UNDERSCORE/>c, <AMPERSAND/>old<UNDERSCORE/>c)</TT>
  to guarantee that a
  <B>physical</B> copy of the relevant information is properly
  stored into the completion.  Unfortunately for oblivious users,
  PM2 does very little verification about the proper use of such
  auxiliary routines. It also provides few facilities to debug them.
  Just believe me: You'd better write a correct code at the first
  attempt!
  </EM>
</WARNING>

<PROGRAM FILE="./Progs/migration2.c" TITLE="Passing completions as parameters" LABEL="prog:migration2"/>

<P>
The technique used above, having the originator thread passing a
completion token to the moving thread, is rather contrived. A better
approach is exemplified in Program <REF LABEL="prog:migration2"/>. A pointer to
the completion is passed as an argument to the thread creation
function. The spawned thread allocates storage locally for a
completion, and copies the argument completion into its personal
resources using the <TT>pm2_completion_copy</TT> routine:
</P>

<PROGRAM FILE="./Progs/completionCopy2.c"/>

<P>
This saves the allocation of some global storage and it is much
cleaner. Yet, observe that this works because the storage for the
original completion is guaranteed to remain accessible at the time of
the copy. There is a subtle synchronization game here, as it will be
only destroyed <EM>after</EM> the completion is signaled back to its
originator. In case of doubt, it would have been much safer to
allocate the original completion in the global space, as in
Program <REF LABEL="prog:migration"/>.
</P>

<PROGRAM FILE="./Progs/migration3.c" TITLE="Passing completions through a RPC call" LABEL="prog:migration3"/>

<P>
If the moving thread is started on a <EM>remote</EM> node using a RPC,
then the completion can be passed as an argument to the RPC call, as
demonstrated in Program <REF LABEL="prog:migration3"/>. This is
probably the cleanest way of all. Unfortunately, the semantics of PM2
does not allow to perform a RPC on the local node, which rules out
this technique in the case of a local creation.
</P>

</SUBSUBSECTION>

<SUBSUBSECTION TITLE="Migrating other threads">

<P>
In Section <REF LABEL="sec:self-migration"/>, we have considered the simple
case: a thread migrates itself off its current node. The migration
facility of PM2 is much more powerful, as it allows a thread to
migrate other sibling threads, providing these threads have agreed
to get migrated.
</P>

<PROGRAM FILE="./Progs/migrate-group.c" TITLE="Migrating off a group of threads" LABEL="prog:migrate-group"/>

<P>
Consider Program <REF LABEL="prog:migrate-group"/>. It
is a good opportunity to demonstrate advanced techniques for threads
management. The idea is the following: Node<SPACE/>0 spawns 4<SPACE/>children
threads, numbered from<SPACE/>0 to<SPACE/>3. (Remind that threads disregard
migration by default.)  Then, the children threads with an even number
agree on migrating, and all children threads loop, repeatedly yielding
control to the scheduler with the <TT>marcel_yield</TT> routine until the variable
<TT>barrier</TT> takes a non null value.  When such a value has been
recorded, the children threads issue a message and terminate. On its
side, the master thread on node<SPACE/>0 records the list of <TT>MIGRATABLE</TT>
threads, and requests the migration of all of them to node<SPACE/>1. Then, it
issues a RPC on node<SPACE/>1 to set the variable <TT>barrier</TT> to 1, and does so
for its own node. Finally, the master thread calls the <TT>halt</TT> routine
to stop all daemon service threads.
</P>

<PROGRAM FILE="./Progs/migration3.txt"/>

<P>
Let's now look more closely at the program. First, observe how the
master thread spawns the children threads and synchronizes with them
using a completion token. Each child thread is created with two
parameters: a completion token <TT>c</TT> and a number <TT>i</TT>. As
the thread creation routine only accepts one argument of type <TT>void
*</TT>, the two parameters have to be packed into a structure; a
pointer to the structure is passed as the argument of the
<TT>pm2_thread_create</TT> routine; the structure is eventually
unpacked by the thread.
</P>

<WARNING>
  <EM>
  This scheme is correct <B>only</B> because of the synchronization
  implemented by the completion token: the master thread cannot spawn
  another thread before the current thread has unpacked the structure.
  Once the completion token has been received from each children
  thread, then the master thread is guaranteed that all of them have
  been created, and have decided upon whether they enable their own
  migration or not.
  </EM>
</WARNING>

<P>
Consider now how the master thread can migrate the children threads.
The first step is to store the children threads to be migrated into an
array.  This can be done by hand, but PM2 provides a specialized
routine to do the work. A call 
</P>

<PROGRAM FILE="./Progs/threadslist.c"/>

<P>
takes as parameters an array of threads
<TT>threads[N]</TT> (a PM2 thread has the type <TT>marcel_t</TT>) and a selection
condition. A number of condition are available, which can be or-ed and
and-ed as usual for conditional flags. Here, we use the <TT>MIGRATABLE_ONLY</TT> flag,
which does exactly what you guess. Well, not exactly, as the flag is
(silently) and-ed with the <TT>NOT_BLOCKED_ONLY</TT> flag: only the threads
which have enabled migration <EM>and</EM> are not currently blocked are
stored into the array. At most <TT>N</TT> such threads are stored, and the
number of actually stored threads is recorded into <TT>n</TT>. Observe that
the calling thread is listed, too, if it meets the condition. This is
not the case in this example.
</P>

<P>
You may object that this sampling operation may yield inconsistent
results as the sampled threads are running asynchronously with the
sampling one, very much as in the <TT>ps</TT> command of Unix. This is
not the case, as the <TT>pm2_threads_list</TT> routine internally
disables Marcel, the thread scheduler of PM2 using the
<TT>lock_task</TT>/<TT>unlock_task</TT> internal routines of
Marcel. However, users have no control about the exact sampling time,
but through the explicit synchronization which have been introduced
into the program.  Once the list of <EM>victim</EM> threads to be
migrated has been constructed, then PM2 can migrate all of them to a
common node using the <TT>pm2_migrate_group</TT> routine. This node
may well be the local node.  Of course, attempting to migrate a thread
which has disabled migration to a remote node raises an error.
</P>

<P>
This raises a new problem: how can user make sure that the threads
sampled out with the <TT>pm2_threads_list</TT> routine are still enabling
migration? Or even, that they are still running on the local node? Or
even, that they are still alive? Well, the only general solution is to 
guarantee the atomicity of the two successive calls. This is the role
of the <TT>pm2_freeze</TT> routine: it disables the scheduler, so that the
state of the other threads may not change until the <TT>pm2_unfreeze</TT>
routine is called. Of course, this also suspends all daemon service
threads, so that no message can be handled. This is up to users to
guarantee that freezing the scheduling will not cause any deadlock in
the system, for instance by overfilling communication buffers. Also, 
calls to <TT>pm2_freeze</TT>/<TT>pm2_unfreeze</TT> may be nested. It is the
responsibility of users to ensure proper nesting.
</P>

<P>
Well, well, well... Imagine the following scenario. Consider a thread
which enables <EM>its own migration</EM>; it freezes the scheduling on the
<EM>local</EM> node, samples the migratable threads, and calls the
<TT>pm2_migrate_group</TT> routine to migrate them away to some
<EM>remote</EM> node. It is itself migrated! Calling the <TT>pm2_unfreeze</TT>
routine afterward will most probably raise an error, as no call to
<TT>pm2_freeze</TT> has ever been made on the <EM>remote</EM> node. Also,
nobody is left on the <EM>local</EM> node to unfreeze the scheduler, and
this would most probably result in a deadlock.  Interesting problem,
isn't it? The only solution is that PM2 calls the <TT>pm2_freeze</TT>
routine on the local node <EM>on behalf</EM> of the migrated thread.
This somewhat non standard call is actually included at the end of the
<TT>pm2_migrate_group</TT> routine.  This is why there is no <EM>explicit</EM>
call to <TT>pm2_unfreeze</TT> in our program.  It is in
fact hidden within the call to <TT>pm2_migrate_group</TT>.
</P>

<P>
Observe that Program <REF LABEL="prog:migrate-group"/> uses a
variable <TT>barrier</TT> which is asynchronously read by many threads and
written by one thread. Strictly speaking, it is not correct to do so,
as there is no reason why the writing operation should be atomic.
Actually, the readers could in general read various intermediate
values from the variable, depending for instance on the order in which
the bits are written. One should guard all accesses to a shared
variable with a <EM>mutual exclusion object</EM> (a mutex, for short).
Mutexes are described in Section <REF LABEL="sec:mutex"/>.  However, the
behavior of this very program does not depend on concurrent reading
and writing, as the variable is only asynchronously tested against<SPACE/>0.
Spotting a bit set to<SPACE/>1 early or late does not matter: for each thread,
once a non-null value has been read, then no null value will ever be
read afterward. Such a situation is well-known in the area of
distributed algorithm: it is called a <EM>stable condition</EM>. 
</P>

<NOTE>
  <EM>
  LB to RN: Difficult section. Please, proofread!
  </EM>
</NOTE>

</SUBSUBSECTION>

<SUBSUBSECTION TITLE="Iso-allocation and dynamically allocated data">

<P>
On migrating, a thread is moved together with its private resources:
its private descriptors and its private stack. All these resources are
allocated by PM2 so that the virtual addresses remains invariant upon
migration. This allows to freely use pointers in the stack of a
migrating thread. This is exemplified in
Program <REF LABEL="prog:iso-alloc"/>. The variable <TT>x</TT> is allocated in the
private stack of the thread on node<SPACE/>1. After migration to node<SPACE/>2, the value
of <TT>x</TT> <EM>and its address</EM> are left unchanged.
</P>

<PROGRAM FILE="./Progs/iso-alloc.c" TITLE="Iso-allocation enables unlimited use of pointers in migrating threads" LABEL="prog:iso-alloc"/>

<PROGRAM FILE="./Progs/isoalloc.txt"/>

<P>
This amazing feature is made possible thanks to the
<EM>iso-allocation</EM> technique used by PM2 to allocate memory
dynamically. The whole virtual space of the nodes is managed in a
consistent way. If some storage is granted to some thread at one node,
then the iso-allocation technique guarantees that the corresponding
range of virtual addresses will not be used by any other node in the
configuration. Then, on a migration, the storage area can just get
copied exactly at the same virtual address in the destination node.
All pointers remain valid without any dependency on the compiler nor
any optimization strategy! You can therefore freely manipulate linked
lists within migrating threads without the smallest worrying...
</P>

<P>
All storage requests in PM2 are handled with this technique, including
the allocated storage for the private stack on creating a fresh
thread.  However, it is also available for explicit, dynamic storage
allocation by programmers, as a substitute for the
<TT>malloc</TT>/<TT>free</TT> functions. They are just called <TT>pm2_isomalloc</TT> and
<TT>pm2_isofree</TT>, with the same profiles as the usual <TT>malloc</TT> and
<TT>free</TT> functions.  Dynamic memory storage allocated with these
functions are included in the private resources of the threads and are
migrated together with the thread. Because of the iso-allocation
technique, the storage is mapped at the <EM>same</EM> virtual address in
all nodes, so that pointers can be freely used: you can use them
within a single storage area, but also across areas, and even between
the private stack and the storage areas.
</P>

<NOTE>
  <EM>
  LB to GA: Please, review this carefully!
  </EM>
</NOTE>

<P>
In Program <REF LABEL="prog:isomalloc"/>, the thread <EM>iso-allocates</EM>
two storage areas at addresses <TT>p</TT> and <TT>q</TT>. The variable located at
<TT>p</TT>, (that is, <TT>*p</TT>) holds the address <TT>q</TT>. The variable located a
<TT>q</TT> (that is, <TT>*q</TT>) holds some data. Thus, <TT>**p</TT> returns this
data through a double indirection. On allocating the storage areas, the
<TT>pm2_isomalloc</TT> function registers them in the private descriptors of
the thread. When the thread is about to migrate, the registered storage
areas are packed together with its descriptors and its stack. They are
unpacked on the destination node, and installed at the same virtual
address as in the origin node. This is why <TT>**p</TT> returns the same
value <EM>after</EM> the migration, as before. 
</P>

<PROGRAM FILE="./Progs/isomalloc.c" TITLE="Allocating dynamic storage which migrates together with the thread" LABEL="prog:isomalloc"/>

<PROGRAM FILE="./Progs/isomalloc.txt"/>

<P>
A thread may allocate as many such dynamic storage areas as
needed. Observe that this allocation primitive is compatible with the
usual <TT>malloc</TT> primitives. A thread may well allocate storage
with both functions within the same program. However, only areas
allocated with <TT>pm2_isomalloc</TT> shall migrate together with the
thread. The result of accessing an allocated address with the ordinary
<TT>malloc</TT> function, after a migration is of course undefined and
will cause a segmentation fault when you try to access the pointer on
the destination node, as you can check by yourself when trying to
execute Program <REF LABEL="prog:isomalloc1"/>.
</P>

<PROGRAM FILE="./Progs/isomalloc1.c" TITLE="Ordinary allocation results in incorrect behavior in the presence of migration" LABEL="prog:isomalloc1"/>

</SUBSUBSECTION>

</SUBSECTION>

<SUBSECTION TITLE="Sharing data across nodes: Distributed Shared Memory">

<P>
Up to now, we have only considered regular threads, which share data
with their sibling threads within the same processing node, that is,
their hosting Unix process. In regular PM2 programming, no action is
made to manage such data and to preserve consistency on migration.
For this purpose, PM2 includes a special module called DSM-PM2 (DSM
stands for <EM>Distributed Shared Memory</EM>) to provide such a
functionality. Variables assigned within a <EM>shared storage area</EM>
can be accessed from any thread, whatever their hosting node. The system
guarantees the consistency of the values observed by the threads
according to a given semantic model, to be specified by users through
an initialization routine. 
</P>

<P>
Consider Program <REF LABEL="prog:dsm"/>. the variable <TT>shvar</TT> is declared to be
<EM>globally shared</EM> by all the threads of the program.  Each
processing node except for the master node, successively spawns a thread to
increment this single shared variable for 20<SPACE/>units, recording the
initial and the final value it observed.  Finally, the master node
prints the value of the variable.  As expected, the final value of the
variable is exactly as it would have been with a sequence of
increments on the master node.
</P>

<PROGRAM FILE="./Progs/dsm.c" TITLE="Distributed shared memory in PM2" LABEL="prog:dsm"/>

<PROGRAM FILE="./Progs/dsm.txt"/>

<P>
DSM-PM2 implements a complex protocol to guarantee that the final
result is the correct one. In this example, the variable is physically
installed on the master node which is called the <EM>home node</EM>. Each
time a thread attempts to <EM>read</EM> the value of the variable, it
gets a (read-only) copy of the variable from the master node, so that
repeated reads are very cheap. Each time a thread attempts
to<EM>write</EM> a new value into the variable, all the subsidiary
copies of the variable are invalidated, and the new value is stored
into the master copy of the variable at its home node. This protocol
is known in the literature as the Li-Hudak's protocol. It implements a
<EM>sequential consistency</EM>: everything happens as if all the
read/write actions had been applied <EM>sequentially</EM> to the
variable on its home node, though the order in which the operations
have been applied is unknown. This is why the program says in the
initialization phase: <TT>dsm_set_default_protocol(LI_HUDAK)</TT>.
</P>

<P>
Other protocols are available in DSM-PM2. For instance, Program <REF
LABEL="prog:dsm1"/> uses the protocol <TT>MIGRATE_THREAD</TT> instead
of <TT>LI_HUDAK</TT>, this protocol is based on dynamic thread
migration: each time a thread attempts to access a globally shared
variable, to read it or to write it, then it silently migrates to the
home node of the variable. 
</P>

<NOTE>
<P>
  <EM>
NF: It looks like threads migrate back to the first node? The following text seems to be incorrect.
  </EM>
</P>
<P>
<EM>
The result is the same as above, but all
the incrementing threads end up on the home node of the shared
variable.
</EM>
</P>
</NOTE>

<PROGRAM FILE="./Progs/dsm1.c" TITLE="Distributed shared memory in PM2, using the 'MIGRATE_THREAD' sequential consistency protocol" LABEL="prog:dsm1"/>

<PROGRAM FILE="./Progs/dsm1.txt"/>

<P>
In the two above examples, each thread accessing the shared variable
<TT>shvar</TT> is launched in turn, waiting for the previous one to signal
completion. Then, no access conflict on the shared variable is to be
threatened. What if the thread attempt to increment the variable
<EM>concurrently</EM> as in Program <REF LABEL="prog:dsm2"/> ? Actually, the
incrementing instruction <TT>shvar++</TT> is compiled into an
<EM>sequence</EM> of elementary, atomic machine instructions, something
like:
</P>

<PROGRAM FILE="./Progs/tmp.c"/>

<P>
Thus, we have to add extra synchronization objects to guarantee that
this sequence of atomic instructions is actually executed in mutual
exclusion, as if it was a single instruction. DSM-PM2 provides
<EM>global</EM> mutexes for this purpose, as shown in Program <REF LABEL="prog:dsm2"/> .
Observe that this mutexes are much more powerful than the regular PM2
mutexes, which are local to a PM2 node.
</P>

<PROGRAM FILE="./Progs/dsm2.c" TITLE="Distributed shared memory in PM2, with concurrent access to shared variables" LABEL="prog:dsm2"/>

<PROGRAM FILE="./Progs/dsm2.txt"/>

<P>
Just for fun, you may wish to try out this last program
<EM>without</EM> the mutexes! This is legal in DSM-PM2, and the
sequential consistency semantics model for the DSM fully addresses
it. However, you should remember that the model specifies the behavior
at the level of elementary atomic instructions and there is therefore
no guarantee that the incrementation instruction <TT>shvar++</TT> will
be executed atomically.
</P>

</SUBSECTION>

</TEXT>
