\chapter{Mastering PM2}

\stamp $Id: mastering.tex,v 1.4 2001/04/18 15:13:13 bouge Exp $

Congratulation! You are now knowledgeable in PM2, ready to write your
own programs.  Interested in getting to the \emph{Master} level? Just
keep on reading!

\section{Invoking services synchronously}

The problem of gracefully terminating a distributed program is at the
very heart of this area of programming. The problem is even sharper
with PM2 programs because of the multithreaded behavior which
dramatically increases the asynchronism.

\figurelisting {rpc-completion.c} {Using completion token to
  garcefully terminate a PM2 program} {prog:rpc-completion}

PM2 provides the programmer with a simple mechanism to solve the
problem, at least in most cases. It is called \emph{completion}.
Essentially, a \emph{completion} is a token which is passed along
during the computation. The idea is that the creator of the token
sends it along to its communication partner. It may eventually block and
\|wait| for receiving back the completion token. When the partner has
completed its task, it can \|signal| it to the token creator by
sending back the token to it. Observe that the completion token can
travel across many partners before being used. 

Consider a typical use of such a mechanism in
Program~\ref{prog:rpc-completion}. It is a minimal variation of
Program~\ref{prog:rpc-threads}. The only difference is that node~0 is
now in charge of triggering the termination by calling the \|pm2_halt|
routine. As observed previously, it cannot do it safely in general.
It has to make sure beforehand that all the communications have
completed. This is precisely the role of the completion token. Node~0
creates token \|c| with the \|pm2_completion_init| routine. Observe
that only node~0 does it, and that it occurs in the concurrent phase
of the program, after the call to \|pm2_init|. Two extra parameters
can be specified on creating a completion token: a specific handler
function and additional arguments.  Setting the last two
parameters to the default \|NULL| value is sufficient in most usual
cases.

Then, the completion token is handled very much like an ordinary
object. It can be packed together with other data into a messages
using the \|pm2_pack_completion| routine, with the usual mode flags.
It can be similarly unpacked with the \|pm2_unpack_completion|
routine.  Finally, one can send back the token to its creator using
the \|pm2_completion_signal| routine. On its side, the token creator
blocks waiting for the token using the \|pm2_completion_wait|
routine. In contrast with Program~\ref{prog:rpc-threads}, it is now
guaranteed that the \|pm2_halt| function is called on node~0
\emph{after} the thread on node~1 has completed its job!

Any number of tokens can be used within a PM2 program. Also, observe
that the \|completion_signal| involves RPC communications in the
background. It is up to the user to guarantee that no deadlock may
occur.

\begin{note}
  LB to RN: Please, proofread and expand the last paragraph. I am not
  sure about the (probably many) pitfalls here...
\end{note}

\section{Thread migration}

\figurelisting {migration.c} {Migrating a thread across the
  configuration} {prog:migration}

\subsection{The basic case: migrating oneself}
\label{sec:self-migration}

We now turn to the most amazing feature of PM2: thread migration. The
essential idea is quite simple. At some point, a thread can request
the local scheduler to migrate some given thread to another node. This
\emph{victim} thread may even be the calling thread itself. The victim
is dynamically preempted in the scheduler queue. Its set of resources
(descriptors, private stack, dynamically allocated memory area) are
packed into a buffer (\emph{marshaling}) and a RPC is issued to the
destination node with the packed resources as argument.  The service
associated with this RPC consists in creating a new thread, granting
it all the packed resources and restarting it.  As far as the victim
thread is concerned, this is completely transparent.  It is not
\emph{aware} of the migration, which merely amounts to an ordinary
context switch.  Yet, the global environment of the thread is now the
one provided by the new node: global variables, node identifier, etc.
In particular, thread migration does not make any attempt to keep the
system descriptors consistent across the migration: open files, pipes,
signals, etc.  Mixing migration and I/O has requires uttermost care.

Obviously, some circumstances require a thread to protect itself
against migration. This occurs for instance on exchanging messages,
doing I/O, etc. PM2 provides the programmer with a
\|pm2_enable_migration| and \|pm2_disable_migration| routines to
manage this aspect. By default, migration is \emph{disabled}.

Program~\ref{prog:migration} is a simple example of thread migration.
Each of the nodes of the configuration stores into its global variable
\|hostname| the name of the hosting physical machine. Then, node~0
spawns a thread which declares itself ready for migration. This thread
migrates from node to node in a round-robin way for a couple of hops,
and eventually stops on a node (which happens not to be the same as
its starting node). The \|pm2_halt| routine is called on the final
node. 

On each hop, it prints the node number provided by PM2 together with
the contents of variable \|hostname| of its \emph{current global
  environment}. Observe that this variable is set in the prelude
phase, \emph{before} the call to the \|pm2_init| routine. It would be
incorrect to do it later, as no assumption may be made about the time
the incoming thread will be unpacked and restarted on the destination
node.

\begin{shell}
ravel% pm2-conf ravel debussy faure
The current PM2 configuration contains 3 host(s) :
0 : ravel
1 : debussy
2 : faure

ravel% pm2-load migration
[Threads : 5 created, 2 imported (0 cached)]
Hop 0: I am on node 0, host ravel...
Hop 0: Leaving to node 1
Hop 3: I am on node 0, host ravel...
Hop 3: Leaving to node 1
Hop 6: I am on node 0, host ravel...
Hop 6: Leaving to node 1

ravel% pm2-logs
*** Process 1 on debussy:
[Threads : 5 created, 3 imported (0 cached)]
Hop 1: I am on node 1, host debussy...
Hop 1: Leaving to node 2
Hop 4: I am on node 1, host debussy...
Hop 4: Leaving to node 2
*** Process 2 on faure:
[Threads : 4 created, 2 imported (0 cached)]
Hop 2: I am on node 2, host faure...
Hop 2: Leaving to node 0
Hop 5: I am on node 2, host faure...
Hop 5: Leaving to node 0
\end{shell}
As usual, there is no restriction whatsoever on the association
between the physical machine and the logical nodes of PM2. For
instance, you could try to run the program on the same physical node.
It works just as well!
\begin{shell}
ravel% pm2-conf ravel ravel ravel
The current PM2 configuration contains 3 host(s) :
0 : ravel
1 : ravel
2 : ravel

ravel% pm2-load migration
[Threads : 5 created, 2 imported (0 cached)]
Hop 0: I am on node 0, host ravel...
Hop 0: Leaving to node 1
Hop 3: I am on node 0, host ravel...
Hop 3: Leaving to node 1
Hop 6: I am on node 0, host ravel...
Hop 6: Leaving to node 1

ravel% pm2-logs
*** Process 1 on ravel:
[Threads : 5 created, 3 imported (0 cached)]
Hop 1: I am on node 1, host ravel...
Hop 1: Leaving to node 2
Hop 4: I am on node 1, host ravel...
Hop 4: Leaving to node 2
*** Process 2 on ravel:
[Threads : 4 created, 2 imported (0 cached)]
Hop 2: I am on node 2, host ravel...
Hop 2: Leaving to node 0
Hop 5: I am on node 2, host ravel...
Hop 5: Leaving to node 0
\end{shell}

\subsection{Combing migration and completion}

\figurelisting {migration1.c} {Using completions in presence of
  migration} {prog:migration1}

The main problem in the previous program is to \emph{gracefully} terminate
the computation. The easy solution implemented in
Program~\ref{prog:migration} was to let the moving thread initiate the
termination. However, this is obviously a rather specific trick, which
does not generalize!

The idea is to combine the migration facility together with the
completion one. On spawning a migrating thread, the spawning thread
generates a completion token and passes it on to the migrating thread.
Then, it blocks, waiting on the completion to be signaled. When the
thread has completed its series of hops, it signals the completion to
its originator, which can then initiates the termination phase.
Observe that this crucially relies on the fact that a completion
includes enough information to be routed back to its originator, and
that this information is physically stored \emph{explicitly} in the
completion (in opposite to being accessed through pointers).  If the
completion token is stored within the resources of the moving thread,
for instance, its stack as a local variable, then it gets migrated
together with the thread and can be used from any node (including the
originator node!)

This approach is exemplified in Program~\ref{prog:migration1}. Observe
how the completion is passed from its originator to the moving thread.
The completion is created in a global variable on node~0. The first
action of the created thread is to copy the completion into a local
variable, so that is gets migrated together with the rest of the
resources. This uses a special routine called \|pm2_completion_copy|.

\begin{warning}
  You are strongly advised not to copy completion directly as in
  \|new_c = old_c|, though it works in the current version of PM2.
  Instead, you should say 
\begin{program}
pm2_completion_copy(&new_c, &old_c)
\end{program}
to guarantee that a \emph{physical} copy of the relevant information
is properly stored into the completion.  Unfortunately for the
oblivious user, PM2 does very little verification about the proper use
of such auxiliary routines. It also provide few facilities to debug
them.  Just believe me: You'd better write a correct code at the first
attempt!
\end{warning}

\figurelisting {migration2.c} {Passing completions as
  parameters} {prog:migration2}

The technique used above, having the originator thread passing a
completion token to the moving thread, is rather contrived. A better
approach is exemplified in Program~\ref{prog:migration2}. A pointer to
the completion is passed as an argument to the thread creation
function. The spawned thread allocate storage locally for a
completion, and copies the argument completion into its personal
resources using the \|pm2_completion_copy| routine:
\begin{program}
pm2_completion_copy(&my_c, (pm2_completion_t *)arg)
\end{program}
This saves the allocation of some global storage and it is much
cleaner. Yet, observe that this works because the storage for the
original completion is guaranteed to remain accessible at the time of
the copy. There is a subtle synchronization game here, as it will be
only destroyed \emph{after} the completion is signaled back to its
originator. In case of doubt, it would have been much safer to
allocate the original completion in the global space, as in
Program~\ref{prog:migration}.

\figurelistingdouble {migration3.c} {Passing completions through a RPC
  call} {prog:migration3}

If the moving thread is started on a \emph{remote} node using a RPC,
then the completion can be passed as an argument to the RPC call, as
demonstrated in Program~\ref{prog:migration3}. This is probably the
cleanest way of all. Unfortunately, the semantics of PM2 does not
allow a RPC to the local node, which rules out this technique in the
case of a local creation.

\subsection{Migrating other threads}

In the Section~\ref{sec:self-migration}, we have considered the simple
case: a thread migrates itself off its current node. The migration
facility of PM2 is much more powerful, as it allows a thread to
migrate off other sibling threads, provided these threads have agreed
on getting migrated.

\figurelistingdouble {migrate-group.c} {Migrating off a group of
  threads} {prog:migrate-group}

Consider the program displayed on Figure~\ref{prog:migrate-group}. It
is a good opportunity to demonstrate advanced techniques for threads
management. The idea is the following: Node~0 spawns 4~children
threads, numbered from~0 to~3. (Remind that threads disregard
migration by default.)  Then, the children threads with an even number
agree on migrating, and all children threads loop, repeatedly yielding
control to the scheduler with the \|marcel_yield| routine until variable
\|barrier| takes a non null value.  When such a value has been
recorded, the children threads issue a message and terminate. On its
side, the master thread on node~0 records the list of \|MIGRATABLE|
threads, and requests the migration of all of them to node~1. Then, it
issues a RPC to node~1 to set variable \|barrier| to 1, and does so
for its own node. Finally, the master thread calls the \|halt| routine
to stop all daemon service threads.
\begin{shell}
ravel% pm2-load migrate-group
Initialization completed on node 0
Thread 0: Created on node 0, host ravel
Thread 1: Created on node 0, host ravel
Thread 2: Created on node 0, host ravel
Thread 3: Created on node 0, host ravel
2 threads among 4 migrated off to node 1
Issuing RPC to node 1
Just halting
Thread 3: Now on node 0, host ravel
Thread 1: Now on node 0, host ravel
[Threads : 6 created, 0 imported (0 cached)]

ravel% pm2-logs

*** Host debussy, process 1:
[Threads : 4 created, 2 imported (0 cached)]
Initialization completed on node 1
Service activated on node 1, host debussy
Thread 0: Now on node 1, host debussy
Thread 2: Now on node 1, host debussy

*** Host faure, process 2:
[Threads : 2 created, 0 imported (0 cached)]
Initialization completed on node 2
\end{shell}

Let's now look more closely at the program. First, observe how the
master thread spawns the children threads and synchronize with them
using a completion token. Each child thread is created with two
parameters: a completion token \|c| and a number \|i|. As the thread
creation routine only accepts one argument of type \|void *|, an
auxiliary structure has to be used: the two parameters have to be
packed into a structure; a pointer to the structure is passed as the
argument of the \|pm2_thread_create| routine; the structure is
eventually unpacked by the thread. 

\begin{warning}
  This scheme is correct \emph{only} because of the synchronization
  implemented by the completion token: the master thread cannot spawn
  another thread before the current thread has unpacked the structure.
  Once one the completion token has been received from each children
  thread, then the master thread is guaranteed that all of them have
  been created, and have decided upon whether they enable their own
  migration or no.
\end{warning}

Consider now how the master thread can migrate the children threads.
The first step is to store the children threads to be migrated into an
array.  This can be done by hand, but PM2 provides a specialized
routine to do the work. A call \|pm2_threads_list(N, threads, &n,
MIGRATABLE_ONLY)| takes as parameters an array of threads
\|threads[N]| (a PM2 thread has type \|marcel_t|) and a selection
condition. A number of condition are available, which can be or-ed and
and-ed as usual for flags. Here, we use the \|MIGRATABLE_ONLY| flag,
which does exactly what you guess. Well, not exactly, as the flag is
(silently) and-ed with the \|NOT_BLOCKED_ONLY| flag: only the threads
which have enabled migration \emph{and} are not currently blocked are
stored into the array. At most \|N| such threads are stored, and the
number of actually stored threads is recorded into \|n|. Observe that
the calling thread is listed, too, if it meets the condition. This is
not the case in this example.

You may object that this sampling operation may yield inconsistent
result as the sampled threads are running asynchronously with the
sampling one, very much as in the \|ps| command of Unix. This is not
the case, as the \|pm2_threads_list| routine internally disables
Marcel, the thread scheduler of PM2 using the
\|lock_task|/\|unlock_task| internal routines of Marcel. However, the
user has no control about the exact sampling time, but through the
explicit synchronization which have been introduced into the program.
Once the list of \emph{victim} threads to be migrated has been
constructed, then PM2 can migrate all of them to a common node using
the \|pm2_migrate_group| routine. This node may well be the local
node.  Of course, attempting to migrate a thread which has disabled
migration to a remote node raises an error.

This raises a new problem: how can the user make sure that the threads
sampled out with the \|pm2_threads_list| routine are still enabling
migration? Or even, that they are still running on the local node? Or
even, that they are still alive? Well, the only general solution is to 
guarantee the atomicity of the two successive calls. This is the role
of the \|pm2_freeze| routine: it disables the scheduler, so that the
state of the other threads may not change until the \|pm2_unfreeze|
routine is called. Of course, this also suspends all daemon service
threads, so that no message can be handled. This is up to the user to
guarantee that freezing the scheduling will not cause any deadlock in
the system, for instance by overfilling communication buffers. Also, 
calls to \|pm2_freeze|/\|pm2_unfreeze| may be nested. It is the
responsibility of the user to ensure proper nesting.

Well, well, well... Imagine the following scenario. Consider a thread
which enables \emph{its own migration}; it freezes the scheduling on the
\emph{local} node, samples the migratable threads, and calls the
\|pm2_migrate_group| routine to migrate them away to some
\emph{remote} node. It is itself migrated! Calling the \|pm2_unfreeze|
routine afterwards will most probably raise an error, as no call to
\|pm2_freeze| has ever been made on the \emph{remote} node. Also,
nobody is left on the \emph{local} node to unfreeze the scheduler, and
this would most probably result in a deadlock.  Interesting problem,
isn't it? The only solution is that PM2 calls the \|pm2_freeze|
routine on the local node \emph{on behalf} of the migrated thread.
This somewhat non standard call is actually included at the end of the
\|pm2_migrate_group| routine.  This is why there is no \emph{explicit}
call to \|pm2_unfreeze| in our program.  It is in
fact hidden within the call to \|pm2_migrate_group|.

Observe that the program on Figure~~\ref{prog:migrate-group} uses a
variable \|barrier| which is asynchronously read by many threads and
written by one thread. Strictly speaking, it is not correct to do so,
as there is no reason why the writing operation should be atomic.
Actually, the readers could in general read various intermediate
values from the variable, depending for instance on the order in which
the bits are written. One should guard all accesses to a shared
variable with a \emph{mutual exclusion object} (a mutex, for short).
Mutexes are described in Section~\ref{sec:mutex}.  However, the
behavior of this very program does not depend on concurrent reading
and writing, as the variable is only asynchronously tested against~0.
Spotting a bit set to~1 early or late does not matter: for each thread,
once a non-null value has been read, then no null value will ever be
read afterwards. Such a situation is well-known in the area of
distributed algorithm: it is called a \emph{stable condition}. 

\begin{note}
  LB to RN: Difficult section. Please, proofread!
\end{note}


\subsection{Iso-allocation and dynamically allocated data}

\figurelisting {iso-alloc.c} {Iso-allocation enables unlimited use of
  pointers in migrating threads} {prog:iso-alloc}

On migrating, a thread is moved together with its private resources:
its private descriptors and its private stack. All these resources are
allocated by PM2 so that the virtual addresses remains invariant upon
migration. This allows to freely use pointers in the stack of a
migrating thread. This is exemplified in
Program~\ref{prog:iso-alloc}. Variable \|x| is allocated in the
private stack of the thread on node~1. After migration to node~2, the value
of \|x| \emph{and its address} are left unchanged.
\begin{shell}
ravel% pm2-logs
*** Process 1 on debussy:
[Threads : 3 created, 0 imported (0 cached)]
First, I am on node 1, host debussy... &x = affafda0, x = 1234
*** Process 2 on faure:
[Threads : 3 created, 1 imported (0 cached)]
Now, I am on node 2, host faure... &x = affafda0, x = 1234
\end{shell}

This amazing feature is made possible thanks to the
\emph{iso-allocation} technique used by PM2 to allocate memory
dynamically. The whole virtual space of the nodes is managed in a
consistent way. If some storage is granted to some thread at one node,
then the iso-allocation technique guarantees that the corresponding
range of virtual addresses will not be used by any other node in the
configuration. Then, on a migration, the storage area can just get
copied exactly at the same virtual address in the destination node.
All pointers remain valid without any dependency on the compiler nor
any optimization strategy! You can therefore freely manipulate linked
lists within migrating threads without the smallest worrying...

\figurelisting {isomalloc.c} {Allocating dynamic storage which
  migrates together with the thread} {prog:isomalloc}

All storage requests in PM2 are handled with this technique, including
the storage allocated for the private stack on creating a fresh
thread.  However, it is also available for explicit, dynamic storage
allocation by the programmer, as a substitute for the
\|malloc|/\|free| functions. They are just called \|pm2_isomalloc| and
\|pm2_isofree|, with the same profiles as the usual \|malloc| and
\|free| functions.  Dynamic memory storage allocated with these
functions are included in the private resources of the threads and are
migrated together with the thread. Because of the iso-allocation
technique, the storage is mapped at the \emph{same} virtual address in
all nodes, so that pointers can be freely used: you can use them
within a single storage area, but also across areas, and even between
the private stack and the storage areas.

\begin{note}
  LB to GA: Please, review this carefully!
\end{note}

In Program~\ref{prog:isomalloc}, the thread \emph{iso-allocates}
two storage area at addresses \|p| and \|q|. The variable located at
\|p|, (that is, \|*p|) holds the address \|q|. The variable located a
\|q| (that is, \|*q|) holds some data. Thus, \|**p| returns this
data through a double indirection. On allocating the storage areas, the
\|pm2_isomalloc| function registers them in the private descriptors of
the thread. When the thread is about migrating, the registered storage
areas are packed together with its descriptors and its stack. They are
unpacked on the destination node, and installed at the same virtual
address as in the origin node. This is why \|**p| returns the same
value \emph{after} the migration, as before. 
\begin{shell}
ravel% pm2-load isomalloc
First, I am on node 0, host ravel...
p = aff8f040, *p = aff8f080, **p = 1234
[Threads : 3 created, 0 imported (0 cached)]

ravel% pm2-logs
*** Host debussy, process 1:
[Threads : 3 created, 1 imported (0 cached)]
Then, I am on node 1, host debussy...
p = aff8f040, *p = aff8f080, **p = 1234
\end{shell}

\figurelisting {isomalloc1.c} {Ordinary allocation results in
  undefined behavior in the presence of
  migration} {prog:isomalloc1}

A thread may allocate as many such dynamic storage areas as
needed. Observe that this allocation primitive is compatible with the
usual \|malloc| primitives. A thread may well allocate storage with
both functions within the same program. However, only areas allocated
with \|pm2_isomalloc| shall migrate together with the thread. The
result of accessing an address allocated with the ordinary \|malloc|
function, after a migration is of course undefined as demonstrated by
Program~\ref{prog:isomalloc1}: the value of \|*p| is \emph{not}
preserved through the migration!
\begin{shell}
ravel% pm2-load isomalloc1
First, I am on node 0, host ravel...
p = 80d2500, *p = 1234
[Threads : 3 created, 0 imported (0 cached)]

ravel% pm2-logs
*** Host debussy, process 1:
[Threads : 3 created, 1 imported (0 cached)]
Then, I am on node 1, host debussy...
p = 80d2500, *p = 8
\end{shell}
