 
/* This file has been autogenerated from source/nptl_mutex.c.m4 */
/*
 * PM2: Parallel Multithreaded Machine
 * Copyright (C) 2001 "the PM2 team" (see AUTHORS file)
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or (at
 * your option) any later version.
 *
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 * General Public License for more details.
 */

#include "marcel.h"

#include <errno.h>

#include "marcel_fastlock.h"

#ifdef MA__LIBPTHREAD
DEF_LIBPTHREAD	(int,	mutex_init,		(pthread_mutex_t *m, const pthread_mutexattr_t *a), (m, a));
DEF_LIBPTHREAD	(int,	mutex_destroy,		(pthread_mutex_t *m), (m));
DEF_LIBPTHREAD	(int,	mutex_trylock,		(pthread_mutex_t *m), (m));
DEF_LIBPTHREAD	(int,	mutexattr_init,		(pthread_mutexattr_t *a), (a));
DEF_LIBPTHREAD	(int,	mutexattr_destroy,	(pthread_mutexattr_t *a), (a));
DEF_LIBPTHREAD	(int,	mutexattr_settype,	(pthread_mutexattr_t *a, int v), (a, v));
DEF_LIBPTHREAD	(int,	mutexattr_gettype,	(pthread_mutexattr_t *a, int v), (a, v));
DEF_LIBPTHREAD	(int,	mutexattr_setpshared,	(pthread_mutexattr_t *a, int v), (a, v));
DEF_LIBPTHREAD	(int,	mutexattr_getpshared,	(pthread_mutexattr_t *__restrict a, int *__restrict v), (a, v));

#  ifdef MARCEL_ONCE_ENABLED
DEF_LIBPTHREAD	(int,	once,			(pthread_once_t *ctrl, void (*init)(void)),(ctrl, init));
//DEF___LIBPTHREAD(void,	once_fork_child,	(void), ());
#  endif /* MARCEL_ONCE_ENABLED */

DEF___LIBPTHREAD(int,	mutex_init,		(pthread_mutex_t *m, const pthread_mutexattr_t * a), (m, a));
DEF___LIBPTHREAD(int,	mutex_destroy,		(pthread_mutex_t *m), (m));
DEF___LIBPTHREAD(int,	mutex_lock,		(pthread_mutex_t *m), (m));
DEF___LIBPTHREAD(int,	mutex_trylock,		(pthread_mutex_t *m), (m));
DEF___LIBPTHREAD(int,	mutex_unlock,		(pthread_mutex_t *m), (m));
DEF___LIBPTHREAD(int,	mutexattr_init,		(pthread_mutexattr_t *a), (a));
DEF___LIBPTHREAD(int,	mutexattr_destroy,	(pthread_mutexattr_t *a), (a));
DEF___LIBPTHREAD(int,	mutexattr_settype,	(pthread_mutexattr_t *a, int v), (a, v));
DEF___LIBPTHREAD(int,	mutexattr_gettype,	(pthread_mutexattr_t *a, int v), (a, v));
#  ifdef MARCEL_ONCE_ENABLED
DEF___LIBPTHREAD(int,	once,			(pthread_once_t *ctrl, void (*init)(void)), (ctrl, init));
DEF___LIBPTHREAD(void,	once_fork_prepare,	(void), ());
DEF___LIBPTHREAD(void,	once_fork_parent,	(void), ());
DEF___LIBPTHREAD(void,	once_fork_child,	(void), ());
#  endif /* MARCEL_ONCE_ENABLED */

versioned_symbol(libpthread,	lpt_mutex_lock,		pthread_mutex_lock,		GLIBC_2_0);
versioned_symbol(libpthread,	lpt_mutex_timedlock,	pthread_mutex_timedlock,	GLIBC_2_2);
versioned_symbol(libpthread,	lpt_mutex_unlock,	pthread_mutex_unlock,		GLIBC_2_0);

weak_alias	(lpt_mutexattr_settype,		pthread_mutexattr_setkind_np);
weak_alias	(lpt_mutexattr_gettype,		pthread_mutexattr_getkind_np);
#endif

#ifdef MA__IFACE_LPT
static const struct lpt_mutexattr lpt_default_attr = {
	/* Default is a normal mutex, not shared between processes.  */
	.mutexkind = LPT_MUTEX_NORMAL
};

int lpt_mutex_init (lpt_mutex_t *mutex, 
		       const lpt_mutexattr_t * mutexattr) {
        LOG_IN();
	const struct lpt_mutexattr *imutexattr;
        mdebug("initializing mutex %p by %p\n", mutex, marcel_self());

	MA_BUG_ON (sizeof (lpt_mutex_t) > __SIZEOF_LPT_MUTEX_T);

	imutexattr = (const struct lpt_mutexattr *) mutexattr 
		?: &lpt_default_attr;

	memset (mutex, '\0', sizeof(lpt_mutex_t));

	/* Copy the values from the attribute.  */
	mutex->__data.__kind = imutexattr->mutexkind & ~0x80000000;
        __lpt_init_lock(&mutex->__data.__lock);
        LOG_RETURN(0);
}
int lpt_mutex_destroy(lpt_mutex_t * mutex) {
        LOG_IN();
        LOG_RETURN(0);
}
int lpt_mutex_lock(lpt_mutex_t * mutex) {
        LOG_IN();
	struct marcel_task *id = MARCEL_SELF;

	MA_BUG_ON (sizeof (mutex->__size) < sizeof (mutex->__data));
        switch (__builtin_expect (mutex->__data.__kind, LPT_MUTEX_TIMED_NP)) {
		/* Recursive mutex.  */
	case LPT_MUTEX_RECURSIVE_NP:
		/* Check whether we already hold the mutex.  */
                        if (mutex->__data.__owner == id) {
			/* Just bump the counter.  */
			if (__builtin_expect (mutex->__data.__count + 1 == 0, 0))
				/* Overflow of the counter.  */
                                        LOG_RETURN(EAGAIN);
			++mutex->__data.__count;
                                LOG_RETURN(0);
		}
		
		/* We have to get the mutex.  */
		__lpt_lock(&mutex->__data.__lock, id);
		
		mutex->__data.__count = 1;
		break;
		
		/* Error checking mutex.  */
	case LPT_MUTEX_ERRORCHECK_NP:
		/* Check whether we already hold the mutex.  */
		if (mutex->__data.__owner == id)
                                LOG_RETURN(EDEADLK);

                default:
		
	case LPT_MUTEX_TIMED_NP:
	case LPT_MUTEX_ADAPTIVE_NP:
		/* Normal mutex.  */
		__lpt_lock(&mutex->__data.__lock, id);
                break;		      
	}
	
	/* Record the ownership.  */
	MA_BUG_ON (mutex->__data.__owner != 0);
	mutex->__data.__owner = id;
	++mutex->__data.__nusers;
	
                LOG_RETURN(0);
}
int lpt_mutex_trylock(lpt_mutex_t * mutex) {
        LOG_IN();
	struct marcel_task *id;

        switch (__builtin_expect (mutex->__data.__kind, LPT_MUTEX_TIMED_NP)) {
		/* Recursive mutex.  */
	case LPT_MUTEX_RECURSIVE_NP:
		id = MARCEL_SELF;
		/* Check whether we already hold the mutex.  */
                        if (mutex->__data.__owner == id) {
			/* Just bump the counter.  */
			if (tbx_unlikely (mutex->__data.__count + 1 == 0))
				/* Overflow of the counter.  */
                                        LOG_RETURN(EAGAIN);
			++mutex->__data.__count;
                                LOG_RETURN(0);
		}
		
                        if (__lpt_trylock(&mutex->__data.__lock) != 0) {
			/* Record the ownership.  */
			mutex->__data.__owner = id;
			mutex->__data.__count = 1;
			++mutex->__data.__nusers;
			return 0;
		}
		break;
		
	case LPT_MUTEX_ERRORCHECK_NP:
		/* Error checking mutex.  We do not check for deadlocks.  */
	default:
		/* Correct code cannot set any other type.  */
	case LPT_MUTEX_TIMED_NP:
	case LPT_MUTEX_ADAPTIVE_NP:
		/* Normal mutex.  */
                        if (__lpt_trylock(&mutex->__data.__lock) != 0) {
			/* Record the ownership.  */
			mutex->__data.__owner = MARCEL_SELF;
                                ++mutex->__data.__nusers;	
                                LOG_RETURN(0);
		}
	}
	
        LOG_RETURN(EBUSY);
}
static int lpt_mutex_blockcell(lpt_mutex_t * mutex,const struct timespec *abstime) {
        struct timeval now, tv;
        unsigned long int timeout;

        /* il faut arrondir au supérieur */
        tv.tv_sec = abstime->tv_sec;
        tv.tv_usec =(abstime->tv_nsec + 999) / 1000;
	
        gettimeofday(&now, NULL);
	
        if(timercmp(&tv, &now, <=)) {
                mdebug("lpt_mutex_blockcell : valeur temporelle invalide\n");
                LOG_RETURN(ETIMEDOUT);
        }
	
        timeout = (((tv.tv_sec*1e6 + tv.tv_usec) -
        		  (now.tv_sec*1e6 + now.tv_usec)) + marcel_gettimeslice()-1)/marcel_gettimeslice();

        lpt_lock_acquire(&mutex->__data.__lock.__spinlock); 

        {
                blockcell c;

                __lpt_register_spinlocked(&mutex->__data.__lock,
        		          marcel_self(), &c);

                //tant que c'est bloqué et qu'il y a du temps...
                while(c.blocked && timeout) {
                        ma_set_current_state(MA_TASK_INTERRUPTIBLE);
                        lpt_lock_release(&mutex->__data.__lock.__spinlock);
                        timeout = ma_schedule_timeout(timeout+1);
                        lpt_lock_acquire(&mutex->__data.__lock.__spinlock);
                }
                // si c'est encore bloqué (cad le temps est écoulé)
                if (c.blocked) {
                        if (__lpt_unregister_spinlocked(&mutex->__data.__lock, &c)) {
                                pm2debug("Strange, we should be in the queue !!! (%s:%d)\n", __FILE__, __LINE__);
                        }
                        //on sort	
                        lpt_lock_release(&mutex->__data.__lock.__spinlock);
                        LOG_RETURN(ETIMEDOUT);
                }
                lpt_lock_release(&mutex->__data.__lock.__spinlock);
        }
        LOG_RETURN(0);
}
int lpt_mutex_timedlock(lpt_mutex_t * mutex,const struct timespec *abstime) {
        LOG_IN();

        marcel_t cthread = marcel_self();
        int ret;

  if (__builtin_expect (abstime->tv_nsec, 0) < 0
		  || __builtin_expect (abstime->tv_nsec, 0) >= 1000000000) {
                mdebug("lpt_mutex_timedlock : valeur temporelle invalide\n");       
                LOG_RETURN(EINVAL);
        }

        switch(mutex->__data.__kind) {
  
                case LPT_MUTEX_RECURSIVE_NP:
                        cthread = cthread;
                        if (mutex->__data.__owner == cthread) {
                                mutex->__data.__count++;
                                LOG_RETURN(0);
    }
                        if (mutex->__data.__nusers == 0)
                                mutex->__data.__count = 1;
                        else {
                                ret = lpt_mutex_blockcell(mutex,abstime);
                                if (ret)
                                        LOG_RETURN(ETIMEDOUT);
                                else
                                        mutex->__data.__count = 1;
      }
                break;

                case LPT_MUTEX_ERRORCHECK_NP:
                        cthread = cthread;
                        if (mutex->__data.__owner == cthread) 
                                LOG_RETURN(EDEADLK);
                        if (mutex->__data.__nusers != 0) {
                                ret = lpt_mutex_blockcell(mutex,abstime);
                                if (ret)
                                        LOG_RETURN(ETIMEDOUT);
                        }
                break;

                case LPT_MUTEX_ADAPTIVE_NP:
                case LPT_MUTEX_TIMED_NP:
                        if (mutex->__data.__nusers != 0) {
                                ret = lpt_mutex_blockcell(mutex,abstime);
                                if (ret)
                                        LOG_RETURN(ETIMEDOUT);
                        }
                break;
  
  default:
                        LOG_RETURN(EINVAL);
  }
    
        mutex->__data.__nusers ++;
        mutex->__data.__owner = cthread;
        __lpt_lock(&mutex->__data.__lock, NULL);
        LOG_RETURN(0);
}
int lpt_mutex_unlock_usercnt(lpt_mutex_t * mutex, int decr) {
        LOG_IN();

        switch (__builtin_expect (mutex->__data.__kind, LPT_MUTEX_TIMED_NP)) {
	case LPT_MUTEX_RECURSIVE_NP:
		/* Recursive mutex.  */
		if (mutex->__data.__owner != MARCEL_SELF)
                                LOG_RETURN(EPERM);
		
		if (--mutex->__data.__count != 0)
			/* We still hold the mutex.  */
                                LOG_RETURN(0);
		break;
		
	case LPT_MUTEX_ERRORCHECK_NP:
		/* Error checking mutex.  */
		if (mutex->__data.__owner != MARCEL_SELF
		    //|| ! lll_mutex_islocked (mutex->__data.__lock)
			)
                        LOG_RETURN(EPERM);
		break;
		
	default:
		/* Correct code cannot set any other type.  */
	case LPT_MUTEX_TIMED_NP:
	case LPT_MUTEX_ADAPTIVE_NP:
		/* Normal mutex.  Nothing special to do.  */
		break;
	}
	
	/* Always reset the owner field.  */
	mutex->__data.__owner = 0;
	if (decr)
		/* One less user.  */
		--mutex->__data.__nusers;
	
	/* Unlock.  */
	__lpt_unlock(&mutex->__data.__lock);
	
        LOG_RETURN(0);
}
 
int lpt_mutex_unlock(lpt_mutex_t * mutex) {
        LOG_IN();
        LOG_RETURN(lpt_mutex_unlock_usercnt (mutex, 1));
}
int lpt_mutexattr_init(lpt_mutexattr_t * attr) {
        LOG_IN();
	if (sizeof (struct lpt_mutexattr) != sizeof (lpt_mutexattr_t))
		memset (attr, '\0', sizeof (*attr));

	/* We use bit 31 to signal whether the mutex is going to be
	   process-shared or not.  By default it is zero, i.e., the
	   mutex is not process-shared.  */
	((struct lpt_mutexattr *) attr)->mutexkind = LPT_MUTEX_NORMAL;
	
        LOG_RETURN(0);
}
int lpt_mutexattr_destroy(lpt_mutexattr_t * attr) {
        LOG_IN();
        LOG_RETURN(0);
}
int lpt_mutexattr_settype(lpt_mutexattr_t * attr, int kind) {
        LOG_IN();
	struct lpt_mutexattr *iattr;

        if (kind < LPT_MUTEX_NORMAL || kind > LPT_MUTEX_ADAPTIVE_NP) {
                mdebug("lpt_mutexattr_settype : valeur kind(%d) invalide\n",kind);
                LOG_RETURN(EINVAL);
	}
	iattr = (struct lpt_mutexattr *) attr;

	/* We use bit 31 to signal whether the mutex is going to be
	   process-shared or not.  */
	iattr->mutexkind = (iattr->mutexkind & 0x80000000) | kind;
        LOG_RETURN(0);
}
int lpt_mutexattr_gettype(const lpt_mutexattr_t * __restrict attr,
	int * __restrict kind) {
        LOG_IN();
	const struct lpt_mutexattr *iattr;
	iattr = (const struct lpt_mutexattr *) attr;

	/* We use bit 31 to signal whether the mutex is going to be
	   process-shared or not.  */
	*kind = iattr->mutexkind & ~0x80000000;

        LOG_RETURN(0);
}
int lpt_mutexattr_setpshared(lpt_mutexattr_t * attr, int pshared) {
        LOG_IN();
	struct lpt_mutexattr *iattr;

	if (pshared != LPT_PROCESS_PRIVATE
        	     && __builtin_expect (pshared != LPT_PROCESS_SHARED, 0)) {
                mdebug("lpt_mutexattr_setpshared : valeur pshared(%d)  invalide\n",pshared);
                LOG_RETURN(EINVAL);
   }
	/* For now it is not possible to share a mutex variable.  */
	if (pshared != MARCEL_PROCESS_PRIVATE) {
                fprintf(stderr,"lpt_mutexattr_setpshared : shared mutex requested!\n");
                LOG_RETURN(ENOTSUP);
	}

	iattr = (struct lpt_mutexattr *) attr;

	/* We use bit 31 to signal whether the mutex is going to be
	   process-shared or not.  */
	if (pshared == LPT_PROCESS_PRIVATE)
		iattr->mutexkind &= ~0x80000000;
	else
		iattr->mutexkind |= 0x80000000;

        LOG_RETURN(0);
}
int lpt_mutexattr_getpshared(const lpt_mutexattr_t * __restrict attr,
	int * __restrict pshared) {
        LOG_IN();
	const struct lpt_mutexattr *iattr;
	
	iattr = (const struct lpt_mutexattr *) attr;

	/* We use bit 31 to signal whether the mutex is going to be
	   process-shared or not.  */
	*pshared = ((iattr->mutexkind & 0x80000000) != 0
		    ? LPT_PROCESS_SHARED : LPT_PROCESS_PRIVATE);
	
        LOG_RETURN(0);
}
#include <limits.h>
#  ifdef MARCEL_ONCE_ENABLED
static marcel_mutex_t once_masterlock = MARCEL_MUTEX_INITIALIZER;
static marcel_cond_t once_finished = MARCEL_COND_INITIALIZER;
static int fork_generation = 0;	/* Child process increments this after fork. */

enum { NEVER = 0, IN_PROGRESS = 1, DONE = 2 };

#ifdef MARCEL_DEVIATION_ENABLED
static void marcel_once_cancelhandler(void *arg)
{
	marcel_once_t *once_control = arg;

	marcel_mutex_lock(&once_masterlock);
	*once_control = NEVER;
	marcel_mutex_unlock(&once_masterlock);
	marcel_cond_broadcast(&once_finished);
}
#endif /* MARCEL_DEVIATION_ENABLED */
int lpt_once(lpt_once_t * once_control, 
	void (*init_routine)(void)) {

        LOG_IN();
	/* flag for doing the condition broadcast outside of mutex */
	int state_changed;

	if (!marcel_test_activity()) {
		/* We're in single-threaded mode, so we can safely call INIT_ROUTINE.  */
		init_routine();
		*once_control = DONE;
		return 0;
	}

	/* Test without locking first for speed */
	if (*once_control == DONE) {
		READ_MEMORY_BARRIER();
                LOG_RETURN(0);
	}
	/* Lock and test again */
	
	state_changed = 0;
	
	marcel_mutex_lock(&once_masterlock);
	
	/* If this object was left in an IN_PROGRESS state in a parent
	   process (indicated by stale generation field), reset it to NEVER. */
	if ((*once_control & 3) == IN_PROGRESS && (*once_control & ~3) != fork_generation)
		*once_control = NEVER;
	
	/* If init_routine is being called from another routine, wait until
	   it completes. */
	while ((*once_control & 3) == IN_PROGRESS) {
		marcel_cond_wait(&once_finished, &once_masterlock);
	}
	/* Here *once_control is stable and either NEVER or DONE. */
	if (*once_control == NEVER) {
		*once_control = IN_PROGRESS | fork_generation;
		marcel_mutex_unlock(&once_masterlock);
#ifdef MARCEL_DEVIATION_ENABLED
		marcel_cleanup_push(marcel_once_cancelhandler, once_control);
#endif /* MARCEL_DEVIATION_ENABLED */
		init_routine();
#ifdef MARCEL_DEVIATION_ENABLED
		marcel_cleanup_pop(0);
#endif /* MARCEL_DEVIATION_ENABLED */
		marcel_mutex_lock(&once_masterlock);
		WRITE_MEMORY_BARRIER();
		*once_control = DONE;
		state_changed = 1;
	}
	marcel_mutex_unlock(&once_masterlock);
	
	if (state_changed)
		marcel_cond_broadcast(&once_finished);
	
        LOG_RETURN(0);
}

/*
 * Handle the state of the marcel_once mechanism across forks.  The
 * once_masterlock is acquired in the parent process prior to a fork to ensure
 * that no thread is in the critical region protected by the lock.  After the
 * fork, the lock is released. In the child, the lock and the condition
 * variable are simply reset.  The child also increments its generation
 * counter which lets marcel_once calls detect stale IN_PROGRESS states
 * and reset them back to NEVER.
 */
void lpt_once_fork_prepare(void) {
	marcel_mutex_lock(&once_masterlock);
}

void lpt_once_fork_parent(void) {
	marcel_mutex_unlock(&once_masterlock);
}

void lpt_once_fork_child(void) {
	marcel_mutex_init(&once_masterlock, NULL);
	marcel_cond_init(&once_finished, NULL);
	if (fork_generation <= INT_MAX - 4)
		fork_generation += 4;	/* leave least significant two bits zero */
	else
		fork_generation = 0;
}

#  endif /* MARCEL_ONCE_ENABLED */
#endif

